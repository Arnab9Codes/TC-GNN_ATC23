TC_Blocks:	11474
Exp_Edges:	1468672
Namespace(classes=3, dataset='pubmed', dim=500, epochs=1, hidden=16, model='gcn', num_layers=2)
CSR (ms):	21.465
Prep. (ms):	20.234
Train (ms):	   nan	Test (ms):	   nan
==PROF== Connected to process 14446 (/home/yuke/anaconda3/envs/tcgnn/bin/python3.7)
==PROF== Disconnected from process 14446
"ID","Process ID","Process Name","Host Name","Kernel Name","Kernel Time","Context","Stream","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,274,729,274.73",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,360,856,392.11",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","159,604",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","Memory [%]","%","40.98",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","SOL DRAM","%","40.98",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","Duration","nsecond","117,216",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.95",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","28.76",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","146,908.43",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","GPU Speed Of Light","SM [%]","%","19.35",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.39",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.36",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.66",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.39",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Compute Workload Analysis","SM Busy","%","21.01",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","364,873,600,873.60",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","Mem Busy","%","28.76",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","Max Bandwidth","%","40.98",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","69.42",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","11.10",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Scheduler Statistics","One or More Eligible","%","19.34",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.19",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Scheduler Statistics","No Eligible","%","80.66",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.19",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","5.17",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","5.18",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.50",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 3.1 cycles being stalled waiting for a math execution pipeline to be available. This represents about 59.6% of the total average of 5.2 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14,174.43",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Instruction Statistics","Executed Instructions","inst","4,649,214",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14,198.43",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Instruction Statistics","Issued Instructions","inst","4,657,086",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","NVLink","Logical Links","","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","NVLink","Physical Links","","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Block Size","","64",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Grid Size","","309",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Registers Per Thread","register/thread","158",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Threads","thread","19,776",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Launch Statistics","Waves Per SM","","3.77",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Block Limit SM","block","16",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Block Limit Registers","block","6",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Block Limit Shared Mem","block","1",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Block Limit Warps","block","24",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Achieved Occupancy","%","4.17",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Source Counters","Branch Instructions","inst","15,450",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Source Counters","Branch Efficiency","%","100",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","Source Counters","Avg. Divergent Branches","","0",
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128180 sectors, got 192296 (1.50x) at PC 0x7f6c7aa5b090"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5b640"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5b650"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5bcf0"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5bd30"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5bd40"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80210 sectors, got 128336 (1.60x) at PC 0x7f6c7aa58f20"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80164 sectors, got 128212 (1.60x) at PC 0x7f6c7aa59000"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80080 sectors, got 128128 (1.60x) at PC 0x7f6c7aa590f0"
"0","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:40:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80080 sectors, got 128128 (1.60x) at PC 0x7f6c7aa59130"
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,834,808,259.59",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,149,415,297.09",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,162",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","Memory [%]","%","20.97",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","SOL DRAM","%","0.15",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","Duration","nsecond","3,616",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","30.70",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","SOL L2 Cache","%","20.97",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,579.65",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","GPU Speed Of Light","SM [%]","%","3.51",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.32",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.12",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.24",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.37",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Compute Workload Analysis","SM Busy","%","9.24",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,132,743,362.83",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","Mem Busy","%","20.97",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","Max Bandwidth","%","20.50",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.98",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.89",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Scheduler Statistics","One or More Eligible","%","10.02",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.10",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Scheduler Statistics","No Eligible","%","89.98",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.37",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.37 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.68",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","27.03",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.00",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 48.4% of the total average of 23.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","127.83",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Instruction Statistics","Executed Instructions","inst","41,929",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","145.94",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Instruction Statistics","Issued Instructions","inst","47,869",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","NVLink","Logical Links","","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","NVLink","Physical Links","","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Block Size","","64",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Grid Size","","1,233",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Threads","thread","78,912",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Launch Statistics","Waves Per SM","","0.94",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Block Limit SM","block","16",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Block Limit Registers","block","64",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Block Limit Shared Mem","block","100",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Block Limit Warps","block","24",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Achieved Occupancy","%","20.73",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Occupancy","Achieved Active Warps Per SM","warp","9.95",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Source Counters","Branch Instructions","inst","4,935",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Source Counters","Branch Efficiency","%","100",
"1","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:40:41","1","7","Source Counters","Avg. Divergent Branches","","0",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,351,555,684.79",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,369,841,212.15",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","150,837",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","Memory [%]","%","10.27",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","SOL DRAM","%","2.40",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","Duration","nsecond","110,048",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.25",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.41",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","SM Active Cycles","cycle","76,449.52",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","GPU Speed Of Light","SM [%]","%","13.10",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.03",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.52",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Compute Workload Analysis","Issue Slots Busy","%","25.83",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Compute Workload Analysis","SM Busy","%","25.83",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","Memory Throughput","byte/second","21,571,387,031.11",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","Mem Busy","%","8.55",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","Max Bandwidth","%","10.27",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","66.84",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.13",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Memory Workload Analysis","Mem Pipes Busy","%","10.27",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Scheduler Statistics","One or More Eligible","%","25.95",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Scheduler Statistics","No Eligible","%","74.05",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.35",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.39",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.35 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.02",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.17",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.83",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.78",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 25.2 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 69.9% of the total average of 36.0 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","19,664.76",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Instruction Statistics","Executed Instructions","inst","6,450,040",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19,743.35",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Instruction Statistics","Issued Instructions","inst","6,475,820",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","NVLink","Logical Links","","0",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","NVLink","Physical Links","","0",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Block Size","","256",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Grid Size","","1,233",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Threads","thread","315,648",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Launch Statistics","Waves Per SM","","2.51",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","LaunchStats","","","","LaunchConfiguration","WRN","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 249 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 22.4%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid."
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Block Limit SM","block","16",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Block Limit Registers","block","6",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Block Limit Shared Mem","block","47",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Block Limit Warps","block","6",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Theoretical Occupancy","%","100",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Achieved Occupancy","%","77.63",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Occupancy","Achieved Active Warps Per SM","warp","37.26",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Source Counters","Branch Instructions Ratio","%","0.24",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Source Counters","Branch Instructions","inst","1,519,899",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Source Counters","Branch Efficiency","%","94.54",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","Source Counters","Avg. Divergent Branches","","136.23",
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 176280 sectors, got 350276 (1.99x) at PC 0x7f6c9a130a00"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 130687 sectors, got 157521 (1.21x) at PC 0x7f6c9a130720"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307c0"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307f0"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b40"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b60"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b80"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130ba0"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130be0"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c00"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c20"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c30"
"2","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:40:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 29458 sectors, got 29499 (1.00x) at PC 0x7f6c9a130890"
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,460,431,654.68",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,231,307,810.89",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,487",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","Memory [%]","%","35.04",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","SOL DRAM","%","35.04",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","Duration","nsecond","4,448",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.90",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","31.13",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,278.04",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","GPU Speed Of Light","SM [%]","%","4.39",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.25",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.15",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","6.98",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.28",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Compute Workload Analysis","SM Busy","%","6.98",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","284,604,316,546.76",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","Mem Busy","%","31.13",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","Max Bandwidth","%","35.04",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","51.93",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","4.39",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Scheduler Statistics","One or More Eligible","%","7.07",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Scheduler Statistics","No Eligible","%","92.93",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.09",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 14.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.09 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","57.81",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","65.09",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.81",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 41.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 71.5% of the total average of 57.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","203.15",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Instruction Statistics","Executed Instructions","inst","66,632",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","228.73",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Instruction Statistics","Issued Instructions","inst","75,025",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","NVLink","Logical Links","","0",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","NVLink","Physical Links","","0",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Block Size","","64",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Grid Size","","1,233",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Threads","thread","78,912",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Launch Statistics","Waves Per SM","","0.94",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Block Limit SM","block","16",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Block Limit Registers","block","42",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Block Limit Shared Mem","block","100",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Block Limit Warps","block","24",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Achieved Occupancy","%","35.40",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","16.99",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Source Counters","Branch Instructions","inst","4,946",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Source Counters","Branch Efficiency","%","99.96",
"3","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:40:49","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,383,333,333.33",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,180,524,553.57",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,070",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","Memory [%]","%","30.74",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","SOL DRAM","%","30.74",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","Duration","nsecond","5,120",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","22.10",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","SOL L2 Cache","%","30.63",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,908.07",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","GPU Speed Of Light","SM [%]","%","32.52",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 1% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.87",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.21",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Compute Workload Analysis","Issue Slots Busy","%","47.43",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.90",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Compute Workload Analysis","SM Busy","%","50.30",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","Memory Throughput","byte/second","247,400,000,000",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","Mem Busy","%","30.63",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","Max Bandwidth","%","30.74",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","L2 Hit Rate","%","57.18",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Memory Workload Analysis","Mem Pipes Busy","%","4.57",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Scheduler Statistics","One or More Eligible","%","48.26",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.48",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Scheduler Statistics","No Eligible","%","51.74",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.46",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","2.60",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.46 active warps per scheduler, but only an average of 2.60 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.59",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.91",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.00",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.47",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1,824.25",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Instruction Statistics","Executed Instructions","inst","598,355",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1,853.62",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Instruction Statistics","Issued Instructions","inst","607,988",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","NVLink","Logical Links","","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","NVLink","Physical Links","","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Block Size","","256",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Grid Size","","492",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Registers Per Thread","register/thread","29",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Threads","thread","125,952",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Launch Statistics","Waves Per SM","","1",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Block Limit SM","block","16",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Block Limit Registers","block","8",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Block Limit Shared Mem","block","100",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Block Limit Warps","block","6",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Theoretical Occupancy","%","100",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Achieved Occupancy","%","75.76",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Occupancy","Achieved Active Warps Per SM","warp","36.36",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Source Counters","Branch Instructions Ratio","%","0.05",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Source Counters","Branch Instructions","inst","30,057",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Source Counters","Branch Efficiency","%","100",
"4","14446","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:40:52","1","7","Source Counters","Avg. Divergent Branches","","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,064,721,969.01",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,327,557,299.13",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","46,659",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","Memory [%]","%","47.16",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","SOL DRAM","%","4.15",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","Duration","nsecond","35,104",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","53.52",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.00",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","41,069.51",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","GPU Speed Of Light","SM [%]","%","15.78",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.42",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.37",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.57",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.42",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Compute Workload Analysis","SM Busy","%","10.57",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","36,153,144,940.75",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","Mem Busy","%","47.16",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","Max Bandwidth","%","15.78",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","75.66",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","41.30",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","15.78",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Scheduler Statistics","One or More Eligible","%","21.18",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Scheduler Statistics","No Eligible","%","78.82",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.74",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.76",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.77",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4,320.35",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Instruction Statistics","Executed Instructions","inst","1,417,074",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4,340.35",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Instruction Statistics","Issued Instructions","inst","1,423,634",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","NVLink","Logical Links","","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","NVLink","Physical Links","","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Block Size","","64",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Grid Size","","309",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Threads","thread","19,776",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Launch Statistics","Waves Per SM","","3.77",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Block Limit SM","block","16",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Block Limit Registers","block","6",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Block Limit Shared Mem","block","1",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Block Limit Warps","block","24",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Achieved Occupancy","%","4.16",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Source Counters","Branch Instructions","inst","6,180",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Source Counters","Branch Efficiency","%","100",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","Source Counters","Avg. Divergent Branches","","0",
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af19860"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af198a0"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af198d0"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af19900"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af199d0"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af19a00"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af19a30"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af19a60"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1232 sectors, got 4928 (4.00x) at PC 0x7f6c7af19ae0"
"5","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:40:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1232 sectors, got 4928 (4.00x) at PC 0x7f6c7af19b00"
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,868,312,757.20",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,150,187,389.77",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,986",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","Memory [%]","%","6.39",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","SOL DRAM","%","0.32",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","Duration","nsecond","2,592",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.42",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.39",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","SM Active Cycles","cycle","903.59",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","GPU Speed Of Light","SM [%]","%","1.03",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.11",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.39",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.14",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Compute Workload Analysis","SM Busy","%","3.39",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,419,753,086.42",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","Mem Busy","%","6.39",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","Max Bandwidth","%","5.73",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.95",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.76",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Scheduler Statistics","One or More Eligible","%","3.68",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Scheduler Statistics","No Eligible","%","96.32",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.31",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 27.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.31 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.60",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.45",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.98",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.10",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.1% of the total average of 35.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","24.03",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Instruction Statistics","Executed Instructions","inst","7,881",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","30.67",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Instruction Statistics","Issued Instructions","inst","10,061",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","NVLink","Logical Links","","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","NVLink","Physical Links","","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Block Size","","64",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Grid Size","","232",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Threads","thread","14,848",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Launch Statistics","Waves Per SM","","0.18",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Block Limit SM","block","16",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Block Limit Registers","block","64",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Block Limit Shared Mem","block","100",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Block Limit Warps","block","24",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Achieved Occupancy","%","9.48",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.55",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Source Counters","Branch Instructions","inst","929",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Source Counters","Branch Efficiency","%","100",
"6","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:00","1","7","Source Counters","Avg. Divergent Branches","","0",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,239,972,807.61",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,349,558,730.70",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","63,590",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","Memory [%]","%","13.12",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","SOL DRAM","%","2.68",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","Duration","nsecond","47,072",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","26.86",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.38",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","SM Active Cycles","cycle","31,023.82",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","GPU Speed Of Light","SM [%]","%","22.95",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.87",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.91",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Compute Workload Analysis","Issue Slots Busy","%","46.99",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.88",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Compute Workload Analysis","SM Busy","%","46.99",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","Memory Throughput","byte/second","23,768,864,717.88",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","Mem Busy","%","8.65",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","Max Bandwidth","%","13.12",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","87.21",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","L2 Hit Rate","%","18.54",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Memory Workload Analysis","Mem Pipes Busy","%","13.12",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Scheduler Statistics","One or More Eligible","%","46.64",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.47",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Scheduler Statistics","No Eligible","%","53.36",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.47",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.89",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.47 active warps per scheduler, but only an average of 0.89 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.31",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.42",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","27.77",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.79",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 9.1 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 45.0% of the total average of 20.3 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14,500.67",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Instruction Statistics","Executed Instructions","inst","4,756,221",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14,576.69",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Instruction Statistics","Issued Instructions","inst","4,781,154",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","NVLink","Logical Links","","0",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","NVLink","Physical Links","","0",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Block Size","","256",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Grid Size","","1,233",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Threads","thread","315,648",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Launch Statistics","Waves Per SM","","2.51",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","LaunchStats","","","","LaunchConfiguration","WRN","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 249 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 20.1%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid."
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Block Limit SM","block","16",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Block Limit Registers","block","6",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Block Limit Shared Mem","block","47",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Block Limit Warps","block","6",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Theoretical Occupancy","%","100",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Achieved Occupancy","%","79.88",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Occupancy","Achieved Active Warps Per SM","warp","38.34",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Source Counters","Branch Instructions Ratio","%","0.28",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Source Counters","Branch Instructions","inst","1,318,179",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Source Counters","Branch Efficiency","%","94.05",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","Source Counters","Avg. Divergent Branches","","123.43",
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 130687 sectors, got 157521 (1.21x) at PC 0x7f6c9a130720"
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307c0"
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307f0"
"7","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:41:04","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 29458 sectors, got 29499 (1.00x) at PC 0x7f6c9a130890"
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,908,045,977.01",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,146,936,576.35",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,267",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","Memory [%]","%","11.83",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","SOL DRAM","%","8.94",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","Duration","nsecond","3,712",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.97",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","SOL L2 Cache","%","11.83",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","SM Active Cycles","cycle","2,113.10",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","GPU Speed Of Light","SM [%]","%","12.26",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 3% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.48",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Compute Workload Analysis","Issue Slots Busy","%","24.70",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Compute Workload Analysis","SM Busy","%","24.70",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","Memory Throughput","byte/second","67,896,551,724.14",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","Mem Busy","%","11.83",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","Max Bandwidth","%","10.80",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","51.62",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","L2 Hit Rate","%","69.90",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Memory Workload Analysis","Mem Pipes Busy","%","9.91",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Scheduler Statistics","One or More Eligible","%","25.24",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Scheduler Statistics","No Eligible","%","74.76",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.31",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.38",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.31 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","13.10",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","13.46",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.40",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 4.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 30.5% of the total average of 13.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","508.09",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Instruction Statistics","Executed Instructions","inst","166,653",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","521.95",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Instruction Statistics","Issued Instructions","inst","171,199",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","NVLink","Logical Links","","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","NVLink","Physical Links","","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Block Size","","128",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Grid Size","","309",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Registers Per Thread","register/thread","17",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Threads","thread","39,552",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Launch Statistics","Waves Per SM","","0.31",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Block Limit SM","block","16",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Block Limit Registers","block","21",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Block Limit Shared Mem","block","100",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Block Limit Warps","block","12",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Theoretical Occupancy","%","100",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Achieved Occupancy","%","27.13",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Occupancy","Achieved Active Warps Per SM","warp","13.02",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Source Counters","Branch Instructions Ratio","%","0.03",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Source Counters","Branch Instructions","inst","4,935",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Source Counters","Branch Efficiency","%","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","Source Counters","Avg. Divergent Branches","","0",
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b82b80"
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b83250"
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3698 sectors, got 7394 (2.00x) at PC 0x7f6ca3b82b30"
"8","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:41:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3698 sectors, got 7394 (2.00x) at PC 0x7f6ca3b83040"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,981,351,981.35",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,170,267,232.77",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,357",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","Memory [%]","%","0.75",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","SOL DRAM","%","0.60",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","Duration","nsecond","4,576",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","16.98",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.75",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","SM Active Cycles","cycle","117.91",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","GPU Speed Of Light","SM [%]","%","0.37",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.44",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.45",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.46",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Compute Workload Analysis","SM Busy","%","11.45",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,615,384,615.38",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","Mem Busy","%","0.75",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","Max Bandwidth","%","0.60",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.91",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.37",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Scheduler Statistics","One or More Eligible","%","12.12",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Scheduler Statistics","No Eligible","%","87.88",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.96",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.96 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.20",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.81",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.62",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.6% of the total average of 16.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","NVLink","Logical Links","","0",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","NVLink","Physical Links","","0",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Block Size","","256",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Grid Size","","3",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Threads","thread","768",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Launch Statistics","Waves Per SM","","0.01",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Block Limit SM","block","16",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Block Limit Registers","block","8",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Block Limit Shared Mem","block","88",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Block Limit Warps","block","6",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Theoretical Occupancy","%","100",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Achieved Occupancy","%","15.66",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.51",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Source Counters","Branch Instructions","inst","267",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Source Counters","Branch Efficiency","%","97.42",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"9","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,227,414,330.22",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,059,037,049.40",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,629",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","Memory [%]","%","0.71",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","Duration","nsecond","3,424",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","41.63",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.71",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","SM Active Cycles","cycle","21.65",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.32",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.31",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.33",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Compute Workload Analysis","SM Busy","%","8.31",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","Memory Throughput","byte/second","74,766,355.14",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","Mem Busy","%","0.71",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Scheduler Statistics","One or More Eligible","%","8.74",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Scheduler Statistics","No Eligible","%","91.26",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.99",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.99 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.75",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.46",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 8.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 37.2% of the total average of 22.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Instruction Statistics","Executed Instructions","inst","572",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Instruction Statistics","Issued Instructions","inst","590",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","NVLink","Logical Links","","0",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","NVLink","Physical Links","","0",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Block Size","","256",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Grid Size","","1",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Threads","thread","256",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Launch Statistics","Waves Per SM","","0.00",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Block Limit SM","block","16",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Block Limit Registers","block","5",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Block Limit Shared Mem","block","88",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Block Limit Warps","block","6",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Achieved Occupancy","%","15.70",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.54",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Source Counters","Branch Instructions","inst","92",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Source Counters","Branch Efficiency","%","96.49",
"10","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:14","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,675,675,675.68",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,132,360,038.61",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,684",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","Memory [%]","%","0.97",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","Duration","nsecond","2,368",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","80.48",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.97",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.18",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.64",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Compute Workload Analysis","SM Busy","%","2.64",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","54,054,054.05",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","Mem Busy","%","0.97",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","Max Bandwidth","%","0.34",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Scheduler Statistics","One or More Eligible","%","2.79",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Scheduler Statistics","No Eligible","%","97.21",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 35.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.13",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.27",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 30.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 83.0% of the total average of 36.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Instruction Statistics","Executed Instructions","inst","81",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Instruction Statistics","Issued Instructions","inst","97",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","NVLink","Logical Links","","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","NVLink","Physical Links","","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Block Size","","128",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Grid Size","","1",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Threads","thread","128",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Launch Statistics","Waves Per SM","","0.00",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Block Limit SM","block","16",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Block Limit Registers","block","32",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Block Limit Shared Mem","block","100",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Block Limit Warps","block","12",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Theoretical Occupancy","%","100",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Achieved Occupancy","%","8.01",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.84",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Source Counters","Branch Instructions","inst","5",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Source Counters","Branch Efficiency","%","0",
"11","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:17","1","7","Source Counters","Avg. Divergent Branches","","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,586,666,666.67",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,256,696,428.57",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,035",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","Memory [%]","%","1.29",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","SOL DRAM","%","0.53",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","Duration","nsecond","4,800",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.76",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.90",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,346.41",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","GPU Speed Of Light","SM [%]","%","1.28",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.16",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.18",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Compute Workload Analysis","SM Busy","%","4.18",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,400,000,000",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","Mem Busy","%","0.90",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","Max Bandwidth","%","1.29",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","70.37",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","L2 Hit Rate","%","74.84",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.28",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Scheduler Statistics","One or More Eligible","%","4.10",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Scheduler Statistics","No Eligible","%","95.90",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.90",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.52",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.06",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.49",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 7.7 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 32.0% of the total average of 23.9 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","52.70",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Instruction Statistics","Executed Instructions","inst","17,284",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","56.27",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Instruction Statistics","Issued Instructions","inst","18,457",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","NVLink","Logical Links","","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","NVLink","Physical Links","","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Block Size","","128",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Grid Size","","31",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Threads","thread","3,968",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Launch Statistics","Waves Per SM","","0.04",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Block Limit SM","block","16",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Block Limit Registers","block","10",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Block Limit Shared Mem","block","16",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Block Limit Warps","block","12",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Achieved Occupancy","%","8.67",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.16",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Source Counters","Branch Instructions","inst","1,142",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Source Counters","Branch Efficiency","%","100",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","Source Counters","Avg. Divergent Branches","","0",
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"12","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,830,148,619.96",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,152,013,193.81",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,789",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","Memory [%]","%","0.48",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","Duration","nsecond","5,024",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","18.52",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.48",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","SM Active Cycles","cycle","48.60",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.13",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.47",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.14",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Compute Workload Analysis","SM Busy","%","3.47",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","Memory Throughput","byte/second","254,777,070.06",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","Mem Busy","%","0.48",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","6.15",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.38",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Scheduler Statistics","One or More Eligible","%","4.11",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Scheduler Statistics","No Eligible","%","95.89",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.32",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.02",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.27",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.48",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 7.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 31.2% of the total average of 24.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 31.3% of the total average of 24.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.58",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Instruction Statistics","Executed Instructions","inst","517",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.69",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Instruction Statistics","Issued Instructions","inst","553",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","NVLink","Logical Links","","0",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","NVLink","Physical Links","","0",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Block Size","","128",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Grid Size","","1",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Threads","thread","128",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Launch Statistics","Waves Per SM","","0.00",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Block Limit SM","block","16",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Block Limit Registers","block","16",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Block Limit Shared Mem","block","100",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Block Limit Warps","block","12",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Theoretical Occupancy","%","100",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Achieved Occupancy","%","7.04",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.38",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Source Counters","Branch Instructions Ratio","%","0.14",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Source Counters","Branch Instructions","inst","71",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Source Counters","Branch Efficiency","%","97.92",
"13","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:25","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,189,125,295.51",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,205,547,112.46",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,441",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","Memory [%]","%","0.74",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","SOL DRAM","%","0.57",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","Duration","nsecond","4,512",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.77",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.74",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","SM Active Cycles","cycle","112.70",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","GPU Speed Of Light","SM [%]","%","0.37",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.46",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.98",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.48",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Compute Workload Analysis","SM Busy","%","11.98",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,482,269,503.55",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","Mem Busy","%","0.74",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","Max Bandwidth","%","0.57",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.86",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.37",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Scheduler Statistics","One or More Eligible","%","12.25",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Scheduler Statistics","No Eligible","%","87.75",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.01",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.01 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.40",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.02",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.62",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.4% of the total average of 16.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","NVLink","Logical Links","","0",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","NVLink","Physical Links","","0",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Block Size","","256",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Grid Size","","3",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Threads","thread","768",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Launch Statistics","Waves Per SM","","0.01",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Block Limit SM","block","16",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Block Limit Registers","block","8",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Block Limit Shared Mem","block","88",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Block Limit Warps","block","6",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Theoretical Occupancy","%","100",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Achieved Occupancy","%","16.18",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.77",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Source Counters","Branch Instructions","inst","267",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Source Counters","Branch Efficiency","%","97.42",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"14","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:28","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,950,617,283.95",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,166,708,002.65",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,034",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","Memory [%]","%","0.73",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","SOL DRAM","%","0.73",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","Duration","nsecond","3,456",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","40.56",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.64",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.22",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","GPU Speed Of Light","SM [%]","%","0.04",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.10",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.32",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Compute Workload Analysis","SM Busy","%","8.10",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","Memory Throughput","byte/second","5,592,592,592.59",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","Mem Busy","%","0.64",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","Max Bandwidth","%","0.73",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.04",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Scheduler Statistics","One or More Eligible","%","8.58",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Scheduler Statistics","No Eligible","%","91.42",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.98",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.98 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.08",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.81",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 38.9% of the total average of 23.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Instruction Statistics","Executed Instructions","inst","572",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Instruction Statistics","Issued Instructions","inst","590",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","NVLink","Logical Links","","0",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","NVLink","Physical Links","","0",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Block Size","","256",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Grid Size","","1",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Threads","thread","256",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Launch Statistics","Waves Per SM","","0.00",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Block Limit SM","block","16",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Block Limit Registers","block","5",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Block Limit Shared Mem","block","88",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Block Limit Warps","block","6",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Achieved Occupancy","%","15.62",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.50",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Source Counters","Branch Instructions","inst","92",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Source Counters","Branch Efficiency","%","96.49",
"15","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:41:32","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,822,222,222.22",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,139,285,714.29",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,736",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","84.93",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.60",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.79",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Compute Workload Analysis","SM Busy","%","2.79",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Scheduler Statistics","One or More Eligible","%","2.90",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Scheduler Statistics","No Eligible","%","97.10",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.48",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.30",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 80.4% of the total average of 34.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Instruction Statistics","Executed Instructions","inst","81",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Instruction Statistics","Issued Instructions","inst","97",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","NVLink","Logical Links","","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","NVLink","Physical Links","","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Block Size","","128",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Grid Size","","1",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Threads","thread","128",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Launch Statistics","Waves Per SM","","0.00",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Block Limit SM","block","16",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Block Limit Registers","block","32",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Block Limit Shared Mem","block","100",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Block Limit Warps","block","12",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Theoretical Occupancy","%","100",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Achieved Occupancy","%","8.18",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.93",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Source Counters","Branch Instructions","inst","5",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Source Counters","Branch Efficiency","%","0",
"16","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:41:35","1","7","Source Counters","Avg. Divergent Branches","","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,106,666,666.67",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,190,119,047.62",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,715",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","Memory [%]","%","1.35",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","SOL DRAM","%","0.55",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","Duration","nsecond","4,800",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.48",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.94",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,409.77",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","GPU Speed Of Light","SM [%]","%","1.35",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.15",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.02",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.16",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Compute Workload Analysis","SM Busy","%","4.02",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,266,666,666.67",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","Mem Busy","%","0.94",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","Max Bandwidth","%","1.35",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","69.21",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","76.68",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.35",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Scheduler Statistics","One or More Eligible","%","4.14",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Scheduler Statistics","No Eligible","%","95.86",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.92",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.64",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.84",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.22",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 7.5 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 31.5% of the total average of 23.9 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","52.85",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Instruction Statistics","Executed Instructions","inst","17,336",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","56.66",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Instruction Statistics","Issued Instructions","inst","18,585",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","NVLink","Logical Links","","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","NVLink","Physical Links","","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Block Size","","128",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Grid Size","","31",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Threads","thread","3,968",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Launch Statistics","Waves Per SM","","0.04",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Block Limit SM","block","16",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Block Limit Registers","block","10",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Block Limit Shared Mem","block","16",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Block Limit Warps","block","12",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Achieved Occupancy","%","8.02",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.85",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Source Counters","Branch Instructions","inst","1,127",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Source Counters","Branch Efficiency","%","100",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","Source Counters","Avg. Divergent Branches","","0",
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"17","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:41:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,832,020,997.38",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,157,691,226.10",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,707",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","Memory [%]","%","0.58",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","SOL DRAM","%","0.04",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","Duration","nsecond","4,064",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","27.29",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.58",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","SM Active Cycles","cycle","32.98",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.08",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Compute Workload Analysis","SM Busy","%","2.08",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","Memory Throughput","byte/second","283,464,566.93",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","Mem Busy","%","0.58",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","Max Bandwidth","%","0.20",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.72",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Scheduler Statistics","One or More Eligible","%","3.31",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Scheduler Statistics","No Eligible","%","96.69",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 30.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","30.00",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","32.92",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.60",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.55",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 12.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 40.9% of the total average of 30.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.62",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Instruction Statistics","Executed Instructions","inst","205",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.69",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Instruction Statistics","Issued Instructions","inst","225",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","NVLink","Logical Links","","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","NVLink","Physical Links","","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Block Size","","128",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Grid Size","","1",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Threads","thread","128",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Launch Statistics","Waves Per SM","","0.00",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Block Limit SM","block","16",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Block Limit Registers","block","16",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Block Limit Shared Mem","block","100",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Block Limit Warps","block","12",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Theoretical Occupancy","%","100",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Achieved Occupancy","%","5.26",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.52",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Source Counters","Branch Instructions Ratio","%","0.20",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Source Counters","Branch Instructions","inst","41",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Source Counters","Branch Efficiency","%","95.45",
"18","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:41:42","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,962,703,962.70",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,170,673,076.92",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,359",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","Memory [%]","%","0.50",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","SOL DRAM","%","0.04",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","Duration","nsecond","4,576",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.64",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.50",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","SM Active Cycles","cycle","43.66",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.58",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Compute Workload Analysis","SM Busy","%","1.58",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","Memory Throughput","byte/second","307,692,307.69",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","Mem Busy","%","0.50",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","7.32",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","L2 Hit Rate","%","96.72",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Scheduler Statistics","One or More Eligible","%","6.36",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Scheduler Statistics","No Eligible","%","93.64",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 15.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","15.67",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.10",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","14.66",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","12.85",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 48.0% of the total average of 15.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 14.7 threads being active per cycle. This is further reduced to 12.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.67",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Instruction Statistics","Executed Instructions","inst","220",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.69",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Instruction Statistics","Issued Instructions","inst","226",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","NVLink","Logical Links","","0",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","NVLink","Physical Links","","0",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Block Size","","32",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Grid Size","","1",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Registers Per Thread","register/thread","34",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","256",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Threads","thread","32",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Launch Statistics","Waves Per SM","","0.00",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Block Limit SM","block","16",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Block Limit Registers","block","48",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Block Limit Shared Mem","block","80",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Block Limit Warps","block","48",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Achieved Occupancy","%","2.11",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.01",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Source Counters","Branch Instructions","inst","26",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Source Counters","Branch Efficiency","%","91.67",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 13 (3.25x) at PC 0x7f6c8ffdca40"
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 11 (2.75x) at PC 0x7f6c8ffdc6f0"
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f6c8ffdc830"
"19","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:41:45","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 7 (1.75x) at PC 0x7f6c8ffdc4e0"
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,789,473,684.21",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,148,143,796.99",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,796",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","Memory [%]","%","0.92",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","83.20",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.92",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.82",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","GPU Speed Of Light","SM [%]","%","0.00",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.99",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Compute Workload Analysis","SM Busy","%","0.99",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","Memory Throughput","byte/second","52,631,578.95",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","Mem Busy","%","0.92",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","Max Bandwidth","%","0.32",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.59",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Scheduler Statistics","One or More Eligible","%","2.20",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Scheduler Statistics","No Eligible","%","97.80",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 45.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","46.86",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","60.74",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","21.67",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.63",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 36.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 78.4% of the total average of 46.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.7 threads being active per cycle. This is further reduced to 21.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.08",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Instruction Statistics","Executed Instructions","inst","27",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.11",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Instruction Statistics","Issued Instructions","inst","35",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","NVLink","Logical Links","","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","NVLink","Physical Links","","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Block Size","","64",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Grid Size","","1",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Threads","thread","64",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Launch Statistics","Waves Per SM","","0.00",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Block Limit SM","block","16",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Block Limit Registers","block","64",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Block Limit Shared Mem","block","100",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Block Limit Warps","block","24",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Achieved Occupancy","%","3.88",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.86",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Source Counters","Branch Instructions Ratio","%","0.19",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Source Counters","Branch Instructions","inst","5",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Source Counters","Branch Efficiency","%","100",
"20","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:48","1","7","Source Counters","Avg. Divergent Branches","","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,102,564,102.56",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,188,083,791.21",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,945",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","Memory [%]","%","0.54",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","Duration","nsecond","4,160",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","23.32",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.54",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","SM Active Cycles","cycle","38.59",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.86",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.03",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Compute Workload Analysis","SM Busy","%","0.86",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","Memory Throughput","byte/second","153,846,153.85",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","Mem Busy","%","0.54",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","7.32",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.44",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Scheduler Statistics","One or More Eligible","%","3.47",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Scheduler Statistics","No Eligible","%","96.53",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 28.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","28.74",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","30.72",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.31",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.84",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 15.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 53.3% of the total average of 28.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.31",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Instruction Statistics","Executed Instructions","inst","102",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.33",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Instruction Statistics","Issued Instructions","inst","109",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","NVLink","Logical Links","","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","NVLink","Physical Links","","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Block Size","","32",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Grid Size","","1",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Threads","thread","32",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Launch Statistics","Waves Per SM","","0.00",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Block Limit SM","block","16",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Block Limit Registers","block","64",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Block Limit Shared Mem","block","100",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Block Limit Warps","block","48",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Achieved Occupancy","%","2.13",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.02",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Source Counters","Branch Instructions Ratio","%","0.20",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Source Counters","Branch Instructions","inst","20",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Source Counters","Branch Efficiency","%","88.89",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 13 (3.25x) at PC 0x7f6c8ffd8670"
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 11 (2.75x) at PC 0x7f6c8ffd8350"
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f6c8ffd83c0"
"21","14446","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:41:52","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 7 (1.75x) at PC 0x7f6c8ffd80a0"
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,772,357,723.58",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,130,335,365.85",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,969",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","Memory [%]","%","6.60",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","Duration","nsecond","2,624",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.56",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.60",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","SM Active Cycles","cycle","891.68",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","GPU Speed Of Light","SM [%]","%","1.03",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.11",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.44",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.14",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Compute Workload Analysis","SM Busy","%","3.44",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","Memory Throughput","byte/second","390,243,902.44",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","Mem Busy","%","6.60",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","Max Bandwidth","%","5.94",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.95",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.76",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Scheduler Statistics","One or More Eligible","%","3.64",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Scheduler Statistics","No Eligible","%","96.36",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.28",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.28 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.30",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.07",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.98",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.10",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 24.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 67.9% of the total average of 35.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","24.03",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Instruction Statistics","Executed Instructions","inst","7,881",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","30.68",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Instruction Statistics","Issued Instructions","inst","10,063",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","NVLink","Logical Links","","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","NVLink","Physical Links","","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Block Size","","64",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Grid Size","","232",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Threads","thread","14,848",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Launch Statistics","Waves Per SM","","0.18",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Block Limit SM","block","16",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Block Limit Registers","block","64",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Block Limit Shared Mem","block","100",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Block Limit Warps","block","24",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Achieved Occupancy","%","9.58",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.60",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Source Counters","Branch Instructions","inst","929",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Source Counters","Branch Efficiency","%","100",
"22","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:41:55","1","7","Source Counters","Avg. Divergent Branches","","0",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,075,117,370.89",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,182,469,818.91",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,375",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","Memory [%]","%","0.75",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","SOL DRAM","%","0.58",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","16.94",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.75",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","SM Active Cycles","cycle","118.20",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","GPU Speed Of Light","SM [%]","%","0.37",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.44",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.42",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.46",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Compute Workload Analysis","SM Busy","%","11.42",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,507,042,253.52",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","Mem Busy","%","0.75",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","Max Bandwidth","%","0.58",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.86",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.37",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Scheduler Statistics","One or More Eligible","%","12.19",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Scheduler Statistics","No Eligible","%","87.81",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.96",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.96 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.06",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.66",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.62",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.9% of the total average of 16.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","NVLink","Logical Links","","0",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","NVLink","Physical Links","","0",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Block Size","","256",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Grid Size","","3",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Threads","thread","768",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Launch Statistics","Waves Per SM","","0.01",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Block Limit SM","block","16",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Block Limit Registers","block","8",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Block Limit Shared Mem","block","88",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Block Limit Warps","block","6",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Theoretical Occupancy","%","100",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Achieved Occupancy","%","15.68",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.53",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Source Counters","Branch Instructions","inst","267",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Source Counters","Branch Efficiency","%","97.42",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"23","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:41:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,283,950,617.28",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,068,039,021.16",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,694",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","Memory [%]","%","2.58",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","Duration","nsecond","3,456",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","40.34",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.58",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.34",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.05",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.32",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Compute Workload Analysis","SM Busy","%","8.05",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","Memory Throughput","byte/second","111,111,111.11",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","Mem Busy","%","0.70",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","Max Bandwidth","%","2.58",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Scheduler Statistics","One or More Eligible","%","8.46",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.08",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Scheduler Statistics","No Eligible","%","91.54",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.32",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","24.05",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 51.2% of the total average of 23.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Instruction Statistics","Executed Instructions","inst","572",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Instruction Statistics","Issued Instructions","inst","590",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","NVLink","Logical Links","","0",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","NVLink","Physical Links","","0",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Block Size","","256",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Grid Size","","1",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Threads","thread","256",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Launch Statistics","Waves Per SM","","0.00",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Block Limit SM","block","16",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Block Limit Registers","block","5",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Block Limit Shared Mem","block","88",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Block Limit Warps","block","6",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Achieved Occupancy","%","15.83",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.60",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Source Counters","Branch Instructions","inst","92",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Source Counters","Branch Efficiency","%","96.49",
"24","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:42:02","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,680,000,000",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,119,047,619.05",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,690",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","Memory [%]","%","0.97",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","81.46",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.97",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.05",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.68",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Compute Workload Analysis","SM Busy","%","2.68",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","Memory Throughput","byte/second","106,666,666.67",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","Mem Busy","%","0.97",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","Max Bandwidth","%","0.34",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Scheduler Statistics","One or More Eligible","%","2.81",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Scheduler Statistics","No Eligible","%","97.19",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 35.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.64",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.68",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 82.2% of the total average of 35.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Instruction Statistics","Executed Instructions","inst","81",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Instruction Statistics","Issued Instructions","inst","97",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","NVLink","Logical Links","","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","NVLink","Physical Links","","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Block Size","","128",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Grid Size","","1",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Threads","thread","128",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Launch Statistics","Waves Per SM","","0.00",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Block Limit SM","block","16",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Block Limit Registers","block","32",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Block Limit Shared Mem","block","100",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Block Limit Warps","block","12",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Theoretical Occupancy","%","100",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Achieved Occupancy","%","8.11",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.89",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Source Counters","Branch Instructions","inst","5",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Source Counters","Branch Efficiency","%","0",
"25","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:42:05","1","7","Source Counters","Avg. Divergent Branches","","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,719,298,245.61",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,135,279,605.26",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,524",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","Memory [%]","%","1.39",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","SOL DRAM","%","0.58",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","Duration","nsecond","4,864",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.97",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.98",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,291.26",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","GPU Speed Of Light","SM [%]","%","1.40",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.16",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.34",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Compute Workload Analysis","SM Busy","%","4.34",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,263,157,894.74",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","Mem Busy","%","0.98",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","Max Bandwidth","%","1.39",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","71.44",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","L2 Hit Rate","%","75.60",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.40",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Scheduler Statistics","One or More Eligible","%","4.16",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Scheduler Statistics","No Eligible","%","95.84",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.51",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","24.90",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.03",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.24",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 8.0 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 34.2% of the total average of 23.5 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","52.89",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Instruction Statistics","Executed Instructions","inst","17,349",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","56.03",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Instruction Statistics","Issued Instructions","inst","18,377",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","NVLink","Logical Links","","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","NVLink","Physical Links","","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Block Size","","128",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Grid Size","","31",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Threads","thread","3,968",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Launch Statistics","Waves Per SM","","0.04",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Block Limit SM","block","16",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Block Limit Registers","block","10",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Block Limit Shared Mem","block","16",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Block Limit Warps","block","12",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Achieved Occupancy","%","8.49",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.08",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Source Counters","Branch Instructions","inst","1,112",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Source Counters","Branch Efficiency","%","100",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","Source Counters","Avg. Divergent Branches","","0",
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"26","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:42:09","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,512,820,512.82",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,095,982,142.86",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,649",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","Duration","nsecond","3,328",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","39.91",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.55",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.53",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Compute Workload Analysis","SM Busy","%","2.53",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","Memory Throughput","byte/second","192,307,692.31",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.58",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Scheduler Statistics","One or More Eligible","%","5.13",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Scheduler Statistics","No Eligible","%","94.87",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.45",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.02",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.99",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.91",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 49.1% of the total average of 19.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.53",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Instruction Statistics","Executed Instructions","inst","173",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.57",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Instruction Statistics","Issued Instructions","inst","187",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","NVLink","Logical Links","","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","NVLink","Physical Links","","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Block Size","","64",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Grid Size","","1",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Threads","thread","64",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Launch Statistics","Waves Per SM","","0.00",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Block Limit SM","block","16",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Block Limit Registers","block","32",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Block Limit Shared Mem","block","100",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Block Limit Warps","block","24",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Achieved Occupancy","%","4.14",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Source Counters","Branch Instructions Ratio","%","0.21",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Source Counters","Branch Instructions","inst","36",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Source Counters","Branch Efficiency","%","88.89",
"27","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:13","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,940,740,740.74",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,166,964,285.71",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,362",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","Memory [%]","%","0.78",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","Duration","nsecond","2,880",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.83",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.78",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","18.06",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.59",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Compute Workload Analysis","SM Busy","%","1.59",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","222,222,222.22",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","Mem Busy","%","0.78",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.58",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Scheduler Statistics","One or More Eligible","%","3.12",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Scheduler Statistics","No Eligible","%","96.88",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.96",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 32.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.96 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","30.71",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.09",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.40",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 15.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 51.1% of the total average of 30.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.24",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Instruction Statistics","Executed Instructions","inst","80",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.29",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Instruction Statistics","Issued Instructions","inst","94",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","NVLink","Logical Links","","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","NVLink","Physical Links","","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Block Size","","64",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Grid Size","","1",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Registers Per Thread","register/thread","22",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Threads","thread","64",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Launch Statistics","Waves Per SM","","0.00",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Block Limit SM","block","16",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Block Limit Registers","block","42",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Block Limit Shared Mem","block","100",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Block Limit Warps","block","24",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Achieved Occupancy","%","4.11",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Source Counters","Branch Instructions Ratio","%","0.19",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Source Counters","Branch Instructions","inst","15",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Source Counters","Branch Efficiency","%","87.50",
"28","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:17","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,703,703,703.70",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,133,838,383.84",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,593",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","Memory [%]","%","0.73",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","Duration","nsecond","3,168",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","41.32",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.73",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","SM Active Cycles","cycle","21.78",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.28",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Compute Workload Analysis","SM Busy","%","2.28",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","Memory Throughput","byte/second","202,020,202.02",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","Mem Busy","%","0.73",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.50",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Scheduler Statistics","One or More Eligible","%","4.64",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Scheduler Statistics","No Eligible","%","95.36",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.55",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.58",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.93",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.91",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 44.2% of the total average of 21.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.45",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Instruction Statistics","Executed Instructions","inst","149",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.50",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Instruction Statistics","Issued Instructions","inst","163",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","NVLink","Logical Links","","0",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","NVLink","Physical Links","","0",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Block Size","","64",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Grid Size","","1",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Threads","thread","64",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Launch Statistics","Waves Per SM","","0.00",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Block Limit SM","block","16",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Block Limit Registers","block","32",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Block Limit Shared Mem","block","100",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Block Limit Warps","block","24",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Achieved Occupancy","%","4.12",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Source Counters","Branch Instructions Ratio","%","0.23",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Source Counters","Branch Instructions","inst","34",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Source Counters","Branch Efficiency","%","88.89",
"29","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:42:21","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,350,427,350.43",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,078,811,813.19",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,694",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","Memory [%]","%","0.97",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","Duration","nsecond","2,496",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","79.01",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.97",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.39",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.24",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","7.17",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.29",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Compute Workload Analysis","SM Busy","%","8.14",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","51,282,051.28",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","Mem Busy","%","0.97",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.68",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Scheduler Statistics","One or More Eligible","%","7.37",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Scheduler Statistics","No Eligible","%","92.63",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.02",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 13.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.02 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","27.39",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","32.77",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.23",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 21.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.8% of the total average of 27.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.68",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Instruction Statistics","Executed Instructions","inst","224",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.82",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Instruction Statistics","Issued Instructions","inst","268",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","NVLink","Logical Links","","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","NVLink","Physical Links","","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Block Size","","256",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Grid Size","","1",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Threads","thread","256",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Launch Statistics","Waves Per SM","","0.00",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Block Limit SM","block","16",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Block Limit Registers","block","16",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Block Limit Shared Mem","block","100",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Block Limit Warps","block","6",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Theoretical Occupancy","%","100",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Achieved Occupancy","%","16.71",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","8.02",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Source Counters","Branch Instructions","inst","16",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Source Counters","Branch Efficiency","%","100",
"30","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:42:24","1","7","Source Counters","Avg. Divergent Branches","","0",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,510,067,114.09",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,249,258,449.19",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","23,829",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","Memory [%]","%","0.77",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","Duration","nsecond","19,072",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","67.96",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.12",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","SM Active Cycles","cycle","269.61",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","GPU Speed Of Light","SM [%]","%","0.30",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.81",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Compute Workload Analysis","Issue Slots Busy","%","20.17",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.81",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Compute Workload Analysis","SM Busy","%","22.14",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","Memory Throughput","byte/second","60,402,684.56",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","Mem Busy","%","0.77",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","Max Bandwidth","%","0.30",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","60.53",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.51",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.30",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Scheduler Statistics","One or More Eligible","%","20.16",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.20",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Scheduler Statistics","No Eligible","%","79.84",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.00",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.27",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.27 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","9.93",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","9.95",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.78",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.31",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","54.27",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Instruction Statistics","Executed Instructions","inst","17,799",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","54.37",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Instruction Statistics","Issued Instructions","inst","17,834",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","NVLink","Logical Links","","0",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","NVLink","Physical Links","","0",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Block Size","","256",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Grid Size","","1",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Registers Per Thread","register/thread","92",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","16,912",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Threads","thread","256",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Launch Statistics","Waves Per SM","","0.01",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Block Limit SM","block","16",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Block Limit Registers","block","2",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Block Limit Shared Mem","block","5",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Block Limit Warps","block","6",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Achieved Occupancy","%","16.62",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.98",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Source Counters","Branch Instructions Ratio","%","0.01",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Source Counters","Branch Instructions","inst","171",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Source Counters","Branch Efficiency","%","91.21",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","Source Counters","Avg. Divergent Branches","","0.02",
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf87d60"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf87e60"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf87f40"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88060"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88b30"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88bd0"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88cd0"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88d80"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88e70"
"31","14446","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:42:28","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 32 sectors, got 128 (4.00x) at PC 0x7f6cabf88ec0"
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,808,153,477.22",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,150,597,379.24",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,119",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","Memory [%]","%","0.63",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","SOL DRAM","%","0.08",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","Duration","nsecond","4,448",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.39",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.63",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","SM Active Cycles","cycle","514.05",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","GPU Speed Of Light","SM [%]","%","0.59",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.22",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.86",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.23",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Compute Workload Analysis","SM Busy","%","5.86",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","Memory Throughput","byte/second","575,539,568.35",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","Mem Busy","%","0.63",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","Max Bandwidth","%","0.54",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","77.01",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","L2 Hit Rate","%","94.90",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.54",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Scheduler Statistics","One or More Eligible","%","6.22",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Scheduler Statistics","No Eligible","%","93.78",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 16.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.31",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.14",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","15.14",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","13.08",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 48.7% of the total average of 16.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 15.1 threads being active per cycle. This is further reduced to 13.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","28.66",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Instruction Statistics","Executed Instructions","inst","9,402",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","30.13",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Instruction Statistics","Issued Instructions","inst","9,882",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","NVLink","Logical Links","","0",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","NVLink","Physical Links","","0",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Block Size","","128",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Grid Size","","15",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Registers Per Thread","register/thread","39",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Threads","thread","1,920",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Launch Statistics","Waves Per SM","","0.02",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 15 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Block Limit SM","block","16",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Block Limit Registers","block","12",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Block Limit Shared Mem","block","100",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Block Limit Warps","block","12",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Theoretical Occupancy","%","100",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Achieved Occupancy","%","8.03",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.85",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Source Counters","Branch Instructions","inst","897",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Source Counters","Branch Efficiency","%","87.45",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","Source Counters","Avg. Divergent Branches","","0.18",
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 60 sectors, got 75 (1.25x) at PC 0x7f6c9c849a70"
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 60 sectors, got 75 (1.25x) at PC 0x7f6c9c849930"
"32","14446","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:42:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 60 sectors, got 75 (1.25x) at PC 0x7f6c9c849970"
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,809,523,809.52",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,137,471,655.33",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,596",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","Memory [%]","%","15.84",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","SOL DRAM","%","15.84",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","Duration","nsecond","4,032",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","14.80",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","SOL L2 Cache","%","14.52",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","SM Active Cycles","cycle","2,443.34",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","GPU Speed Of Light","SM [%]","%","7.88",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.49",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.26",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Compute Workload Analysis","Issue Slots Busy","%","13.66",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.55",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Compute Workload Analysis","SM Busy","%","13.66",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","Memory Throughput","byte/second","118,761,904,761.90",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","Mem Busy","%","14.52",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","Max Bandwidth","%","15.84",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","51.67",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","L2 Hit Rate","%","54.11",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Memory Workload Analysis","Mem Pipes Busy","%","7.88",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Scheduler Statistics","One or More Eligible","%","14.41",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.14",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Scheduler Statistics","No Eligible","%","85.59",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.10",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.20",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.10 active warps per scheduler, but only an average of 0.20 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.53",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.86",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.30",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.19",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 8.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 41.1% of the total average of 21.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","301.09",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Instruction Statistics","Executed Instructions","inst","98,757",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","333.67",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Instruction Statistics","Issued Instructions","inst","109,445",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","NVLink","Logical Links","","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","NVLink","Physical Links","","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Block Size","","128",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Grid Size","","309",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Registers Per Thread","register/thread","21",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Threads","thread","39,552",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Launch Statistics","Waves Per SM","","0.31",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Block Limit SM","block","16",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Block Limit Registers","block","21",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Block Limit Shared Mem","block","100",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Block Limit Warps","block","12",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Theoretical Occupancy","%","100",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Achieved Occupancy","%","25.12",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Occupancy","Achieved Active Warps Per SM","warp","12.06",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Source Counters","Branch Instructions","inst","7,401",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Source Counters","Branch Efficiency","%","0",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","Source Counters","Avg. Divergent Branches","","3.76",
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b13280"
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b132f0"
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b135d0"
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3698 sectors, got 7394 (2.00x) at PC 0x7f6ca3b13270"
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3678 sectors, got 7344 (2.00x) at PC 0x7f6ca3b132d0"
"33","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 2, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:42:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3664 sectors, got 7309 (1.99x) at PC 0x7f6ca3b134a0"
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,642,276,422.76",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,121,679,006.97",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,947",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","Memory [%]","%","6.64",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","SOL DRAM","%","0.45",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","Duration","nsecond","2,624",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.41",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.64",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","904.34",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","GPU Speed Of Light","SM [%]","%","1.04",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.11",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.39",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.14",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Compute Workload Analysis","SM Busy","%","3.39",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,317,073,170.73",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","Mem Busy","%","6.64",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","Max Bandwidth","%","5.97",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.97",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.77",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Scheduler Statistics","One or More Eligible","%","3.68",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Scheduler Statistics","No Eligible","%","96.32",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.30",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 27.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.30 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.37",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.15",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.98",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.10",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.9% of the total average of 35.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","24.03",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Instruction Statistics","Executed Instructions","inst","7,881",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","30.67",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Instruction Statistics","Issued Instructions","inst","10,061",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","NVLink","Logical Links","","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","NVLink","Physical Links","","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Block Size","","64",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Grid Size","","232",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Threads","thread","14,848",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Launch Statistics","Waves Per SM","","0.18",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Block Limit SM","block","16",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Block Limit Registers","block","64",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Block Limit Shared Mem","block","100",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Block Limit Warps","block","24",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Achieved Occupancy","%","10.25",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.92",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Source Counters","Branch Instructions","inst","929",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Source Counters","Branch Efficiency","%","100",
"34","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:42:38","1","7","Source Counters","Avg. Divergent Branches","","0",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,175,938,489.37",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,340,076,201.78",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","63,278",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","Memory [%]","%","13.18",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","SOL DRAM","%","2.69",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","Duration","nsecond","47,168",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","26.67",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.39",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","SM Active Cycles","cycle","31,245.82",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","GPU Speed Of Light","SM [%]","%","23.06",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.86",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.92",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Compute Workload Analysis","Issue Slots Busy","%","46.65",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.87",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Compute Workload Analysis","SM Busy","%","46.65",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","Memory Throughput","byte/second","23,652,645,861.60",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","Mem Busy","%","8.70",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","Max Bandwidth","%","13.18",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","87.24",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","L2 Hit Rate","%","18.55",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Memory Workload Analysis","Mem Pipes Busy","%","13.18",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Scheduler Statistics","One or More Eligible","%","46.63",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.47",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Scheduler Statistics","No Eligible","%","53.37",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.49",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.89",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.49 active warps per scheduler, but only an average of 0.89 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.34",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.45",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","27.77",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.79",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 9.2 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 45.0% of the total average of 20.3 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14,500.67",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Instruction Statistics","Executed Instructions","inst","4,756,221",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14,576.44",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Instruction Statistics","Issued Instructions","inst","4,781,072",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","NVLink","Logical Links","","0",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","NVLink","Physical Links","","0",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Block Size","","256",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Grid Size","","1,233",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Threads","thread","315,648",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Launch Statistics","Waves Per SM","","2.51",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","LaunchStats","","","","LaunchConfiguration","WRN","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 249 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 20.9%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid."
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Block Limit SM","block","16",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Block Limit Registers","block","6",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Block Limit Shared Mem","block","47",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Block Limit Warps","block","6",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Theoretical Occupancy","%","100",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Achieved Occupancy","%","79.07",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Occupancy","Achieved Active Warps Per SM","warp","37.95",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Source Counters","Branch Instructions Ratio","%","0.28",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Source Counters","Branch Instructions","inst","1,318,179",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Source Counters","Branch Efficiency","%","94.05",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","Source Counters","Avg. Divergent Branches","","123.43",
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 130687 sectors, got 157521 (1.21x) at PC 0x7f6c9a130720"
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307c0"
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307f0"
"35","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:42:41","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 29458 sectors, got 29499 (1.00x) at PC 0x7f6c9a130890"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,996,461,429.58",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,313,765,354.87",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","39,660",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","Memory [%]","%","41.59",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","SOL DRAM","%","0.97",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","Duration","nsecond","30,144",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","47.04",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.64",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","SM Active Cycles","cycle","35,015.89",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","GPU Speed Of Light","SM [%]","%","17.43",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.49",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.44",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Compute Workload Analysis","Issue Slots Busy","%","12.41",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.50",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Compute Workload Analysis","SM Busy","%","12.41",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","Memory Throughput","byte/second","8,348,195,329.09",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","Mem Busy","%","41.59",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","Max Bandwidth","%","17.43",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","57.93",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","L2 Hit Rate","%","95.06",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Memory Workload Analysis","Mem Pipes Busy","%","17.43",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Scheduler Statistics","One or More Eligible","%","24.88",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Scheduler Statistics","No Eligible","%","75.12",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.25",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.02",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.04",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.89",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4,326",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Instruction Statistics","Executed Instructions","inst","1,418,928",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4,347",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Instruction Statistics","Issued Instructions","inst","1,425,816",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","NVLink","Logical Links","","0",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","NVLink","Physical Links","","0",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Block Size","","64",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Grid Size","","309",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Threads","thread","19,776",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Launch Statistics","Waves Per SM","","3.77",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Block Limit SM","block","16",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Block Limit Registers","block","6",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Block Limit Shared Mem","block","1",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Block Limit Warps","block","24",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Achieved Occupancy","%","4.17",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Source Counters","Branch Instructions","inst","6,180",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Source Counters","Branch Efficiency","%","100",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","Source Counters","Avg. Divergent Branches","","0",
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 617 sectors, got 1234 (2.00x) at PC 0x7f6c7aefef90"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 617 sectors, got 1234 (2.00x) at PC 0x7f6c7aefefc0"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 617 sectors, got 1234 (2.00x) at PC 0x7f6c7aefeff0"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 617 sectors, got 1233 (2.00x) at PC 0x7f6c7aeff0b0"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 617 sectors, got 1233 (2.00x) at PC 0x7f6c7aeff0c0"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 617 sectors, got 1233 (2.00x) at PC 0x7f6c7aeff0d0"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 616 sectors, got 1232 (2.00x) at PC 0x7f6c7aeff100"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 616 sectors, got 1232 (2.00x) at PC 0x7f6c7aeff130"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 616 sectors, got 1232 (2.00x) at PC 0x7f6c7aeff140"
"36","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:42:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 616 sectors, got 1232 (2.00x) at PC 0x7f6c7aeff1a0"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,471,298,237.66",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,391,892,667.92",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","1,572,020",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","Memory [%]","%","0.31",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","SOL DRAM","%","0.15",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","Duration","nsecond","1,129,408",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","25.39",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.07",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","SM Active Cycles","cycle","19,158.85",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","GPU Speed Of Light","SM [%]","%","0.31",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.42",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.52",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.42",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Compute Workload Analysis","SM Busy","%","20.11",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,350,031,166.77",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","Mem Busy","%","0.19",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","Max Bandwidth","%","0.31",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","79.15",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","L2 Hit Rate","%","40.85",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.31",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Scheduler Statistics","One or More Eligible","%","21.04",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Scheduler Statistics","No Eligible","%","78.96",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.75",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.75",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 2.6 cycles being stalled waiting for a math execution pipeline to be available. This represents about 55.5% of the total average of 4.8 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2,015.26",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Instruction Statistics","Executed Instructions","inst","661,006",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2,015.38",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Instruction Statistics","Issued Instructions","inst","661,046",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","NVLink","Logical Links","","0",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","NVLink","Physical Links","","0",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Block Size","","64",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Grid Size","","1",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Threads","thread","64",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Launch Statistics","Waves Per SM","","0.01",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Block Limit SM","block","16",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Block Limit Registers","block","6",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Block Limit Shared Mem","block","1",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Block Limit Warps","block","24",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Achieved Occupancy","%","4.17",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Source Counters","Branch Instructions","inst","1,254",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Source Counters","Branch Efficiency","%","100",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","Source Counters","Avg. Divergent Branches","","0",
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0f630"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0f640"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0f650"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0f660"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0fb80"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0fb90"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0fbc0"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af0fbe0"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af10360"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2456 sectors, got 9824 (4.00x) at PC 0x7f6c7af10370"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0f630"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0f640"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0f650"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0f660"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0fb80"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0fb90"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0fbc0"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af0fbe0"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af10360"
"37","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:42:51","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1228 sectors, got 3684 (3.00x) at PC 0x7f6c7af10370"
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,602,150,537.63",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,262,500,000",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,272",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","Memory [%]","%","39.02",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","SOL DRAM","%","39.02",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","Duration","nsecond","4,960",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","18.36",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","SOL L2 Cache","%","30.60",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,952.33",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","GPU Speed Of Light","SM [%]","%","4.46",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.24",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.15",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Compute Workload Analysis","Issue Slots Busy","%","7.07",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.28",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Compute Workload Analysis","SM Busy","%","7.07",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","Memory Throughput","byte/second","322,270,967,741.94",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","Mem Busy","%","30.60",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","Max Bandwidth","%","39.02",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","L2 Hit Rate","%","47.72",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.84",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Scheduler Statistics","One or More Eligible","%","7.52",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.08",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Scheduler Statistics","No Eligible","%","92.48",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.47",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 13.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.47 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","59.41",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","68.93",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.00",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.99",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 44.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 74.0% of the total average of 59.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","240.74",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Instruction Statistics","Executed Instructions","inst","78,962",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","279.34",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Instruction Statistics","Issued Instructions","inst","91,624",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","NVLink","Logical Links","","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","NVLink","Physical Links","","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Block Size","","64",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Grid Size","","1,233",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Threads","thread","78,912",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Launch Statistics","Waves Per SM","","0.94",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Block Limit SM","block","16",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Block Limit Registers","block","42",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Block Limit Shared Mem","block","100",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Block Limit Warps","block","24",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Achieved Occupancy","%","35.40",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Occupancy","Achieved Active Warps Per SM","warp","16.99",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Source Counters","Branch Instructions","inst","4,946",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Source Counters","Branch Efficiency","%","99.96",
"38","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:55","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,013,400,335.01",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,166,726,489.59",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","7,442",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","Memory [%]","%","51.74",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","SOL DRAM","%","51.74",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","Duration","nsecond","6,368",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.90",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","SOL L2 Cache","%","34.02",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","SM Active Cycles","cycle","5,373.05",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","GPU Speed Of Light","SM [%]","%","3.24",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.15",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.11",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.23",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Compute Workload Analysis","SM Busy","%","4.23",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","Memory Throughput","byte/second","398,050,251,256.28",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","Mem Busy","%","34.02",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","Max Bandwidth","%","51.74",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","L2 Hit Rate","%","35.13",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.24",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Scheduler Statistics","One or More Eligible","%","4.34",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Scheduler Statistics","No Eligible","%","95.66",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.48",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.48 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","103.19",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","115.38",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.81",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 85.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 83.0% of the total average of 103.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","203.15",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Instruction Statistics","Executed Instructions","inst","66,632",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","227.15",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Instruction Statistics","Issued Instructions","inst","74,504",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","NVLink","Logical Links","","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","NVLink","Physical Links","","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Block Size","","64",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Grid Size","","1,233",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Threads","thread","78,912",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Launch Statistics","Waves Per SM","","0.94",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Block Limit SM","block","16",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Block Limit Registers","block","42",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Block Limit Shared Mem","block","100",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Block Limit Warps","block","24",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Achieved Occupancy","%","34.90",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Occupancy","Achieved Active Warps Per SM","warp","16.75",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Source Counters","Branch Instructions","inst","4,946",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Source Counters","Branch Efficiency","%","99.96",
"39","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:42:59","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,073,394,495.41",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,183,158,584.53",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,132",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","Memory [%]","%","21.14",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","SOL DRAM","%","0.62",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","Duration","nsecond","3,488",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","29.79",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","SOL L2 Cache","%","21.14",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,627.74",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","GPU Speed Of Light","SM [%]","%","3.54",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.12",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.98",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Compute Workload Analysis","SM Busy","%","8.98",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,844,036,697.25",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","Mem Busy","%","21.14",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","Max Bandwidth","%","20.67",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","L2 Hit Rate","%","100.01",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.92",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Scheduler Statistics","One or More Eligible","%","9.87",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.10",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Scheduler Statistics","No Eligible","%","90.13",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.50",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.50 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.31",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","28.95",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.00",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.3% of the total average of 25.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","127.83",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Instruction Statistics","Executed Instructions","inst","41,929",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","146.20",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Instruction Statistics","Issued Instructions","inst","47,955",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","NVLink","Logical Links","","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","NVLink","Physical Links","","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Block Size","","64",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Grid Size","","1,233",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Threads","thread","78,912",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Launch Statistics","Waves Per SM","","0.94",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Block Limit SM","block","16",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Block Limit Registers","block","64",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Block Limit Shared Mem","block","100",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Block Limit Warps","block","24",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Achieved Occupancy","%","19.64",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Occupancy","Achieved Active Warps Per SM","warp","9.43",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Source Counters","Branch Instructions","inst","4,935",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Source Counters","Branch Efficiency","%","100",
"40","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:02","1","7","Source Counters","Avg. Divergent Branches","","0",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,496,825,396.83",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,392,087,850.77",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","149,766",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","Memory [%]","%","10.34",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","SOL DRAM","%","2.44",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","Duration","nsecond","107,520",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.47",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.44",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","SM Active Cycles","cycle","75,643.54",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","GPU Speed Of Light","SM [%]","%","13.19",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.53",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Compute Workload Analysis","Issue Slots Busy","%","26.10",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.04",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Compute Workload Analysis","SM Busy","%","26.10",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","Memory Throughput","byte/second","22,229,761,904.76",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","Mem Busy","%","8.61",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","Max Bandwidth","%","10.34",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","66.87",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.18",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Memory Workload Analysis","Mem Pipes Busy","%","10.34",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Scheduler Statistics","One or More Eligible","%","26.03",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Scheduler Statistics","No Eligible","%","73.97",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.38",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.39",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.38 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.05",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.19",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.83",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.78",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 25.4 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 70.4% of the total average of 36.0 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","19,664.76",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Instruction Statistics","Executed Instructions","inst","6,450,040",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19,740.49",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Instruction Statistics","Issued Instructions","inst","6,474,882",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","NVLink","Logical Links","","0",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","NVLink","Physical Links","","0",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Block Size","","256",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Grid Size","","1,233",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Threads","thread","315,648",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Launch Statistics","Waves Per SM","","2.51",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","LaunchStats","","","","LaunchConfiguration","WRN","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 249 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 21.5%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid."
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Block Limit SM","block","16",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Block Limit Registers","block","6",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Block Limit Shared Mem","block","47",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Block Limit Warps","block","6",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Theoretical Occupancy","%","100",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Achieved Occupancy","%","78.46",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Occupancy","Achieved Active Warps Per SM","warp","37.66",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Source Counters","Branch Instructions Ratio","%","0.24",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Source Counters","Branch Instructions","inst","1,519,899",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Source Counters","Branch Efficiency","%","94.54",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","Source Counters","Avg. Divergent Branches","","136.23",
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 176280 sectors, got 350276 (1.99x) at PC 0x7f6c9a130a00"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 130687 sectors, got 157521 (1.21x) at PC 0x7f6c9a130720"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307c0"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307f0"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b40"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b60"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b80"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130ba0"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130be0"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c00"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c20"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c30"
"41","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:43:06","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 29458 sectors, got 29499 (1.00x) at PC 0x7f6c9a130890"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,232,715,770.45",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,346,683,154.74",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","95,525",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","Memory [%]","%","60.40",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","SOL DRAM","%","60.40",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","Duration","nsecond","70,816",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","44.15",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","SOL L2 Cache","%","40.04",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","SM Active Cycles","cycle","88,366.41",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","GPU Speed Of Light","SM [%]","%","19.34",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis report section to see where the memory system bottleneck is. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or whether there are values you can (re)compute."
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.83",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.77",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Compute Workload Analysis","Issue Slots Busy","%","20.87",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.83",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Compute Workload Analysis","SM Busy","%","20.87",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","Memory Throughput","byte/second","535,358,337,098.96",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","Mem Busy","%","40.04",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","Max Bandwidth","%","60.40",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","12.92",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.81",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Memory Workload Analysis","Mem Pipes Busy","%","16.69",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Scheduler Statistics","One or More Eligible","%","21.23",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Scheduler Statistics","No Eligible","%","78.77",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.74",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.75",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.12",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","18,419.41",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Instruction Statistics","Executed Instructions","inst","6,041,568",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","18,443.41",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Instruction Statistics","Issued Instructions","inst","6,049,440",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","NVLink","Logical Links","","0",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","NVLink","Physical Links","","0",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Block Size","","128",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Grid Size","","1,236",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Registers Per Thread","register/thread","148",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","73,728",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Threads","thread","158,208",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Launch Statistics","Waves Per SM","","15.07",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Block Limit SM","block","16",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Block Limit Registers","block","3",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Block Limit Shared Mem","block","1",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Block Limit Warps","block","12",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Theoretical Active Warps per SM","warp","4",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Theoretical Occupancy","%","8.33",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Achieved Occupancy","%","8.27",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.97",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Source Counters","Branch Instructions Ratio","%","0.01",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Source Counters","Branch Instructions","inst","49,440",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Source Counters","Branch Efficiency","%","100",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","Source Counters","Avg. Divergent Branches","","0",
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78928 sectors, got 88792 (1.12x) at PC 0x7f6c7acfd730"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78928 sectors, got 88792 (1.12x) at PC 0x7f6c7acfd760"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78928 sectors, got 88792 (1.12x) at PC 0x7f6c7acfd800"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 71529 sectors, got 81393 (1.14x) at PC 0x7f6c7acfd8b0"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78848 sectors, got 88704 (1.12x) at PC 0x7f6c7acfdb00"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78848 sectors, got 88704 (1.12x) at PC 0x7f6c7acfdbb0"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78848 sectors, got 88704 (1.12x) at PC 0x7f6c7acfdc90"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 71456 sectors, got 81312 (1.14x) at PC 0x7f6c7acfdd30"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78848 sectors, got 88704 (1.12x) at PC 0x7f6c7acfdfd0"
"42","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4>(cutlass_80_tensorop_s1688gemm_128x64_16x6_tn_align4::Params)","2021-Feb-26 11:43:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 78848 sectors, got 88704 (1.12x) at PC 0x7f6c7acfe0a0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,474,717,132.27",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,392,390,315.74",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","1,334,968",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","Memory [%]","%","4.67",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","SOL DRAM","%","4.67",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","Duration","nsecond","958,752",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","21.02",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.40",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","SM Active Cycles","cycle","129,695.73",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","GPU Speed Of Light","SM [%]","%","2.31",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.40",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.02",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.40",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Compute Workload Analysis","SM Busy","%","23.76",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","Memory Throughput","byte/second","42,508,861,519.98",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","Mem Busy","%","3.40",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","Max Bandwidth","%","4.67",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","L2 Hit Rate","%","66.24",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.04",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Scheduler Statistics","One or More Eligible","%","20.05",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.20",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Scheduler Statistics","No Eligible","%","79.95",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.20",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.20 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.99",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.99",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.93",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 3.3 cycles being stalled waiting for a math execution pipeline to be available. This represents about 66.1% of the total average of 5.0 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12,988.54",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Instruction Statistics","Executed Instructions","inst","4,260,240",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12,989.61",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Instruction Statistics","Issued Instructions","inst","4,260,592",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","NVLink","Logical Links","","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","NVLink","Physical Links","","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Block Size","","64",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Grid Size","","8",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Registers Per Thread","register/thread","156",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Threads","thread","512",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Launch Statistics","Waves Per SM","","0.10",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Block Limit SM","block","16",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Block Limit Registers","block","6",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Block Limit Shared Mem","block","1",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Block Limit Warps","block","24",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Achieved Occupancy","%","4.17",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Source Counters","Branch Instructions","inst","10,032",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Source Counters","Branch Efficiency","%","100",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","Source Counters","Avg. Divergent Branches","","0",
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 157184 sectors, got 235776 (1.50x) at PC 0x7f6c7aa528d0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 157184 sectors, got 235776 (1.50x) at PC 0x7f6c7aa52f50"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 149816 sectors, got 225952 (1.51x) at PC 0x7f6c7aa521c0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 149816 sectors, got 225952 (1.51x) at PC 0x7f6c7aa528c0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 149816 sectors, got 225952 (1.51x) at PC 0x7f6c7aa52e50"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 149816 sectors, got 225952 (1.51x) at PC 0x7f6c7aa52f70"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 484 sectors, got 752 (1.55x) at PC 0x7f6c7aa50e20"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 484 sectors, got 752 (1.55x) at PC 0x7f6c7aa50e80"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 484 sectors, got 752 (1.55x) at PC 0x7f6c7aa50f10"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 484 sectors, got 752 (1.55x) at PC 0x7f6c7aa50f50"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 38068 sectors, got 108064 (2.84x) at PC 0x7f6c7aa52f70"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 38068 sectors, got 108064 (2.84x) at PC 0x7f6c7aa528c0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 39296 sectors, got 108064 (2.75x) at PC 0x7f6c7aa52f50"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 39296 sectors, got 108064 (2.75x) at PC 0x7f6c7aa528d0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 38068 sectors, got 97012 (2.55x) at PC 0x7f6c7aa521c0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 38068 sectors, got 97012 (2.55x) at PC 0x7f6c7aa52e50"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 19648 sectors, got 39296 (2.00x) at PC 0x7f6c7aa530a0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 19648 sectors, got 39296 (2.00x) at PC 0x7f6c7aa529a0"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 126 sectors, got 240 (1.90x) at PC 0x7f6c7aa50e80"
"43","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align4::Params)","2021-Feb-26 11:43:15","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 126 sectors, got 240 (1.90x) at PC 0x7f6c7aa50f50"
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,821,705,426.36",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","SM Frequency","cycle/second","997,664,036.54",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,747",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","Memory [%]","%","1.86",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","Duration","nsecond","2,752",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.95",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.86",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","SM Active Cycles","cycle","326.90",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","GPU Speed Of Light","SM [%]","%","0.16",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.32",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Compute Workload Analysis","SM Busy","%","1.32",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","Memory Throughput","byte/second","93,023,255.81",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","Mem Busy","%","1.86",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","Max Bandwidth","%","1.15",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.80",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.11",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Scheduler Statistics","One or More Eligible","%","2.73",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Scheduler Statistics","No Eligible","%","97.27",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 36.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.61",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.36",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 30.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 83.2% of the total average of 36.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.32",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Instruction Statistics","Executed Instructions","inst","1,090",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.30",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Instruction Statistics","Issued Instructions","inst","1,410",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","NVLink","Logical Links","","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","NVLink","Physical Links","","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Block Size","","64",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Grid Size","","32",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Threads","thread","2,048",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Launch Statistics","Waves Per SM","","0.02",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Block Limit SM","block","16",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Block Limit Registers","block","64",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Block Limit Shared Mem","block","100",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Block Limit Warps","block","24",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Achieved Occupancy","%","4.35",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.09",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Source Counters","Branch Instructions","inst","130",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Source Counters","Branch Efficiency","%","100",
"44","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:19","1","7","Source Counters","Avg. Divergent Branches","","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,776,470,588.24",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","SM Frequency","cycle/second","999,422,268.91",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,720",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","Memory [%]","%","1.83",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","Duration","nsecond","2,720",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.81",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.83",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","SM Active Cycles","cycle","336.72",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","GPU Speed Of Light","SM [%]","%","0.16",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.28",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Compute Workload Analysis","SM Busy","%","1.28",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","Memory Throughput","byte/second","141,176,470.59",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","Mem Busy","%","1.83",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","Max Bandwidth","%","1.11",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.79",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.11",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Scheduler Statistics","One or More Eligible","%","2.61",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Scheduler Statistics","No Eligible","%","97.39",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 38.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.32",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","49.57",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.6% of the total average of 38.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.32",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Instruction Statistics","Executed Instructions","inst","1,090",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.30",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Instruction Statistics","Issued Instructions","inst","1,410",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","NVLink","Logical Links","","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","NVLink","Physical Links","","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Block Size","","64",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Grid Size","","32",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Threads","thread","2,048",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Launch Statistics","Waves Per SM","","0.02",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Block Limit SM","block","16",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Block Limit Registers","block","64",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Block Limit Shared Mem","block","100",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Block Limit Warps","block","24",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Achieved Occupancy","%","4.03",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.94",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Source Counters","Branch Instructions","inst","130",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Source Counters","Branch Efficiency","%","100",
"45","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:43:22","1","7","Source Counters","Avg. Divergent Branches","","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,969,348,659.00",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,178,622,742.20",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,283",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","Memory [%]","%","3.02",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","SOL DRAM","%","3.02",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","Duration","nsecond","2,784",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.44",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.91",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","SM Active Cycles","cycle","548.77",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","GPU Speed Of Light","SM [%]","%","0.19",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.01",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Compute Workload Analysis","SM Busy","%","1.01",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","Memory Throughput","byte/second","23,080,459,770.11",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","Mem Busy","%","2.91",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","Max Bandwidth","%","3.02",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","L2 Hit Rate","%","56.21",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.19",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Scheduler Statistics","One or More Eligible","%","2.06",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Scheduler Statistics","No Eligible","%","97.94",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 48.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","48.79",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","59.28",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.59",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 23.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.3% of the total average of 48.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 18.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.5% of the total average of 48.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4.57",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Instruction Statistics","Executed Instructions","inst","1,498",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5.55",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Instruction Statistics","Issued Instructions","inst","1,820",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","NVLink","Logical Links","","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","NVLink","Physical Links","","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Block Size","","64",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Grid Size","","32",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Registers Per Thread","register/thread","20",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Threads","thread","2,048",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Launch Statistics","Waves Per SM","","0.02",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Block Limit SM","block","16",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Block Limit Registers","block","42",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Block Limit Shared Mem","block","100",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Block Limit Warps","block","24",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Achieved Occupancy","%","4.15",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Source Counters","Branch Instructions Ratio","%","0.09",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Source Counters","Branch Instructions","inst","138",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Source Counters","Branch Efficiency","%","100",
"46","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:26","1","7","Source Counters","Avg. Divergent Branches","","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,823,529,411.76",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,002,713,585.43",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,273",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","Memory [%]","%","2.20",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","SOL DRAM","%","1.51",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","Duration","nsecond","3,264",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.26",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.20",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","SM Active Cycles","cycle","543.63",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","GPU Speed Of Light","SM [%]","%","0.15",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.91",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Compute Workload Analysis","SM Busy","%","0.91",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","Memory Throughput","byte/second","9,882,352,941.18",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","Mem Busy","%","2.20",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","Max Bandwidth","%","1.51",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.90",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.14",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Scheduler Statistics","One or More Eligible","%","1.92",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Scheduler Statistics","No Eligible","%","98.08",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 52.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","53.21",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","66.33",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.38",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 25.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.1% of the total average of 53.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 18.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 34.3% of the total average of 53.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.98",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Instruction Statistics","Executed Instructions","inst","1,306",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.96",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Instruction Statistics","Issued Instructions","inst","1,628",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","NVLink","Logical Links","","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","NVLink","Physical Links","","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Block Size","","64",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Grid Size","","32",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Threads","thread","2,048",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Launch Statistics","Waves Per SM","","0.02",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Block Limit SM","block","16",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Block Limit Registers","block","64",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Block Limit Shared Mem","block","100",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Block Limit Warps","block","24",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Achieved Occupancy","%","3.97",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.90",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Source Counters","Branch Instructions","inst","138",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Source Counters","Branch Efficiency","%","100",
"47","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:30","1","7","Source Counters","Avg. Divergent Branches","","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,850,187,265.92",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,153,240,369.18",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,286",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","Memory [%]","%","3.01",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","SOL DRAM","%","3.01",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","Duration","nsecond","2,848",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.40",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.90",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","SM Active Cycles","cycle","554.23",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","GPU Speed Of Light","SM [%]","%","0.19",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.00",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Compute Workload Analysis","SM Busy","%","1.00",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","Memory Throughput","byte/second","22,696,629,213.48",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","Mem Busy","%","2.90",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","Max Bandwidth","%","3.01",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","L2 Hit Rate","%","56.22",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.19",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Scheduler Statistics","One or More Eligible","%","2.08",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Scheduler Statistics","No Eligible","%","97.92",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 48.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","49.35",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","59.96",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.59",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 23.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 46.8% of the total average of 49.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 18.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 38.3% of the total average of 49.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4.57",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Instruction Statistics","Executed Instructions","inst","1,498",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5.55",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Instruction Statistics","Issued Instructions","inst","1,820",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","NVLink","Logical Links","","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","NVLink","Physical Links","","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Block Size","","64",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Grid Size","","32",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Registers Per Thread","register/thread","20",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Threads","thread","2,048",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Launch Statistics","Waves Per SM","","0.02",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Block Limit SM","block","16",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Block Limit Registers","block","42",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Block Limit Shared Mem","block","100",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Block Limit Warps","block","24",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Achieved Occupancy","%","4.10",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.97",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Source Counters","Branch Instructions Ratio","%","0.09",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Source Counters","Branch Instructions","inst","138",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Source Counters","Branch Efficiency","%","100",
"48","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:43:33","1","7","Source Counters","Avg. Divergent Branches","","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,058,823,529.41",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,037,158,613.45",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,386",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","Memory [%]","%","2.09",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","SOL DRAM","%","1.48",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","Duration","nsecond","3,264",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.27",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.09",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","SM Active Cycles","cycle","541.72",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","GPU Speed Of Light","SM [%]","%","0.15",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.92",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Compute Workload Analysis","SM Busy","%","0.92",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","Memory Throughput","byte/second","10,039,215,686.27",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","Mem Busy","%","2.09",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","Max Bandwidth","%","1.48",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.37",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.14",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Scheduler Statistics","One or More Eligible","%","1.89",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Scheduler Statistics","No Eligible","%","98.11",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 52.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","52.59",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","65.56",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.38",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 25.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 48.3% of the total average of 52.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 18.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 34.5% of the total average of 52.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.98",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Instruction Statistics","Executed Instructions","inst","1,306",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.96",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Instruction Statistics","Issued Instructions","inst","1,628",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","NVLink","Logical Links","","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","NVLink","Physical Links","","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Block Size","","64",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Grid Size","","32",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Threads","thread","2,048",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Launch Statistics","Waves Per SM","","0.02",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Block Limit SM","block","16",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Block Limit Registers","block","64",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Block Limit Shared Mem","block","100",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Block Limit Warps","block","24",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Achieved Occupancy","%","3.95",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.90",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Source Counters","Branch Instructions","inst","138",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Source Counters","Branch Efficiency","%","100",
"49","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:37","1","7","Source Counters","Avg. Divergent Branches","","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,970,695,970.70",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,170,771,193.09",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,410",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","Memory [%]","%","2.90",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","SOL DRAM","%","2.90",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","Duration","nsecond","2,912",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.16",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.76",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","SM Active Cycles","cycle","586.07",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","GPU Speed Of Light","SM [%]","%","0.23",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.24",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Compute Workload Analysis","SM Busy","%","1.24",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","Memory Throughput","byte/second","22,197,802,197.80",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","Mem Busy","%","2.76",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","Max Bandwidth","%","2.90",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","L2 Hit Rate","%","55.63",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.23",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Scheduler Statistics","One or More Eligible","%","2.56",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Scheduler Statistics","No Eligible","%","97.44",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 39.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","39.28",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","48.38",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.91",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 19.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 48.3% of the total average of 39.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 13.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 34.3% of the total average of 39.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","5.92",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Instruction Statistics","Executed Instructions","inst","1,942",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","7.29",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Instruction Statistics","Issued Instructions","inst","2,392",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","NVLink","Logical Links","","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","NVLink","Physical Links","","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Block Size","","64",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Grid Size","","32",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Registers Per Thread","register/thread","24",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Threads","thread","2,048",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Launch Statistics","Waves Per SM","","0.02",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Block Limit SM","block","16",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Block Limit Registers","block","42",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Block Limit Shared Mem","block","100",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Block Limit Warps","block","24",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Achieved Occupancy","%","4.31",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.07",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Source Counters","Branch Instructions","inst","138",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Source Counters","Branch Efficiency","%","100",
"50","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:41","1","7","Source Counters","Avg. Divergent Branches","","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,000,000,000",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,179,594,494.05",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,624",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","Memory [%]","%","1.98",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","SOL DRAM","%","1.36",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","Duration","nsecond","3,072",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.79",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.98",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","SM Active Cycles","cycle","636.15",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","GPU Speed Of Light","SM [%]","%","0.36",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.02",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Compute Workload Analysis","SM Busy","%","2.02",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","Memory Throughput","byte/second","10,458,333,333.33",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","Mem Busy","%","1.98",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","Max Bandwidth","%","1.36",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.92",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.13",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Scheduler Statistics","One or More Eligible","%","4.13",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Scheduler Statistics","No Eligible","%","95.87",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.54",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.57",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.44",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 40.1% of the total average of 24.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.89",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Instruction Statistics","Executed Instructions","inst","3,900",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.87",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Instruction Statistics","Issued Instructions","inst","4,222",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","NVLink","Logical Links","","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","NVLink","Physical Links","","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Block Size","","64",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Grid Size","","32",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Registers Per Thread","register/thread","21",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Threads","thread","2,048",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Launch Statistics","Waves Per SM","","0.02",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Block Limit SM","block","16",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Block Limit Registers","block","42",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Block Limit Shared Mem","block","100",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Block Limit Warps","block","24",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Achieved Occupancy","%","4.14",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Source Counters","Branch Instructions","inst","652",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Source Counters","Branch Efficiency","%","100",
"51","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:45","1","7","Source Counters","Avg. Divergent Branches","","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,940,074,906.37",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,164,175,361.16",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,318",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","Memory [%]","%","2.21",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","SOL DRAM","%","1.49",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","Duration","nsecond","2,848",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.32",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.21",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","533.55",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","GPU Speed Of Light","SM [%]","%","0.15",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.93",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Compute Workload Analysis","SM Busy","%","0.93",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","11,325,842,696.63",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","Mem Busy","%","2.21",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","Max Bandwidth","%","1.49",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","71.42",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.14",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Scheduler Statistics","One or More Eligible","%","1.90",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Scheduler Statistics","No Eligible","%","98.10",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 52.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","53.18",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","66.29",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.38",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 26.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 50.5% of the total average of 53.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 18.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 34.9% of the total average of 53.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.98",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Instruction Statistics","Executed Instructions","inst","1,306",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.96",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Instruction Statistics","Issued Instructions","inst","1,628",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","NVLink","Logical Links","","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","NVLink","Physical Links","","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Block Size","","64",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Grid Size","","32",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Threads","thread","2,048",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Launch Statistics","Waves Per SM","","0.02",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Block Limit SM","block","16",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Block Limit Registers","block","64",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Block Limit Shared Mem","block","100",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Block Limit Warps","block","24",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Achieved Occupancy","%","4.18",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.01",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Source Counters","Branch Instructions","inst","138",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Source Counters","Branch Efficiency","%","100",
"52","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:49","1","7","Source Counters","Avg. Divergent Branches","","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,822,222,222.22",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,146,875,000",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,305",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","Memory [%]","%","2.18",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","SOL DRAM","%","1.49",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","Duration","nsecond","2,880",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.41",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.18",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","518.99",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","GPU Speed Of Light","SM [%]","%","0.16",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.99",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Compute Workload Analysis","SM Busy","%","0.99",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","11,155,555,555.56",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","Mem Busy","%","2.18",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","Max Bandwidth","%","1.49",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.93",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.14",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Scheduler Statistics","One or More Eligible","%","2.05",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Scheduler Statistics","No Eligible","%","97.95",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 48.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","48.93",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","60.43",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.46",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 23.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.8% of the total average of 48.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 17.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.0% of the total average of 48.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4.18",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Instruction Statistics","Executed Instructions","inst","1,370",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5.16",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Instruction Statistics","Issued Instructions","inst","1,692",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","NVLink","Logical Links","","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","NVLink","Physical Links","","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Block Size","","64",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Grid Size","","32",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Threads","thread","2,048",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Launch Statistics","Waves Per SM","","0.02",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Block Limit SM","block","16",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Block Limit Registers","block","64",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Block Limit Warps","block","24",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Achieved Occupancy","%","4.13",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Source Counters","Branch Instructions","inst","138",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Source Counters","Branch Efficiency","%","100",
"53","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:43:53","1","7","Source Counters","Avg. Divergent Branches","","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,150,943,396.23",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,191,164,083.56",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,044",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","Memory [%]","%","3.63",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","SOL DRAM","%","3.63",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","Duration","nsecond","3,392",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.43",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.90",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","SM Active Cycles","cycle","826.61",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","GPU Speed Of Light","SM [%]","%","0.54",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.10",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.62",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Compute Workload Analysis","SM Busy","%","2.62",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","Memory Throughput","byte/second","28,415,094,339.62",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","Mem Busy","%","2.90",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","Max Bandwidth","%","3.63",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","25",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","L2 Hit Rate","%","46.60",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.23",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Scheduler Statistics","One or More Eligible","%","5.33",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Scheduler Statistics","No Eligible","%","94.67",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 18.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.80",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.07",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.47",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 6.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 34.0% of the total average of 18.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","20.30",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Instruction Statistics","Executed Instructions","inst","6,660",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","21.68",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Instruction Statistics","Issued Instructions","inst","7,110",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","NVLink","Logical Links","","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","NVLink","Physical Links","","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Block Size","","64",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Grid Size","","32",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Registers Per Thread","register/thread","28",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Threads","thread","2,048",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Launch Statistics","Waves Per SM","","0.02",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Block Limit SM","block","16",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Block Limit Registers","block","32",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Block Limit Shared Mem","block","100",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Block Limit Warps","block","24",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Achieved Occupancy","%","4.14",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Source Counters","Branch Instructions","inst","1,154",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Source Counters","Branch Efficiency","%","100",
"54","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:43:56","1","7","Source Counters","Avg. Divergent Branches","","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,649,122,807.02",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,123,237,781.95",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,735",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","83.86",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.73",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.31",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Compute Workload Analysis","SM Busy","%","1.31",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","Memory Throughput","byte/second","52,631,578.95",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Scheduler Statistics","One or More Eligible","%","2.69",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Scheduler Statistics","No Eligible","%","97.31",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 37.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","37.09",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.39",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.67",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 78.8% of the total average of 37.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.11",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Instruction Statistics","Executed Instructions","inst","36",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.14",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Instruction Statistics","Issued Instructions","inst","46",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","NVLink","Logical Links","","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","NVLink","Physical Links","","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Block Size","","64",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Grid Size","","1",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Threads","thread","64",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Launch Statistics","Waves Per SM","","0.00",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Block Limit SM","block","16",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Block Limit Registers","block","64",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Block Limit Shared Mem","block","100",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Block Limit Warps","block","24",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Achieved Occupancy","%","4.12",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Source Counters","Branch Instructions","inst","6",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Source Counters","Branch Efficiency","%","100",
"55","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:00","1","7","Source Counters","Avg. Divergent Branches","","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,573,333,333.33",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,115,476,190.48",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,679",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","80.74",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.15",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.26",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Compute Workload Analysis","SM Busy","%","1.26",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","Max Bandwidth","%","0.34",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.68",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Scheduler Statistics","One or More Eligible","%","2.59",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Scheduler Statistics","No Eligible","%","97.41",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 38.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.91",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","49.72",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.67",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 31.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.0% of the total average of 38.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.11",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Instruction Statistics","Executed Instructions","inst","36",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.14",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Instruction Statistics","Issued Instructions","inst","46",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","NVLink","Logical Links","","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","NVLink","Physical Links","","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Block Size","","64",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Grid Size","","1",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Threads","thread","64",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Launch Statistics","Waves Per SM","","0.00",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Block Limit SM","block","16",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Block Limit Registers","block","64",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Block Limit Shared Mem","block","100",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Block Limit Warps","block","24",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Achieved Occupancy","%","4.20",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.01",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Source Counters","Branch Instructions","inst","6",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Source Counters","Branch Efficiency","%","100",
"56","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:03","1","7","Source Counters","Avg. Divergent Branches","","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,586,666,666.67",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","SM Frequency","cycle/second","971,294,642.86",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,111",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","Memory [%]","%","0.83",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","Duration","nsecond","3,200",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","54.95",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.83",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","SM Active Cycles","cycle","16.38",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.53",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Compute Workload Analysis","SM Busy","%","1.53",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","Memory Throughput","byte/second","160,000,000",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","Mem Busy","%","0.83",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","Max Bandwidth","%","0.29",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.12",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Scheduler Statistics","One or More Eligible","%","3.14",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Scheduler Statistics","No Eligible","%","96.86",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 31.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.90",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","37.37",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","26.29",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.91",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 16.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 51.7% of the total average of 31.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.21",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Instruction Statistics","Executed Instructions","inst","70",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.25",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Instruction Statistics","Issued Instructions","inst","82",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","NVLink","Logical Links","","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","NVLink","Physical Links","","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Block Size","","64",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Grid Size","","1",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Threads","thread","64",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Launch Statistics","Waves Per SM","","0.00",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Block Limit SM","block","16",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Block Limit Registers","block","64",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Block Limit Shared Mem","block","100",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Block Limit Warps","block","24",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Achieved Occupancy","%","4.05",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.94",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Source Counters","Branch Instructions Ratio","%","0.21",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Source Counters","Branch Instructions","inst","15",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Source Counters","Branch Efficiency","%","87.50",
"57","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:07","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,679,867,986.80",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","SM Frequency","cycle/second","980,330,622.35",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,171",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","Memory [%]","%","0.82",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","Duration","nsecond","3,232",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","56.64",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.82",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","SM Active Cycles","cycle","15.89",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.71",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Compute Workload Analysis","SM Busy","%","1.71",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","Memory Throughput","byte/second","198,019,801.98",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","Mem Busy","%","0.82",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.72",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Scheduler Statistics","One or More Eligible","%","3.41",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Scheduler Statistics","No Eligible","%","96.59",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 29.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","28.92",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.43",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","25.97",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.73",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 16.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 55.9% of the total average of 28.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.23",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Instruction Statistics","Executed Instructions","inst","77",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.27",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Instruction Statistics","Issued Instructions","inst","89",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","NVLink","Logical Links","","0",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","NVLink","Physical Links","","0",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Block Size","","64",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Grid Size","","1",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Registers Per Thread","register/thread","20",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Threads","thread","64",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Launch Statistics","Waves Per SM","","0.00",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Block Limit SM","block","16",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Block Limit Registers","block","42",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Block Limit Shared Mem","block","100",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Block Limit Warps","block","24",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Achieved Occupancy","%","4.30",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.07",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Source Counters","Branch Instructions Ratio","%","0.19",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Source Counters","Branch Instructions","inst","15",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Source Counters","Branch Efficiency","%","87.50",
"58","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:10","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,809,061,488.67",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,000,563,453.54",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,300",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","Memory [%]","%","0.79",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","Duration","nsecond","3,296",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","50.93",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.79",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","SM Active Cycles","cycle","17.67",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.41",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Compute Workload Analysis","SM Busy","%","1.41",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","Memory Throughput","byte/second","116,504,854.37",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","Mem Busy","%","0.79",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.12",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Scheduler Statistics","One or More Eligible","%","2.87",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Scheduler Statistics","No Eligible","%","97.13",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.49",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.40",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","26.29",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.91",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 16.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 48.7% of the total average of 34.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.21",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Instruction Statistics","Executed Instructions","inst","70",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.25",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Instruction Statistics","Issued Instructions","inst","82",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","NVLink","Logical Links","","0",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","NVLink","Physical Links","","0",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Block Size","","64",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Grid Size","","1",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Threads","thread","64",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Launch Statistics","Waves Per SM","","0.00",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Block Limit SM","block","16",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Block Limit Registers","block","64",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Block Limit Shared Mem","block","100",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Block Limit Warps","block","24",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Achieved Occupancy","%","4.09",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.96",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Source Counters","Branch Instructions Ratio","%","0.21",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Source Counters","Branch Instructions","inst","15",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Source Counters","Branch Efficiency","%","87.50",
"59","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:14","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,730,158,730.16",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","SM Frequency","cycle/second","990,093,537.41",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,329",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","Memory [%]","%","0.79",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","Duration","nsecond","3,360",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.30",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.79",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","SM Active Cycles","cycle","18.26",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.75",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Compute Workload Analysis","SM Busy","%","1.75",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","Memory Throughput","byte/second","190,476,190.48",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","Mem Busy","%","0.79",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.72",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Scheduler Statistics","One or More Eligible","%","3.58",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Scheduler Statistics","No Eligible","%","96.42",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 27.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","28.00",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","33.03",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","25.53",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.45",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 15.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 53.5% of the total average of 28.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.27",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Instruction Statistics","Executed Instructions","inst","89",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.32",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Instruction Statistics","Issued Instructions","inst","105",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","NVLink","Logical Links","","0",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","NVLink","Physical Links","","0",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Block Size","","64",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Grid Size","","1",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Registers Per Thread","register/thread","24",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Threads","thread","64",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Launch Statistics","Waves Per SM","","0.00",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Block Limit SM","block","16",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Block Limit Registers","block","42",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Block Limit Shared Mem","block","100",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Block Limit Warps","block","24",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Achieved Occupancy","%","4.21",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.02",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Source Counters","Branch Instructions","inst","15",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Source Counters","Branch Efficiency","%","87.50",
"60","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:18","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,680,000,000",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,123,883,928.57",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,598",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","Duration","nsecond","3,200",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","42.78",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","SM Active Cycles","cycle","21.04",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.16",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Compute Workload Analysis","SM Busy","%","2.16",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","Memory Throughput","byte/second","120,000,000",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.12",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Scheduler Statistics","One or More Eligible","%","4.39",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Scheduler Statistics","No Eligible","%","95.61",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.04",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.04 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.65",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.72",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","27.21",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.81",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 39.0% of the total average of 23.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.42",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Instruction Statistics","Executed Instructions","inst","137",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.45",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Instruction Statistics","Issued Instructions","inst","149",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","NVLink","Logical Links","","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","NVLink","Physical Links","","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Block Size","","64",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Grid Size","","1",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Registers Per Thread","register/thread","21",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Threads","thread","64",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Launch Statistics","Waves Per SM","","0.00",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Block Limit SM","block","16",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Block Limit Registers","block","42",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Block Limit Shared Mem","block","100",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Block Limit Warps","block","24",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Achieved Occupancy","%","4.14",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Source Counters","Branch Instructions Ratio","%","0.32",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Source Counters","Branch Instructions","inst","44",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Source Counters","Branch Efficiency","%","92.31",
"61","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:22","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,937,254,901.96",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,170,010,504.20",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,184",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","Memory [%]","%","0.82",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","Duration","nsecond","2,720",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","53.40",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.82",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","SM Active Cycles","cycle","16.85",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.48",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Compute Workload Analysis","SM Busy","%","1.48",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","Memory Throughput","byte/second","94,117,647.06",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","Mem Busy","%","0.82",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.04",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Scheduler Statistics","One or More Eligible","%","2.92",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Scheduler Statistics","No Eligible","%","97.08",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.59",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.34",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","26.29",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.91",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 17.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 52.7% of the total average of 33.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.21",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Instruction Statistics","Executed Instructions","inst","70",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.25",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Instruction Statistics","Issued Instructions","inst","82",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","NVLink","Logical Links","","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","NVLink","Physical Links","","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Block Size","","64",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Grid Size","","1",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Threads","thread","64",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Launch Statistics","Waves Per SM","","0.00",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Block Limit SM","block","16",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Block Limit Registers","block","64",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Block Limit Shared Mem","block","100",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Block Limit Warps","block","24",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Achieved Occupancy","%","4.25",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.04",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Source Counters","Branch Instructions Ratio","%","0.21",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Source Counters","Branch Instructions","inst","15",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Source Counters","Branch Efficiency","%","87.50",
"62","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:26","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,875,968,992.25",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,164,607,558.14",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,209",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","Memory [%]","%","0.81",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","Duration","nsecond","2,752",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","54.46",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.81",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","SM Active Cycles","cycle","16.52",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.55",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Compute Workload Analysis","SM Busy","%","1.55",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","Memory Throughput","byte/second","139,534,883.72",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","Mem Busy","%","0.81",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","L2 Hit Rate","%","202.41",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Scheduler Statistics","One or More Eligible","%","3.16",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Scheduler Statistics","No Eligible","%","96.84",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 31.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","31.69",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.97",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","26.22",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.89",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 16.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 52.4% of the total average of 31.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.22",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Instruction Statistics","Executed Instructions","inst","72",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.26",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Instruction Statistics","Issued Instructions","inst","84",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","NVLink","Logical Links","","0",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","NVLink","Physical Links","","0",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Block Size","","64",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Grid Size","","1",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Threads","thread","64",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Launch Statistics","Waves Per SM","","0.00",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Block Limit SM","block","16",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Block Limit Registers","block","64",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Block Limit Shared Mem","block","100",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Block Limit Warps","block","24",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Achieved Occupancy","%","4.15",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Source Counters","Branch Instructions Ratio","%","0.21",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Source Counters","Branch Instructions","inst","15",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Source Counters","Branch Efficiency","%","87.50",
"63","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:44:30","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,755,555,555.56",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","SM Frequency","cycle/second","993,005,952.38",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,816",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","Memory [%]","%","0.69",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","Duration","nsecond","3,840",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","37.37",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.69",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","SM Active Cycles","cycle","24.09",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.11",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.94",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.12",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Compute Workload Analysis","SM Busy","%","2.94",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","Memory Throughput","byte/second","166,666,666.67",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","Mem Busy","%","0.69",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","25",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.41",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Scheduler Statistics","One or More Eligible","%","6.04",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Scheduler Statistics","No Eligible","%","93.96",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 16.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.66",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","18.07",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","25.94",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","23.48",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 6.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 40.8% of the total average of 16.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 25.9 threads being active per cycle. This is further reduced to 23.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.65",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Instruction Statistics","Executed Instructions","inst","214",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.71",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Instruction Statistics","Issued Instructions","inst","232",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","NVLink","Logical Links","","0",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","NVLink","Physical Links","","0",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Block Size","","64",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Grid Size","","1",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Registers Per Thread","register/thread","28",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Threads","thread","64",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Launch Statistics","Waves Per SM","","0.00",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Block Limit SM","block","16",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Block Limit Registers","block","32",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Block Limit Shared Mem","block","100",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Block Limit Warps","block","24",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Achieved Occupancy","%","4.20",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.02",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Source Counters","Branch Instructions Ratio","%","0.30",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Source Counters","Branch Instructions","inst","64",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Source Counters","Branch Efficiency","%","94.44",
"64","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:44:33","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,311,475,409.84",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,366,144,613.58",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","160,108",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","Memory [%]","%","40.85",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","SOL DRAM","%","40.85",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","Duration","nsecond","117,120",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.95",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","28.79",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","146,938.71",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","GPU Speed Of Light","SM [%]","%","19.29",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.39",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.35",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.66",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.39",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Compute Workload Analysis","SM Busy","%","21.01",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","365,203,278,688.52",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","Mem Busy","%","28.79",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","Max Bandwidth","%","40.85",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","69.13",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","11.07",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Scheduler Statistics","One or More Eligible","%","19.31",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.19",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Scheduler Statistics","No Eligible","%","80.69",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.19",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","5.17",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","5.18",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.50",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 3.1 cycles being stalled waiting for a math execution pipeline to be available. This represents about 59.7% of the total average of 5.2 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14,174.43",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Instruction Statistics","Executed Instructions","inst","4,649,214",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14,198.43",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Instruction Statistics","Issued Instructions","inst","4,657,086",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","NVLink","Logical Links","","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","NVLink","Physical Links","","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Block Size","","64",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Grid Size","","309",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Registers Per Thread","register/thread","158",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Threads","thread","19,776",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Launch Statistics","Waves Per SM","","3.77",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Block Limit SM","block","16",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Block Limit Registers","block","6",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Block Limit Shared Mem","block","1",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Block Limit Warps","block","24",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Achieved Occupancy","%","4.16",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Source Counters","Branch Instructions","inst","15,450",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Source Counters","Branch Efficiency","%","100",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","Source Counters","Avg. Divergent Branches","","0",
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128180 sectors, got 192296 (1.50x) at PC 0x7f6c7aa5b090"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5b640"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5b650"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5bcf0"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5bd30"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 128128 sectors, got 192192 (1.50x) at PC 0x7f6c7aa5bd40"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80210 sectors, got 128336 (1.60x) at PC 0x7f6c7aa58f20"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80164 sectors, got 128212 (1.60x) at PC 0x7f6c7aa59000"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80080 sectors, got 128128 (1.60x) at PC 0x7f6c7aa590f0"
"65","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align4::Params)","2021-Feb-26 11:44:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 80080 sectors, got 128128 (1.60x) at PC 0x7f6c7aa59130"
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,836,257,309.94",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,143,287,907.27",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,176",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","Memory [%]","%","20.95",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","SOL DRAM","%","0.32",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","Duration","nsecond","3,648",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","30.87",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","SOL L2 Cache","%","20.95",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,574.02",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","GPU Speed Of Light","SM [%]","%","3.50",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.32",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.12",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.27",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.37",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Compute Workload Analysis","SM Busy","%","9.27",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,385,964,912.28",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","Mem Busy","%","20.95",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","Max Bandwidth","%","20.48",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","L2 Hit Rate","%","100.04",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.88",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Scheduler Statistics","One or More Eligible","%","9.96",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.10",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Scheduler Statistics","No Eligible","%","90.04",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.36",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.36 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.69",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","27.05",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.00",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.9% of the total average of 23.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","127.83",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Instruction Statistics","Executed Instructions","inst","41,929",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","145.93",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Instruction Statistics","Issued Instructions","inst","47,866",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","NVLink","Logical Links","","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","NVLink","Physical Links","","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Block Size","","64",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Grid Size","","1,233",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Threads","thread","78,912",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Launch Statistics","Waves Per SM","","0.94",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Block Limit SM","block","16",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Block Limit Registers","block","64",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Block Limit Shared Mem","block","100",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Block Limit Warps","block","24",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Achieved Occupancy","%","20.76",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Occupancy","Achieved Active Warps Per SM","warp","9.97",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Source Counters","Branch Instructions","inst","4,935",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Source Counters","Branch Efficiency","%","100",
"66","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:41","1","7","Source Counters","Avg. Divergent Branches","","0",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,555,817,610.06",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,400,062,910.63",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","152,080",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","Memory [%]","%","10.19",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","SOL DRAM","%","2.41",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","Duration","nsecond","108,544",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.38",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.38",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","SM Active Cycles","cycle","75,944.98",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","GPU Speed Of Light","SM [%]","%","12.99",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.04",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.52",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Compute Workload Analysis","Issue Slots Busy","%","25.99",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.04",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Compute Workload Analysis","SM Busy","%","25.99",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","Memory Throughput","byte/second","22,083,726,415.09",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","Mem Busy","%","8.49",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","Max Bandwidth","%","10.19",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","66.92",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","L2 Hit Rate","%","69.97",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Memory Workload Analysis","Mem Pipes Busy","%","10.19",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Scheduler Statistics","One or More Eligible","%","25.98",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Scheduler Statistics","No Eligible","%","74.02",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.35",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.39",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.35 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.01",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.15",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.83",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.78",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 25.2 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 69.9% of the total average of 36.0 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","19,664.76",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Instruction Statistics","Executed Instructions","inst","6,450,040",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19,740.50",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Instruction Statistics","Issued Instructions","inst","6,474,884",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","NVLink","Logical Links","","0",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","NVLink","Physical Links","","0",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Block Size","","256",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Grid Size","","1,233",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Threads","thread","315,648",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Launch Statistics","Waves Per SM","","2.51",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","LaunchStats","","","","LaunchConfiguration","WRN","A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 249 thread blocks. Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for up to 33.3% of the total kernel runtime with a lower occupancy of 22.0%. Try launching a grid with no partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for a grid."
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Block Limit SM","block","16",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Block Limit Registers","block","6",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Block Limit Shared Mem","block","47",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Block Limit Warps","block","6",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Theoretical Occupancy","%","100",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Achieved Occupancy","%","78.02",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Occupancy","Achieved Active Warps Per SM","warp","37.45",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Source Counters","Branch Instructions Ratio","%","0.24",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Source Counters","Branch Instructions","inst","1,519,899",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Source Counters","Branch Efficiency","%","94.54",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","Source Counters","Avg. Divergent Branches","","136.23",
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 176280 sectors, got 350276 (1.99x) at PC 0x7f6c9a130a00"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 130687 sectors, got 157521 (1.21x) at PC 0x7f6c9a130720"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307c0"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307f0"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b40"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b60"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130b80"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130ba0"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130be0"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c00"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c20"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 11474 sectors, got 22948 (2.00x) at PC 0x7f6c9a130c30"
"67","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:44:44","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 29458 sectors, got 29499 (1.00x) at PC 0x7f6c9a130890"
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,056,338,028.17",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,173,226,861.17",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,342",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","Memory [%]","%","36.13",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","SOL DRAM","%","36.13",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.45",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","31.99",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,279.32",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","GPU Speed Of Light","SM [%]","%","4.51",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details."
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.25",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.15",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","6.97",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.28",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Compute Workload Analysis","SM Busy","%","6.97",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","279,408,450,704.23",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","Mem Busy","%","31.99",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","Max Bandwidth","%","36.13",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","51.93",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","4.51",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Scheduler Statistics","One or More Eligible","%","7.18",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Scheduler Statistics","No Eligible","%","92.82",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.26",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 13.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.26 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","59.25",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","66.69",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.81",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 41.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 70.1% of the total average of 59.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","203.15",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Instruction Statistics","Executed Instructions","inst","66,632",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","228.66",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Instruction Statistics","Issued Instructions","inst","75,000",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","NVLink","Logical Links","","0",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","NVLink","Physical Links","","0",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Block Size","","64",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Grid Size","","1,233",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Threads","thread","78,912",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Launch Statistics","Waves Per SM","","0.94",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Block Limit SM","block","16",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Block Limit Registers","block","42",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Block Limit Shared Mem","block","100",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Block Limit Warps","block","24",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Achieved Occupancy","%","35.17",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","16.88",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Source Counters","Branch Instructions","inst","4,946",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Source Counters","Branch Efficiency","%","99.96",
"68","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:44:49","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,151,237,396.88",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,339,457,574.96",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","46,842",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","Memory [%]","%","47.12",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","SOL DRAM","%","4.13",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","Duration","nsecond","34,912",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","53.54",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.98",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","41,160.45",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","GPU Speed Of Light","SM [%]","%","15.73",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.42",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.37",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.54",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.42",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Compute Workload Analysis","SM Busy","%","10.54",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","36,278,643,446.38",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","Mem Busy","%","47.12",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","Max Bandwidth","%","15.73",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","75.66",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","41.36",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","15.73",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Scheduler Statistics","One or More Eligible","%","21.12",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Scheduler Statistics","No Eligible","%","78.88",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.72",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.75",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.77",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4,320.35",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Instruction Statistics","Executed Instructions","inst","1,417,074",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4,340.35",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Instruction Statistics","Issued Instructions","inst","1,423,634",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","NVLink","Logical Links","","0",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","NVLink","Physical Links","","0",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Block Size","","64",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Grid Size","","309",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Threads","thread","19,776",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Launch Statistics","Waves Per SM","","3.77",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Block Limit SM","block","16",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Block Limit Registers","block","6",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Block Limit Shared Mem","block","1",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Block Limit Warps","block","24",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Achieved Occupancy","%","4.15",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Source Counters","Branch Instructions","inst","6,180",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Source Counters","Branch Efficiency","%","100",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","Source Counters","Avg. Divergent Branches","","0",
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af19860"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af198a0"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af198d0"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1234 sectors, got 4936 (4.00x) at PC 0x7f6c7af19900"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af199d0"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af19a00"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af19a30"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1233 sectors, got 4930 (4.00x) at PC 0x7f6c7af19a60"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1232 sectors, got 4928 (4.00x) at PC 0x7f6c7af19ae0"
"69","14446","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:44:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1232 sectors, got 4928 (4.00x) at PC 0x7f6c7af19b00"
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,772,357,723.58",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,136,814,024.39",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,985",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","Memory [%]","%","6.56",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","Duration","nsecond","2,624",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.55",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.56",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","893.15",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","GPU Speed Of Light","SM [%]","%","1.03",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details."
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.11",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.44",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.14",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Compute Workload Analysis","SM Busy","%","3.44",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","195,121,951.22",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","Mem Busy","%","6.56",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","Max Bandwidth","%","5.91",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.94",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.76",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Scheduler Statistics","One or More Eligible","%","3.63",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Scheduler Statistics","No Eligible","%","96.37",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.27",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 27.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.27 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.08",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","44.79",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.98",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.10",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 26.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.4% of the total average of 35.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","24.03",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Instruction Statistics","Executed Instructions","inst","7,881",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","30.68",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Instruction Statistics","Issued Instructions","inst","10,063",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","NVLink","Logical Links","","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","NVLink","Physical Links","","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Block Size","","64",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Grid Size","","232",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Threads","thread","14,848",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Launch Statistics","Waves Per SM","","0.18",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Block Limit SM","block","16",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Block Limit Registers","block","64",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Block Limit Shared Mem","block","100",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Block Limit Warps","block","24",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Achieved Occupancy","%","10.47",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","5.03",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Source Counters","Branch Instructions","inst","929",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Source Counters","Branch Efficiency","%","100",
"70","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:44:57","1","7","Source Counters","Avg. Divergent Branches","","0",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,460,535,346.60",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,381,198,524.37",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","64,490",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","Memory [%]","%","12.94",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","SOL DRAM","%","2.63",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","Duration","nsecond","46,624",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","26.97",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.37",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","SM Active Cycles","cycle","30,902.49",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","GPU Speed Of Light","SM [%]","%","22.64",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.88",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.90",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Compute Workload Analysis","Issue Slots Busy","%","47.17",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.89",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Compute Workload Analysis","SM Busy","%","47.17",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","Memory Throughput","byte/second","23,884,694,577.90",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","Mem Busy","%","8.54",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","Max Bandwidth","%","12.94",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","87.23",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","L2 Hit Rate","%","21.36",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Memory Workload Analysis","Mem Pipes Busy","%","12.94",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Scheduler Statistics","One or More Eligible","%","46.96",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.47",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Scheduler Statistics","No Eligible","%","53.04",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","9.55",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 9.55 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.33",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.44",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","27.77",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.79",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 9.1 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 44.9% of the total average of 20.3 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","14,500.67",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Instruction Statistics","Executed Instructions","inst","4,756,221",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14,576.45",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Instruction Statistics","Issued Instructions","inst","4,781,076",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","NVLink","Logical Links","","0",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","NVLink","Physical Links","","0",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Block Size","","256",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Grid Size","","1,233",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Threads","thread","315,648",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Launch Statistics","Waves Per SM","","2.51",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Block Limit SM","block","16",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Block Limit Registers","block","6",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Block Limit Shared Mem","block","47",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Block Limit Warps","block","6",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Theoretical Occupancy","%","100",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Achieved Occupancy","%","80.35",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Occupancy","Achieved Active Warps Per SM","warp","38.57",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Source Counters","Branch Instructions Ratio","%","0.28",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Source Counters","Branch Instructions","inst","1,318,179",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Source Counters","Branch Efficiency","%","94.05",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","Source Counters","Avg. Divergent Branches","","123.43",
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 130687 sectors, got 157521 (1.21x) at PC 0x7f6c9a130720"
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307c0"
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 29491 sectors, got 54206 (1.84x) at PC 0x7f6c9a1307f0"
"71","14446","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:45:00","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 29458 sectors, got 29499 (1.00x) at PC 0x7f6c9a130890"
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,166,666,666.67",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,178,372,130.10",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,231",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","Memory [%]","%","11.91",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","SOL DRAM","%","8.75",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","Duration","nsecond","3,584",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.72",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","SOL L2 Cache","%","11.91",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","SM Active Cycles","cycle","2,139.23",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","GPU Speed Of Light","SM [%]","%","12.36",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 3% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.48",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Compute Workload Analysis","Issue Slots Busy","%","24.40",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.98",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Compute Workload Analysis","SM Busy","%","24.40",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","Memory Throughput","byte/second","68,571,428,571.43",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","Mem Busy","%","11.91",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","Max Bandwidth","%","10.86",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","51.22",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","L2 Hit Rate","%","69.86",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Memory Workload Analysis","Mem Pipes Busy","%","9.99",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Scheduler Statistics","One or More Eligible","%","24.63",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Scheduler Statistics","No Eligible","%","75.37",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.25",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.37",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.25 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","13.21",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","13.58",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.40",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 4.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 30.8% of the total average of 13.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","508.09",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Instruction Statistics","Executed Instructions","inst","166,653",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","522.00",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Instruction Statistics","Issued Instructions","inst","171,215",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","NVLink","Logical Links","","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","NVLink","Physical Links","","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Block Size","","128",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Grid Size","","309",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Registers Per Thread","register/thread","17",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Threads","thread","39,552",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Launch Statistics","Waves Per SM","","0.31",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Block Limit SM","block","16",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Block Limit Registers","block","21",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Block Limit Shared Mem","block","100",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Block Limit Warps","block","12",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Theoretical Occupancy","%","100",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Achieved Occupancy","%","27.33",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Occupancy","Achieved Active Warps Per SM","warp","13.12",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Source Counters","Branch Instructions Ratio","%","0.03",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Source Counters","Branch Instructions","inst","4,935",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Source Counters","Branch Efficiency","%","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","Source Counters","Avg. Divergent Branches","","0",
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b82b80"
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3697 sectors, got 7394 (2.00x) at PC 0x7f6ca3b83250"
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3698 sectors, got 7394 (2.00x) at PC 0x7f6ca3b82b30"
"72","14446","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 2, true>(float*, float const*, int, int, int)","2021-Feb-26 11:45:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 3698 sectors, got 7394 (2.00x) at PC 0x7f6ca3b83040"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,056,338,028.17",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,184,576,207.24",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,384",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","Memory [%]","%","0.75",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","SOL DRAM","%","0.58",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.41",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.75",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","SM Active Cycles","cycle","115.02",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","GPU Speed Of Light","SM [%]","%","0.37",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.45",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.73",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.47",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Compute Workload Analysis","SM Busy","%","11.73",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,450,704,225.35",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","Mem Busy","%","0.75",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","Max Bandwidth","%","0.58",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.86",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.37",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Scheduler Statistics","One or More Eligible","%","12.46",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Scheduler Statistics","No Eligible","%","87.54",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.03",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.03 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.28",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.89",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.62",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.5% of the total average of 16.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","NVLink","Logical Links","","0",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","NVLink","Physical Links","","0",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Block Size","","256",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Grid Size","","3",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Threads","thread","768",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Launch Statistics","Waves Per SM","","0.01",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Block Limit SM","block","16",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Block Limit Registers","block","8",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Block Limit Shared Mem","block","88",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Block Limit Warps","block","6",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Theoretical Occupancy","%","100",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Achieved Occupancy","%","15.74",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.56",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Source Counters","Branch Instructions","inst","267",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Source Counters","Branch Efficiency","%","97.42",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"73","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:07","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,314,285,714.29",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,073,681,972.79",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,611",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","Memory [%]","%","0.71",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","Duration","nsecond","3,360",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","39.99",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.71",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.54",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Compute Workload Analysis","Issue Slots Busy","%","7.98",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.32",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Compute Workload Analysis","SM Busy","%","7.98",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","Memory Throughput","byte/second","38,095,238.10",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","Mem Busy","%","0.71",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Scheduler Statistics","One or More Eligible","%","8.46",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.08",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Scheduler Statistics","No Eligible","%","91.54",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.98",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.98 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.41",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","24.15",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 8.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 37.7% of the total average of 23.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Instruction Statistics","Executed Instructions","inst","572",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Instruction Statistics","Issued Instructions","inst","590",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","NVLink","Logical Links","","0",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","NVLink","Physical Links","","0",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Block Size","","256",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Grid Size","","1",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Threads","thread","256",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Launch Statistics","Waves Per SM","","0.00",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Block Limit SM","block","16",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Block Limit Registers","block","5",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Block Limit Shared Mem","block","88",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Block Limit Warps","block","6",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Achieved Occupancy","%","15.81",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.59",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Source Counters","Branch Instructions","inst","92",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Source Counters","Branch Efficiency","%","96.49",
"74","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:11","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,751,111,111.11",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,131,011,904.76",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,717",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","84.05",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.71",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.76",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Compute Workload Analysis","SM Busy","%","2.76",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","Memory Throughput","byte/second","106,666,666.67",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.68",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Scheduler Statistics","One or More Eligible","%","2.94",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Scheduler Statistics","No Eligible","%","97.06",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.48",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.30",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 80.1% of the total average of 34.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Instruction Statistics","Executed Instructions","inst","81",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Instruction Statistics","Issued Instructions","inst","97",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","NVLink","Logical Links","","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","NVLink","Physical Links","","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Block Size","","128",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Grid Size","","1",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Threads","thread","128",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Launch Statistics","Waves Per SM","","0.00",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Block Limit SM","block","16",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Block Limit Registers","block","32",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Block Limit Shared Mem","block","100",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Block Limit Warps","block","12",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Theoretical Occupancy","%","100",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Achieved Occupancy","%","8.12",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.90",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Source Counters","Branch Instructions","inst","5",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Source Counters","Branch Efficiency","%","0",
"75","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:14","1","7","Source Counters","Avg. Divergent Branches","","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,035,320,088.30",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,175,851,466.41",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,686",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","Memory [%]","%","1.36",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","SOL DRAM","%","0.56",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","Duration","nsecond","4,832",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.92",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,308.09",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","GPU Speed Of Light","SM [%]","%","1.36",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.16",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.27",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Compute Workload Analysis","SM Busy","%","4.27",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,291,390,728.48",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","Max Bandwidth","%","1.36",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","71.91",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","77.30",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.36",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Scheduler Statistics","One or More Eligible","%","4.27",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Scheduler Statistics","No Eligible","%","95.73",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.05",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.34",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.82",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.35",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 7.9 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 32.8% of the total average of 24.1 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","53.05",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Instruction Statistics","Executed Instructions","inst","17,401",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","55.88",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Instruction Statistics","Issued Instructions","inst","18,329",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","NVLink","Logical Links","","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","NVLink","Physical Links","","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Block Size","","128",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Grid Size","","31",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Threads","thread","3,968",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Launch Statistics","Waves Per SM","","0.04",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Block Limit SM","block","16",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Block Limit Registers","block","10",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Block Limit Shared Mem","block","16",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Block Limit Warps","block","12",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Achieved Occupancy","%","8.89",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.26",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Source Counters","Branch Instructions","inst","1,136",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Source Counters","Branch Efficiency","%","100",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","Source Counters","Avg. Divergent Branches","","0",
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"76","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,883,333,333.33",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,155,636,160.71",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,919",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","Memory [%]","%","0.47",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","Duration","nsecond","5,120",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","18.23",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.47",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","SM Active Cycles","cycle","49.38",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.13",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.41",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.14",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Compute Workload Analysis","SM Busy","%","3.41",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","Memory Throughput","byte/second","250,000,000",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","Mem Busy","%","0.47",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","6.15",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.09",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Scheduler Statistics","One or More Eligible","%","4.07",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Scheduler Statistics","No Eligible","%","95.93",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.52",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.23",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.27",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.48",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 32.5% of the total average of 24.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.58",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Instruction Statistics","Executed Instructions","inst","517",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.69",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Instruction Statistics","Issued Instructions","inst","553",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","NVLink","Logical Links","","0",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","NVLink","Physical Links","","0",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Block Size","","128",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Grid Size","","1",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Threads","thread","128",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Launch Statistics","Waves Per SM","","0.00",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Block Limit SM","block","16",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Block Limit Registers","block","16",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Block Limit Shared Mem","block","100",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Block Limit Warps","block","12",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Theoretical Occupancy","%","100",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Achieved Occupancy","%","7.04",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.38",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Source Counters","Branch Instructions Ratio","%","0.14",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Source Counters","Branch Instructions","inst","71",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Source Counters","Branch Efficiency","%","97.92",
"77","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:21","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,222,222,222.22",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","SM Frequency","cycle/second","914,445,307.11",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,060",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","Memory [%]","%","0.49",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","SOL DRAM","%","0.04",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","Duration","nsecond","6,624",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","8.82",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.49",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","SM Active Cycles","cycle","102.29",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","GPU Speed Of Light","SM [%]","%","0.06",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.13",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.69",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.15",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Compute Workload Analysis","SM Busy","%","3.69",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","Memory Throughput","byte/second","212,560,386.47",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","Mem Busy","%","0.49",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","40.14",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","L2 Hit Rate","%","96.86",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Scheduler Statistics","One or More Eligible","%","7.49",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Scheduler Statistics","No Eligible","%","92.51",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.07",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 13.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","13.42",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","14.91",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","27.76",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.36",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 41.9% of the total average of 13.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.39",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Instruction Statistics","Executed Instructions","inst","1,113",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.77",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Instruction Statistics","Issued Instructions","inst","1,237",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","NVLink","Logical Links","","0",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","NVLink","Physical Links","","0",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Block Size","","64",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Grid Size","","2",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Threads","thread","128",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Launch Statistics","Waves Per SM","","0.00",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Block Limit SM","block","16",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Block Limit Registers","block","32",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Block Limit Shared Mem","block","88",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Block Limit Warps","block","24",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Achieved Occupancy","%","4.19",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.01",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Source Counters","Branch Instructions","inst","201",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Source Counters","Branch Efficiency","%","93.38",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","Source Counters","Avg. Divergent Branches","","0.03",
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 23 (2.88x) at PC 0x7f6c96279bf0"
"78","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:45:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 15 sectors, got 23 (1.53x) at PC 0x7f6c96279b90"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,836,734,693.88",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,150,722,789.12",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,417",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","Memory [%]","%","0.74",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","SOL DRAM","%","0.56",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","Duration","nsecond","4,704",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.56",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.74",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","SM Active Cycles","cycle","114.02",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","GPU Speed Of Light","SM [%]","%","0.37",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.46",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.84",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.47",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Compute Workload Analysis","SM Busy","%","11.84",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,244,897,959.18",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","Mem Busy","%","0.74",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","Max Bandwidth","%","0.56",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.89",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.37",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Scheduler Statistics","One or More Eligible","%","12.41",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Scheduler Statistics","No Eligible","%","87.59",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.02",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.02 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.25",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.86",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.62",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.5% of the total average of 16.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","NVLink","Logical Links","","0",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","NVLink","Physical Links","","0",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Block Size","","256",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Grid Size","","3",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Threads","thread","768",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Launch Statistics","Waves Per SM","","0.01",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Block Limit SM","block","16",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Block Limit Registers","block","8",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Block Limit Shared Mem","block","88",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Block Limit Warps","block","6",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Theoretical Occupancy","%","100",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Achieved Occupancy","%","15.91",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.64",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Source Counters","Branch Instructions","inst","267",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Source Counters","Branch Efficiency","%","97.42",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"79","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:45:29","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,194,968,553.46",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,057,277,628.03",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,588",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","Duration","nsecond","3,392",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","40.67",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.16",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.12",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.32",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Compute Workload Analysis","SM Busy","%","8.12",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","Memory Throughput","byte/second","75,471,698.11",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Scheduler Statistics","One or More Eligible","%","8.66",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Scheduler Statistics","No Eligible","%","91.34",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.99",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.99 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.95",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.67",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 39.3% of the total average of 22.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Instruction Statistics","Executed Instructions","inst","572",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Instruction Statistics","Issued Instructions","inst","590",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","NVLink","Logical Links","","0",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","NVLink","Physical Links","","0",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Block Size","","256",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Grid Size","","1",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Threads","thread","256",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Launch Statistics","Waves Per SM","","0.00",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Block Limit SM","block","16",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Block Limit Registers","block","5",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Block Limit Shared Mem","block","88",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Block Limit Warps","block","6",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Achieved Occupancy","%","15.63",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.50",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Source Counters","Branch Instructions","inst","92",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Source Counters","Branch Efficiency","%","96.49",
"80","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:45:33","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,964,912,280.70",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,169,995,300.75",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,847",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","Memory [%]","%","0.91",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","83.96",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.91",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.72",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.76",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Compute Workload Analysis","SM Busy","%","2.76",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","Memory Throughput","byte/second","105,263,157.89",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","Mem Busy","%","0.91",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","Max Bandwidth","%","0.32",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Scheduler Statistics","One or More Eligible","%","2.91",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Scheduler Statistics","No Eligible","%","97.09",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.01",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.93",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.0% of the total average of 35.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Instruction Statistics","Executed Instructions","inst","81",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Instruction Statistics","Issued Instructions","inst","97",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","NVLink","Logical Links","","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","NVLink","Physical Links","","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Block Size","","128",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Grid Size","","1",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Threads","thread","128",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Launch Statistics","Waves Per SM","","0.00",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Block Limit SM","block","16",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Block Limit Registers","block","32",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Block Limit Shared Mem","block","100",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Block Limit Warps","block","12",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Theoretical Occupancy","%","100",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Achieved Occupancy","%","7.97",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.83",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Source Counters","Branch Instructions","inst","5",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Source Counters","Branch Efficiency","%","0",
"81","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:45:36","1","7","Source Counters","Avg. Divergent Branches","","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,420,289,855.07",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,085,375,998.23",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,595",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","Memory [%]","%","1.37",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","SOL DRAM","%","0.55",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","Duration","nsecond","5,152",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.92",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,298.39",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","GPU Speed Of Light","SM [%]","%","1.38",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.16",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.32",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Compute Workload Analysis","SM Busy","%","4.32",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,950,310,559.01",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","Mem Busy","%","0.97",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","Max Bandwidth","%","1.37",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","71.29",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","L2 Hit Rate","%","75.71",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.38",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Scheduler Statistics","One or More Eligible","%","4.19",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Scheduler Statistics","No Eligible","%","95.81",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.97",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.97 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.04",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","24.50",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32.01",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.44",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 10.4 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 45.4% of the total average of 23.0 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","52.70",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Instruction Statistics","Executed Instructions","inst","17,284",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","56.03",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Instruction Statistics","Issued Instructions","inst","18,377",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","NVLink","Logical Links","","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","NVLink","Physical Links","","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Block Size","","128",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Grid Size","","31",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Threads","thread","3,968",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Launch Statistics","Waves Per SM","","0.04",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Block Limit SM","block","16",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Block Limit Registers","block","10",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Block Limit Shared Mem","block","16",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Block Limit Warps","block","12",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Achieved Occupancy","%","8.36",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.01",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Source Counters","Branch Instructions","inst","1,100",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Source Counters","Branch Efficiency","%","100",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","Source Counters","Avg. Divergent Branches","","0",
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"82","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:45:39","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,580,052,493.44",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,111,818,053.99",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,520",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","Duration","nsecond","4,064",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","27.49",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","SM Active Cycles","cycle","32.74",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.09",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Compute Workload Analysis","SM Busy","%","2.09",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","Memory Throughput","byte/second","251,968,503.94",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","Max Bandwidth","%","0.20",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.64",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Scheduler Statistics","One or More Eligible","%","3.36",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Scheduler Statistics","No Eligible","%","96.64",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 29.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","29.80",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","32.71",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.60",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.55",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 12.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 41.2% of the total average of 29.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.62",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Instruction Statistics","Executed Instructions","inst","205",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.69",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Instruction Statistics","Issued Instructions","inst","225",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","NVLink","Logical Links","","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","NVLink","Physical Links","","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Block Size","","128",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Grid Size","","1",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Threads","thread","128",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Launch Statistics","Waves Per SM","","0.00",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Block Limit SM","block","16",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Block Limit Registers","block","16",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Block Limit Shared Mem","block","100",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Block Limit Warps","block","12",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Theoretical Occupancy","%","100",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Achieved Occupancy","%","5.22",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.51",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Source Counters","Branch Instructions Ratio","%","0.20",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Source Counters","Branch Instructions","inst","41",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Source Counters","Branch Efficiency","%","95.45",
"83","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:45:43","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,736,263,736.26",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,130,003,924.65",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,296",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","Memory [%]","%","0.80",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","Duration","nsecond","2,912",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.43",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.80",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","SM Active Cycles","cycle","18.21",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.74",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Compute Workload Analysis","SM Busy","%","1.74",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","Memory Throughput","byte/second","395,604,395.60",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","Mem Busy","%","0.80",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","3.12",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.40",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Scheduler Statistics","One or More Eligible","%","3.55",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Scheduler Statistics","No Eligible","%","96.45",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 28.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","27.66",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","31.27",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.26",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.96",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 13.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 50.2% of the total average of 27.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.28",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Instruction Statistics","Executed Instructions","inst","92",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.32",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Instruction Statistics","Issued Instructions","inst","104",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","NVLink","Logical Links","","0",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","NVLink","Physical Links","","0",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Block Size","","64",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Grid Size","","1",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Registers Per Thread","register/thread","26",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Threads","thread","64",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Launch Statistics","Waves Per SM","","0.00",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Block Limit SM","block","16",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Block Limit Registers","block","32",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Block Limit Shared Mem","block","100",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Block Limit Warps","block","24",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Achieved Occupancy","%","4.11",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.97",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Source Counters","Branch Instructions","inst","15",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Source Counters","Branch Efficiency","%","87.50",
"84","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:45:47","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,647,798,742.14",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,117,840,296.50",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,794",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","Memory [%]","%","0.69",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","Duration","nsecond","3,392",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","38.36",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.69",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","SM Active Cycles","cycle","23.46",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.85",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Compute Workload Analysis","SM Busy","%","1.85",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","Memory Throughput","byte/second","37,735,849.06",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","Mem Busy","%","0.69",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Scheduler Statistics","One or More Eligible","%","3.82",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Scheduler Statistics","No Eligible","%","96.18",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 26.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","26.37",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","28.81",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.63",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.32",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.40",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Instruction Statistics","Executed Instructions","inst","130",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.43",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Instruction Statistics","Issued Instructions","inst","142",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","NVLink","Logical Links","","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","NVLink","Physical Links","","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Block Size","","64",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Grid Size","","1",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Threads","thread","64",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Launch Statistics","Waves Per SM","","0.00",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Block Limit SM","block","16",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Block Limit Registers","block","32",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Block Limit Shared Mem","block","100",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Block Limit Warps","block","24",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Achieved Occupancy","%","4.09",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.96",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Source Counters","Branch Instructions Ratio","%","0.25",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Source Counters","Branch Instructions","inst","33",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Source Counters","Branch Efficiency","%","96.15",
"85","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:50","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,540,880,503.14",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","SM Frequency","cycle/second","960,944,519.32",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,891",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","Memory [%]","%","0.54",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","Duration","nsecond","5,088",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","23.87",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.54",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","SM Active Cycles","cycle","37.76",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.70",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Compute Workload Analysis","SM Busy","%","1.70",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","Memory Throughput","byte/second","125,786,163.52",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","Mem Busy","%","0.54",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","L2 Hit Rate","%","100",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Scheduler Statistics","One or More Eligible","%","6.89",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Scheduler Statistics","No Eligible","%","93.11",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.07",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 14.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","14.50",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.64",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.39",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.28",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 6.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.8% of the total average of 14.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.56",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Instruction Statistics","Executed Instructions","inst","183",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.64",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Instruction Statistics","Issued Instructions","inst","210",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","NVLink","Logical Links","","0",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","NVLink","Physical Links","","0",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Block Size","","32",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Grid Size","","1",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Threads","thread","32",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Launch Statistics","Waves Per SM","","0.00",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Block Limit SM","block","16",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Block Limit Registers","block","64",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Block Limit Shared Mem","block","88",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Block Limit Warps","block","48",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Achieved Occupancy","%","2.07",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.00",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Source Counters","Branch Instructions","inst","32",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Source Counters","Branch Efficiency","%","91.30",
"86","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:45:54","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,874,509,803.92",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,154,858,193.28",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,289",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","Memory [%]","%","2.44",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","SOL DRAM","%","0.51",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","Duration","nsecond","5,440",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.16",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.44",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,739.54",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","GPU Speed Of Light","SM [%]","%","1.92",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.05",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.38",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Compute Workload Analysis","SM Busy","%","3.22",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,858,823,529.41",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","Mem Busy","%","2.44",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","Max Bandwidth","%","2.03",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","91.55",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.78",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Scheduler Statistics","One or More Eligible","%","4.78",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Scheduler Statistics","No Eligible","%","95.22",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 20.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.92",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.61",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.95",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.46",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 43.8% of the total average of 20.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","86.18",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Instruction Statistics","Executed Instructions","inst","28,267",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","89.02",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Instruction Statistics","Issued Instructions","inst","29,200",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","NVLink","Logical Links","","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","NVLink","Physical Links","","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Block Size","","64",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Grid Size","","78",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Threads","thread","4,992",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Launch Statistics","Waves Per SM","","0.06",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Block Limit SM","block","16",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Block Limit Registers","block","32",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Block Limit Shared Mem","block","100",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Block Limit Warps","block","24",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Achieved Occupancy","%","4.25",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.04",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Source Counters","Branch Instructions","inst","7,566",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Source Counters","Branch Efficiency","%","99.98",
"87","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:45:57","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,277,958,740.50",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,218,226,500.70",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","11,971",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","Memory [%]","%","2.03",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","SOL DRAM","%","2.03",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","Duration","nsecond","9,824",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","48.66",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.09",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","SM Active Cycles","cycle","123.55",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","GPU Speed Of Light","SM [%]","%","0.17",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.60",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Compute Workload Analysis","Issue Slots Busy","%","15.66",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.63",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Compute Workload Analysis","SM Busy","%","15.66",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","Memory Throughput","byte/second","16,130,293,159.61",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","Mem Busy","%","1.09",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","Max Bandwidth","%","2.03",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.99",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","L2 Hit Rate","%","21.12",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.17",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Scheduler Statistics","One or More Eligible","%","16.06",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.16",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Scheduler Statistics","No Eligible","%","83.94",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.98",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.98 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.79",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.93",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.83",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.01",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 14.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 57.1% of the total average of 24.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","18.49",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Instruction Statistics","Executed Instructions","inst","6,066",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19.35",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Instruction Statistics","Issued Instructions","inst","6,346",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","NVLink","Logical Links","","0",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","NVLink","Physical Links","","0",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Block Size","","512",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Grid Size","","1",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Threads","thread","512",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Launch Statistics","Waves Per SM","","0.00",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Block Limit SM","block","16",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Block Limit Registers","block","4",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Block Limit Shared Mem","block","19",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Block Limit Warps","block","3",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Theoretical Occupancy","%","100",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Achieved Occupancy","%","32.89",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.79",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Source Counters","Branch Instructions","inst","641",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Source Counters","Branch Efficiency","%","99.64",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2465 sectors, got 4929 (2.00x) at PC 0x7f6ca4905240"
"88","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:01","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2465 sectors, got 4929 (2.00x) at PC 0x7f6ca4905250"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,094,562,647.75",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,189,209,726.44",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,367",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","Memory [%]","%","0.75",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","SOL DRAM","%","0.59",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","Duration","nsecond","4,512",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.85",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.75",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","SM Active Cycles","cycle","112.16",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","GPU Speed Of Light","SM [%]","%","0.37",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.46",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Compute Workload Analysis","Issue Slots Busy","%","12.03",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.48",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Compute Workload Analysis","SM Busy","%","12.03",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,567,375,886.52",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","Mem Busy","%","0.75",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","Max Bandwidth","%","0.59",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.82",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.37",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Scheduler Statistics","One or More Eligible","%","12.42",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Scheduler Statistics","No Eligible","%","87.58",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.00",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.15",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.08",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.68",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.52",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 35.9% of the total average of 16.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","NVLink","Logical Links","","0",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","NVLink","Physical Links","","0",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Block Size","","256",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Grid Size","","3",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Threads","thread","768",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Launch Statistics","Waves Per SM","","0.01",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Block Limit SM","block","16",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Block Limit Registers","block","8",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Block Limit Shared Mem","block","88",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Block Limit Warps","block","6",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Theoretical Occupancy","%","100",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Achieved Occupancy","%","16.53",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.93",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Source Counters","Branch Instructions","inst","267",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Source Counters","Branch Efficiency","%","97.42",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"89","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,220,125,786.16",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,056,477,425.88",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,587",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","Duration","nsecond","3,392",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","40.67",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.16",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.12",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.32",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Compute Workload Analysis","SM Busy","%","8.12",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","Memory Throughput","byte/second","37,735,849.06",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Scheduler Statistics","One or More Eligible","%","8.61",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Scheduler Statistics","No Eligible","%","91.39",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.96",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.96 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.81",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.53",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 39.6% of the total average of 22.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Instruction Statistics","Executed Instructions","inst","572",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Instruction Statistics","Issued Instructions","inst","590",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","NVLink","Logical Links","","0",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","NVLink","Physical Links","","0",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Block Size","","256",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Grid Size","","1",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Threads","thread","256",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Launch Statistics","Waves Per SM","","0.00",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Block Limit SM","block","16",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Block Limit Registers","block","5",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Block Limit Shared Mem","block","88",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Block Limit Warps","block","6",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Achieved Occupancy","%","15.71",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.54",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Source Counters","Branch Instructions","inst","92",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Source Counters","Branch Efficiency","%","96.49",
"90","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:08","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,680,000,000",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,132,916,666.67",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,720",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","84.93",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.60",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.79",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Compute Workload Analysis","SM Busy","%","2.79",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","Memory Throughput","byte/second","106,666,666.67",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.52",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Scheduler Statistics","One or More Eligible","%","2.90",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Scheduler Statistics","No Eligible","%","97.10",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.99",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.70",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 31.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 92.3% of the total average of 34.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Instruction Statistics","Executed Instructions","inst","81",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Instruction Statistics","Issued Instructions","inst","97",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","NVLink","Logical Links","","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","NVLink","Physical Links","","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Block Size","","128",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Grid Size","","1",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Threads","thread","128",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Launch Statistics","Waves Per SM","","0.00",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Block Limit SM","block","16",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Block Limit Registers","block","32",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Block Limit Shared Mem","block","100",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Block Limit Warps","block","12",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Theoretical Occupancy","%","100",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Achieved Occupancy","%","9.09",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.36",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Source Counters","Branch Instructions","inst","5",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Source Counters","Branch Efficiency","%","0",
"91","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:12","1","7","Source Counters","Avg. Divergent Branches","","0",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,949,367,088.61",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,174,559,222.42",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,942",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","Memory [%]","%","1.32",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","SOL DRAM","%","0.53",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","Duration","nsecond","5,056",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.01",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,562.88",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","GPU Speed Of Light","SM [%]","%","1.32",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.14",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.63",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.15",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Compute Workload Analysis","SM Busy","%","3.63",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,025,316,455.70",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","Max Bandwidth","%","1.32",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","68.29",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","L2 Hit Rate","%","76.38",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.32",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Scheduler Statistics","One or More Eligible","%","4.19",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Scheduler Statistics","No Eligible","%","95.81",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.38",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.90",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.90",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.32",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 9.2 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 37.7% of the total average of 24.4 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","53.34",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Instruction Statistics","Executed Instructions","inst","17,496",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","56.68",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Instruction Statistics","Issued Instructions","inst","18,591",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","NVLink","Logical Links","","0",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","NVLink","Physical Links","","0",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Block Size","","128",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Grid Size","","31",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Threads","thread","3,968",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Launch Statistics","Waves Per SM","","0.04",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Block Limit SM","block","16",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Block Limit Registers","block","10",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Block Limit Shared Mem","block","16",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Block Limit Warps","block","12",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Achieved Occupancy","%","7.39",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.55",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Source Counters","Branch Instructions","inst","1,165",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Source Counters","Branch Efficiency","%","99.85",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"92","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,142,857,142.86",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,198,102,678.57",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,591",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","Memory [%]","%","0.47",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","SOL DRAM","%","0.18",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","Duration","nsecond","7,168",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.85",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.47",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","SM Active Cycles","cycle","233.65",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","GPU Speed Of Light","SM [%]","%","0.14",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.19",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.97",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Compute Workload Analysis","SM Busy","%","4.97",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,410,714,285.71",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","Mem Busy","%","0.47",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","Max Bandwidth","%","0.21",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","13.13",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","L2 Hit Rate","%","83.31",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.07",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Scheduler Statistics","One or More Eligible","%","5.11",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Scheduler Statistics","No Eligible","%","94.89",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.59",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.16",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.92",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.67",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 50.2% of the total average of 19.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.29",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Instruction Statistics","Executed Instructions","inst","3,702",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","11.62",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Instruction Statistics","Issued Instructions","inst","3,810",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","NVLink","Logical Links","","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","NVLink","Physical Links","","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Block Size","","128",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Grid Size","","3",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Threads","thread","384",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Launch Statistics","Waves Per SM","","0.00",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Block Limit SM","block","16",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Block Limit Registers","block","16",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Block Limit Shared Mem","block","100",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Block Limit Warps","block","12",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Theoretical Occupancy","%","100",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Achieved Occupancy","%","8.18",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.93",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Source Counters","Branch Instructions","inst","424",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Source Counters","Branch Efficiency","%","100",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","Source Counters","Avg. Divergent Branches","","0",
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 48 sectors, got 60 (1.25x) at PC 0x7f6cae099220"
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 48 sectors, got 60 (1.25x) at PC 0x7f6cae09a650"
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 48 sectors, got 60 (1.25x) at PC 0x7f6cae09ba70"
"93","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:18","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 44 sectors, got 54 (1.23x) at PC 0x7f6cae09cf00"
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,509,090,909.09",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","SM Frequency","cycle/second","960,470,779.22",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,764",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","Memory [%]","%","0.70",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","SOL DRAM","%","0.24",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","Duration","nsecond","7,040",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.21",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.70",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","SM Active Cycles","cycle","115.06",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","GPU Speed Of Light","SM [%]","%","0.44",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.94",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Compute Workload Analysis","Issue Slots Busy","%","25.91",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.04",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Compute Workload Analysis","SM Busy","%","25.91",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,472,727,272.73",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","Mem Busy","%","0.70",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","Max Bandwidth","%","0.34",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","39.44",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","L2 Hit Rate","%","85.55",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.17",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Scheduler Statistics","One or More Eligible","%","26.04",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Scheduler Statistics","No Eligible","%","73.96",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.94",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.44",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.94 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","15.12",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.62",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.40",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.90",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 35.0% of the total average of 15.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","27.12",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Instruction Statistics","Executed Instructions","inst","8,897",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","29.82",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Instruction Statistics","Issued Instructions","inst","9,780",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","NVLink","Logical Links","","0",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","NVLink","Physical Links","","0",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Block Size","","512",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Grid Size","","2",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Threads","thread","1,024",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Launch Statistics","Waves Per SM","","0.01",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Block Limit SM","block","16",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Block Limit Registers","block","4",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Block Limit Shared Mem","block","88",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Block Limit Warps","block","3",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Theoretical Occupancy","%","100",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Achieved Occupancy","%","32.75",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.72",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Source Counters","Branch Instructions","inst","1,601",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Source Counters","Branch Efficiency","%","94.03",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","Source Counters","Avg. Divergent Branches","","0.20",
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 63 sectors, got 188 (2.98x) at PC 0x7f6c96279bf0"
"94","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:46:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 125 sectors, got 188 (1.50x) at PC 0x7f6c96279b90"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,962,174,940.90",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,176,481,762.92",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,310",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","Memory [%]","%","0.76",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","SOL DRAM","%","0.58",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","Duration","nsecond","4,512",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.83",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.76",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","SM Active Cycles","cycle","112.33",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","GPU Speed Of Light","SM [%]","%","0.38",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.46",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Compute Workload Analysis","Issue Slots Busy","%","12.02",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.48",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Compute Workload Analysis","SM Busy","%","12.02",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,425,531,914.89",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","Mem Busy","%","0.76",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","Max Bandwidth","%","0.58",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.88",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.38",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Scheduler Statistics","One or More Eligible","%","12.51",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.13",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Scheduler Statistics","No Eligible","%","87.49",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.99",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.99 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","15.92",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.51",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.52",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.6% of the total average of 15.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","NVLink","Logical Links","","0",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","NVLink","Physical Links","","0",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Block Size","","256",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Grid Size","","3",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Threads","thread","768",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Launch Statistics","Waves Per SM","","0.01",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Block Limit SM","block","16",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Block Limit Registers","block","8",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Block Limit Shared Mem","block","88",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Block Limit Warps","block","6",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Theoretical Occupancy","%","100",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Achieved Occupancy","%","16.03",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.69",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Source Counters","Branch Instructions","inst","267",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Source Counters","Branch Efficiency","%","97.42",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"95","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:46:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,243,902,439.02",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","SM Frequency","cycle/second","920,042,102.21",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,625",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","Memory [%]","%","0.71",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","Duration","nsecond","3,936",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","41.75",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.71",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","SM Active Cycles","cycle","21.59",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.32",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.33",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.33",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Compute Workload Analysis","SM Busy","%","8.33",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","Memory Throughput","byte/second","65,040,650.41",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","Mem Busy","%","0.71",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Scheduler Statistics","One or More Eligible","%","8.72",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Scheduler Statistics","No Eligible","%","91.28",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.62",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.34",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 12.9 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 57.0% of the total average of 22.6 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 39.6% of the total average of 22.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Instruction Statistics","Executed Instructions","inst","572",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Instruction Statistics","Issued Instructions","inst","590",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","NVLink","Logical Links","","0",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","NVLink","Physical Links","","0",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Block Size","","256",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Grid Size","","1",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Threads","thread","256",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Launch Statistics","Waves Per SM","","0.00",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Block Limit SM","block","16",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Block Limit Registers","block","5",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Block Limit Shared Mem","block","88",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Block Limit Warps","block","6",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Achieved Occupancy","%","16.14",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.75",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Source Counters","Branch Instructions","inst","92",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Source Counters","Branch Efficiency","%","96.49",
"96","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:46:30","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,649,122,807.02",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,121,240,601.50",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,729",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","85.12",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.57",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.80",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Compute Workload Analysis","SM Busy","%","2.80",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","Memory Throughput","byte/second","52,631,578.95",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Scheduler Statistics","One or More Eligible","%","2.90",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Scheduler Statistics","No Eligible","%","97.10",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.03",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.75",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.3% of the total average of 34.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Instruction Statistics","Executed Instructions","inst","81",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Instruction Statistics","Issued Instructions","inst","97",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","NVLink","Logical Links","","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","NVLink","Physical Links","","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Block Size","","128",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Grid Size","","1",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Threads","thread","128",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Launch Statistics","Waves Per SM","","0.00",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Block Limit SM","block","16",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Block Limit Registers","block","32",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Block Limit Shared Mem","block","100",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Block Limit Warps","block","12",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Theoretical Occupancy","%","100",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Achieved Occupancy","%","8.21",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.94",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Source Counters","Branch Instructions","inst","5",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Source Counters","Branch Efficiency","%","0",
"97","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:46:34","1","7","Source Counters","Avg. Divergent Branches","","0",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,633,986,928.10",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,123,103,408.03",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,502",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","Memory [%]","%","1.43",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","SOL DRAM","%","0.59",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","Duration","nsecond","4,896",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.99",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.03",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,308.76",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","GPU Speed Of Light","SM [%]","%","1.43",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.16",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.33",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Compute Workload Analysis","SM Busy","%","4.33",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,313,725,490.20",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","Mem Busy","%","1.03",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","Max Bandwidth","%","1.43",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","68.25",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","L2 Hit Rate","%","75.17",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.43",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Scheduler Statistics","One or More Eligible","%","4.44",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Scheduler Statistics","No Eligible","%","95.56",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.06",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","24.54",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.36",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 8.2 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 35.6% of the total average of 23.1 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","53.26",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Instruction Statistics","Executed Instructions","inst","17,470",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","56.68",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Instruction Statistics","Issued Instructions","inst","18,591",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","NVLink","Logical Links","","0",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","NVLink","Physical Links","","0",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Block Size","","128",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Grid Size","","31",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Threads","thread","3,968",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Launch Statistics","Waves Per SM","","0.04",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Block Limit SM","block","16",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Block Limit Registers","block","10",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Block Limit Shared Mem","block","16",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Block Limit Warps","block","12",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Achieved Occupancy","%","8.13",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.90",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Source Counters","Branch Instructions","inst","1,174",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Source Counters","Branch Efficiency","%","99.85",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"98","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:46:37","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714b0"
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,454,976,303.32",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,241,938,896.41",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,388",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","Memory [%]","%","0.42",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","SOL DRAM","%","0.16",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","Duration","nsecond","6,752",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","11.66",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.42",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","SM Active Cycles","cycle","77.16",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","GPU Speed Of Light","SM [%]","%","0.04",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.17",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.43",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.18",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Compute Workload Analysis","SM Busy","%","4.49",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,308,056,872.04",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","Mem Busy","%","0.42",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","L2 Hit Rate","%","84.56",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Scheduler Statistics","One or More Eligible","%","4.48",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Scheduler Statistics","No Eligible","%","95.52",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.67",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.34",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.26",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.04",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 11.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 52.6% of the total average of 22.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.32",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Instruction Statistics","Executed Instructions","inst","1,088",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.41",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Instruction Statistics","Issued Instructions","inst","1,120",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","NVLink","Logical Links","","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","NVLink","Physical Links","","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Block Size","","128",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Grid Size","","1",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Threads","thread","128",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Launch Statistics","Waves Per SM","","0.00",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Block Limit SM","block","16",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Block Limit Registers","block","16",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Block Limit Shared Mem","block","100",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Block Limit Warps","block","12",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Theoretical Occupancy","%","100",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Achieved Occupancy","%","8.35",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.01",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Source Counters","Branch Instructions","inst","128",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Source Counters","Branch Efficiency","%","100",
"99","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:46:41","1","7","Source Counters","Avg. Divergent Branches","","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,627,240,143.37",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,128,744,239.63",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,361",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","SOL DRAM","%","0.39",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","Duration","nsecond","2,976",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","23.81",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","SM Active Cycles","cycle","37.79",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.94",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Compute Workload Analysis","SM Busy","%","1.94",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,881,720,430.11",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","Max Bandwidth","%","0.41",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.50",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","L2 Hit Rate","%","83.46",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.03",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Scheduler Statistics","One or More Eligible","%","4.00",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Scheduler Statistics","No Eligible","%","96.00",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 25.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.03",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","27.55",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.78",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.14",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.9% of the total average of 25.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 8.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 34.8% of the total average of 25.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.66",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Instruction Statistics","Executed Instructions","inst","218",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.73",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Instruction Statistics","Issued Instructions","inst","240",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","NVLink","Logical Links","","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","NVLink","Physical Links","","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Block Size","","64",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Grid Size","","2",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Registers Per Thread","register/thread","26",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Threads","thread","128",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Launch Statistics","Waves Per SM","","0.00",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Block Limit SM","block","16",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Block Limit Registers","block","32",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Block Limit Shared Mem","block","100",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Block Limit Warps","block","24",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Achieved Occupancy","%","4.10",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.97",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Source Counters","Branch Instructions","inst","26",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Source Counters","Branch Efficiency","%","100",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","Source Counters","Avg. Divergent Branches","","0",
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f6c84afc8d0"
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f6c84afc8e0"
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f6c84afc8f0"
"100","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:46:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f6c84afc900"
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,843,137,254.90",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,157,037,815.13",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,298",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","Memory [%]","%","0.46",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","Duration","nsecond","5,440",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","8.91",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.46",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","SM Active Cycles","cycle","101.04",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.28",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Compute Workload Analysis","SM Busy","%","3.09",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","Memory Throughput","byte/second","141,176,470.59",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","Mem Busy","%","0.46",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.50",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Scheduler Statistics","One or More Eligible","%","4.58",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Scheduler Statistics","No Eligible","%","95.42",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.74",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.46",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.37",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.96",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 44.2% of the total average of 21.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2.23",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Instruction Statistics","Executed Instructions","inst","733",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2.31",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Instruction Statistics","Issued Instructions","inst","757",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","NVLink","Logical Links","","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","NVLink","Physical Links","","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Block Size","","64",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Grid Size","","2",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Threads","thread","128",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Launch Statistics","Waves Per SM","","0.00",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Block Limit SM","block","16",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Block Limit Registers","block","32",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Block Limit Shared Mem","block","100",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Block Limit Warps","block","24",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Achieved Occupancy","%","4.19",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.01",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Source Counters","Branch Instructions","inst","197",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Source Counters","Branch Efficiency","%","99.42",
"101","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:48","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,329,243,353.78",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,072,770,595.97",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,597",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","Memory [%]","%","0.52",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","SOL DRAM","%","0.11",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","Duration","nsecond","5,216",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","21.33",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.52",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","SM Active Cycles","cycle","42.41",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","GPU Speed Of Light","SM [%]","%","0.10",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.48",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Compute Workload Analysis","Issue Slots Busy","%","13.05",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.52",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Compute Workload Analysis","SM Busy","%","13.05",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","Memory Throughput","byte/second","785,276,073.62",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","Mem Busy","%","0.52",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","Max Bandwidth","%","0.19",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.80",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","L2 Hit Rate","%","90.54",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.10",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Scheduler Statistics","One or More Eligible","%","13.57",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.14",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Scheduler Statistics","No Eligible","%","86.43",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.00",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.15",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","14.77",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.08",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.75",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.80",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 35.7% of the total average of 14.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","5.09",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Instruction Statistics","Executed Instructions","inst","1,668",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5.54",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Instruction Statistics","Issued Instructions","inst","1,816",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","NVLink","Logical Links","","0",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","NVLink","Physical Links","","0",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Block Size","","256",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Grid Size","","1",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","2,048",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Threads","thread","256",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Launch Statistics","Waves Per SM","","0.00",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Block Limit SM","block","16",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Block Limit Registers","block","8",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Block Limit Shared Mem","block","32",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Block Limit Warps","block","6",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Theoretical Occupancy","%","100",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Achieved Occupancy","%","16.55",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.94",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Source Counters","Branch Instructions Ratio","%","0.14",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Source Counters","Branch Instructions","inst","233",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Source Counters","Branch Efficiency","%","99.47",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 63 sectors, got 125 (1.98x) at PC 0x7f6ca4905240"
"102","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 63 sectors, got 125 (1.98x) at PC 0x7f6ca4905250"
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,126,984,126.98",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,195,312,500",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,428",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","Memory [%]","%","2.39",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","SOL DRAM","%","0.50",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","Duration","nsecond","5,376",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.11",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.39",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,731.77",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","GPU Speed Of Light","SM [%]","%","1.87",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.05",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.39",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Compute Workload Analysis","SM Busy","%","3.23",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,880,952,380.95",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","Mem Busy","%","2.39",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","Max Bandwidth","%","1.98",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","L2 Hit Rate","%","91.60",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.76",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Scheduler Statistics","One or More Eligible","%","4.81",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Scheduler Statistics","No Eligible","%","95.19",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 20.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.78",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.46",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.95",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.46",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 43.8% of the total average of 20.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","86.18",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Instruction Statistics","Executed Instructions","inst","28,267",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","89.02",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Instruction Statistics","Issued Instructions","inst","29,200",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","NVLink","Logical Links","","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","NVLink","Physical Links","","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Block Size","","64",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Grid Size","","78",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Threads","thread","4,992",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Launch Statistics","Waves Per SM","","0.06",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Block Limit SM","block","16",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Block Limit Registers","block","32",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Block Limit Shared Mem","block","100",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Block Limit Warps","block","24",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Achieved Occupancy","%","4.15",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Source Counters","Branch Instructions","inst","7,566",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Source Counters","Branch Efficiency","%","99.98",
"103","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:46:55","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,205,128,205.13",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,205,042,353.48",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","12,034",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","Memory [%]","%","2.02",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","SOL DRAM","%","2.02",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","Duration","nsecond","9,984",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","48.31",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.08",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","SM Active Cycles","cycle","124.44",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","GPU Speed Of Light","SM [%]","%","0.17",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.59",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Compute Workload Analysis","Issue Slots Busy","%","15.55",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.62",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Compute Workload Analysis","SM Busy","%","15.55",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","Memory Throughput","byte/second","15,935,897,435.90",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","Mem Busy","%","1.08",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","Max Bandwidth","%","2.02",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.99",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","L2 Hit Rate","%","21.13",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.17",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Scheduler Statistics","One or More Eligible","%","15.55",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.16",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Scheduler Statistics","No Eligible","%","84.45",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.95",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.20",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.95 active warps per scheduler, but only an average of 0.20 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.41",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.58",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.83",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.01",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 14.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 55.0% of the total average of 25.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","18.49",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Instruction Statistics","Executed Instructions","inst","6,066",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19.35",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Instruction Statistics","Issued Instructions","inst","6,346",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","NVLink","Logical Links","","0",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","NVLink","Physical Links","","0",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Block Size","","512",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Grid Size","","1",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Threads","thread","512",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Launch Statistics","Waves Per SM","","0.00",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Block Limit SM","block","16",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Block Limit Registers","block","4",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Block Limit Shared Mem","block","19",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Block Limit Warps","block","3",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Theoretical Occupancy","%","100",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Achieved Occupancy","%","32.33",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.52",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Source Counters","Branch Instructions","inst","641",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Source Counters","Branch Efficiency","%","99.64",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2465 sectors, got 4929 (2.00x) at PC 0x7f6ca4905240"
"104","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:46:59","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2465 sectors, got 4929 (2.00x) at PC 0x7f6ca4905250"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,054,421,768.71",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,185,009,718.17",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,576",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","SOL DRAM","%","0.54",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","Duration","nsecond","4,704",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.35",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","SM Active Cycles","cycle","115.41",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","GPU Speed Of Light","SM [%]","%","0.36",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.45",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.69",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.47",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Compute Workload Analysis","SM Busy","%","11.69",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,190,476,190.48",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","Max Bandwidth","%","0.54",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.88",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.36",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Scheduler Statistics","One or More Eligible","%","11.96",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Scheduler Statistics","No Eligible","%","88.04",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.50",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.12",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.40",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.7% of the total average of 16.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","NVLink","Logical Links","","0",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","NVLink","Physical Links","","0",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Block Size","","256",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Grid Size","","3",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Threads","thread","768",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Launch Statistics","Waves Per SM","","0.01",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Block Limit SM","block","16",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Block Limit Registers","block","8",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Block Limit Shared Mem","block","88",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Block Limit Warps","block","6",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Theoretical Occupancy","%","100",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Achieved Occupancy","%","16.22",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.79",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Source Counters","Branch Instructions","inst","267",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Source Counters","Branch Efficiency","%","97.42",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"105","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,320,754,716.98",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,077,998,652.29",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,658",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","Memory [%]","%","0.70",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","Duration","nsecond","3,392",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","40.85",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.70",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.06",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.32",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.15",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.33",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Compute Workload Analysis","SM Busy","%","8.15",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","Memory Throughput","byte/second","75,471,698.11",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","Mem Busy","%","0.70",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Scheduler Statistics","One or More Eligible","%","8.63",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Scheduler Statistics","No Eligible","%","91.37",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.00",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.19",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.92",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 38.6% of the total average of 23.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Instruction Statistics","Executed Instructions","inst","572",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Instruction Statistics","Issued Instructions","inst","590",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","NVLink","Logical Links","","0",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","NVLink","Physical Links","","0",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Block Size","","256",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Grid Size","","1",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Threads","thread","256",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Launch Statistics","Waves Per SM","","0.00",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Block Limit SM","block","16",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Block Limit Registers","block","5",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Block Limit Shared Mem","block","88",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Block Limit Warps","block","6",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Achieved Occupancy","%","15.70",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.54",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Source Counters","Branch Instructions","inst","92",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Source Counters","Branch Efficiency","%","96.49",
"106","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:06","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,680,000,000",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,130,357,142.86",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,714",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","84.34",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.67",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.77",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Compute Workload Analysis","SM Busy","%","2.77",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Scheduler Statistics","One or More Eligible","%","2.90",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Scheduler Statistics","No Eligible","%","97.10",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.06",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.06 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.38",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.57",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 77.4% of the total average of 36.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Instruction Statistics","Executed Instructions","inst","81",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Instruction Statistics","Issued Instructions","inst","97",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","NVLink","Logical Links","","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","NVLink","Physical Links","","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Block Size","","128",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Grid Size","","1",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Threads","thread","128",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Launch Statistics","Waves Per SM","","0.00",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Block Limit SM","block","16",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Block Limit Registers","block","32",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Block Limit Shared Mem","block","100",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Block Limit Warps","block","12",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Theoretical Occupancy","%","100",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Achieved Occupancy","%","8.03",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.85",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Source Counters","Branch Instructions","inst","5",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Source Counters","Branch Efficiency","%","0",
"107","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:09","1","7","Source Counters","Avg. Divergent Branches","","0",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,854,545,454.55",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,155,005,411.26",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,101",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","Memory [%]","%","1.30",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","SOL DRAM","%","0.52",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","Duration","nsecond","5,280",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.87",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.99",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,352.83",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","GPU Speed Of Light","SM [%]","%","1.30",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.16",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.24",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.17",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Compute Workload Analysis","SM Busy","%","4.24",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,951,515,151.52",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","Mem Busy","%","0.99",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","Max Bandwidth","%","1.30",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","66.64",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","L2 Hit Rate","%","75.95",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.30",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Scheduler Statistics","One or More Eligible","%","4.26",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Scheduler Statistics","No Eligible","%","95.74",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.61",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.12",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.48",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 9.4 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 39.9% of the total average of 23.6 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","53.88",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Instruction Statistics","Executed Instructions","inst","17,674",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","57.32",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Instruction Statistics","Issued Instructions","inst","18,801",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","NVLink","Logical Links","","0",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","NVLink","Physical Links","","0",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Block Size","","128",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Grid Size","","31",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Threads","thread","3,968",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Launch Statistics","Waves Per SM","","0.04",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Block Limit SM","block","16",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Block Limit Registers","block","10",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Block Limit Shared Mem","block","16",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Block Limit Warps","block","12",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Achieved Occupancy","%","9.01",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.33",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Source Counters","Branch Instructions","inst","1,213",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Source Counters","Branch Efficiency","%","99.70",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"108","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714d0"
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,304,414,003.04",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,220,380,789.30",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,554",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","Memory [%]","%","0.63",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","SOL DRAM","%","0.37",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","Duration","nsecond","7,008",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.23",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.63",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","SM Active Cycles","cycle","469.68",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","GPU Speed Of Light","SM [%]","%","0.27",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.19",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.95",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Compute Workload Analysis","SM Busy","%","4.95",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,977,168,949.77",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","Mem Busy","%","0.63",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","Max Bandwidth","%","0.37",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","13.36",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","L2 Hit Rate","%","75.99",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.13",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Scheduler Statistics","One or More Eligible","%","5.07",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Scheduler Statistics","No Eligible","%","94.93",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.69",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.26",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.92",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.67",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 50.6% of the total average of 19.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","22.57",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Instruction Statistics","Executed Instructions","inst","7,404",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","23.23",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Instruction Statistics","Issued Instructions","inst","7,620",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","NVLink","Logical Links","","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","NVLink","Physical Links","","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Block Size","","128",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Grid Size","","6",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Threads","thread","768",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Launch Statistics","Waves Per SM","","0.01",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Block Limit SM","block","16",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Block Limit Registers","block","16",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Block Limit Shared Mem","block","100",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Block Limit Warps","block","12",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Theoretical Occupancy","%","100",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Achieved Occupancy","%","8.10",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.89",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Source Counters","Branch Instructions","inst","848",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Source Counters","Branch Efficiency","%","100",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","Source Counters","Avg. Divergent Branches","","0",
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 120 (1.25x) at PC 0x7f6cae099220"
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 120 (1.25x) at PC 0x7f6cae09a650"
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 120 (1.25x) at PC 0x7f6cae09ba70"
"109","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:16","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 87 sectors, got 109 (1.25x) at PC 0x7f6cae09cf00"
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,542,056,074.77",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","SM Frequency","cycle/second","959,842,289.72",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,574",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","Memory [%]","%","1.04",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","SOL DRAM","%","0.49",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","Duration","nsecond","6,848",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.11",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.04",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","SM Active Cycles","cycle","230.54",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","GPU Speed Of Light","SM [%]","%","0.90",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.93",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Compute Workload Analysis","Issue Slots Busy","%","25.64",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.03",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Compute Workload Analysis","SM Busy","%","25.64",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,084,112,149.53",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","Mem Busy","%","1.04",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","Max Bandwidth","%","0.57",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","38.96",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","L2 Hit Rate","%","80.62",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.35",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Scheduler Statistics","One or More Eligible","%","25.79",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.26",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Scheduler Statistics","No Eligible","%","74.21",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.94",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.43",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.94 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","15.29",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.82",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.67",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.13",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 34.8% of the total average of 15.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","53.74",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Instruction Statistics","Executed Instructions","inst","17,627",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","59.11",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Instruction Statistics","Issued Instructions","inst","19,389",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","NVLink","Logical Links","","0",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","NVLink","Physical Links","","0",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Block Size","","512",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Grid Size","","4",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Threads","thread","2,048",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Launch Statistics","Waves Per SM","","0.02",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Block Limit SM","block","16",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Block Limit Registers","block","4",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Block Limit Shared Mem","block","88",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Block Limit Warps","block","3",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Theoretical Occupancy","%","100",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Achieved Occupancy","%","32.77",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.73",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Source Counters","Branch Instructions","inst","3,168",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Source Counters","Branch Efficiency","%","94.11",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","Source Counters","Avg. Divergent Branches","","0.39",
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 125 sectors, got 375 (3.00x) at PC 0x7f6c96279bf0"
"110","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:47:21","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 250 sectors, got 375 (1.50x) at PC 0x7f6c96279b90"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,109,589,041.10",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,197,131,849.32",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,596",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","SOL DRAM","%","0.55",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","Duration","nsecond","4,672",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","17.07",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","117.28",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","GPU Speed Of Light","SM [%]","%","0.36",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.44",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","11.51",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.46",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Compute Workload Analysis","SM Busy","%","11.51",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","4,273,972,602.74",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","Max Bandwidth","%","0.55",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","1.90",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","67.82",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.36",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Scheduler Statistics","One or More Eligible","%","11.98",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.12",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Scheduler Statistics","No Eligible","%","88.02",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.14",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 8.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.43",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.05",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.58",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.40",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 38.3% of the total average of 16.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13.01",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Instruction Statistics","Executed Instructions","inst","4,267",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.50",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Instruction Statistics","Issued Instructions","inst","4,427",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","NVLink","Logical Links","","0",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","NVLink","Physical Links","","0",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Block Size","","256",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Grid Size","","3",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Registers Per Thread","register/thread","30",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Threads","thread","768",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Launch Statistics","Waves Per SM","","0.01",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 3 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Block Limit SM","block","16",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Block Limit Registers","block","8",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Block Limit Shared Mem","block","88",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Block Limit Warps","block","6",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Theoretical Occupancy","%","100",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Achieved Occupancy","%","15.85",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.61",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Source Counters","Branch Instructions","inst","267",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Source Counters","Branch Efficiency","%","97.42",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876870"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876880"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876890"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768a0"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768b0"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768c0"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768d0"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c8768e0"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876af0"
"111","14446","python3.7","127.0.0.1","void cub::DeviceReduceKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::GridEvenShare<int>, cub::Sum)","2021-Feb-26 11:47:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f6c9c876b00"
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,212,698,412.70",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,060,374,149.66",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,565",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","Memory [%]","%","0.72",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","Duration","nsecond","3,360",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","39.99",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.72",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.54",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.31",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Compute Workload Analysis","Issue Slots Busy","%","7.98",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.32",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Compute Workload Analysis","SM Busy","%","7.98",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","Memory Throughput","byte/second","76,190,476.19",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","Mem Busy","%","0.72",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","Max Bandwidth","%","0.25",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.51",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Scheduler Statistics","One or More Eligible","%","8.38",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.08",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Scheduler Statistics","No Eligible","%","91.62",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.02",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.02 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.10",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","24.86",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.06",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.73",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 37.2% of the total average of 24.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.74",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Instruction Statistics","Executed Instructions","inst","572",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.80",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Instruction Statistics","Issued Instructions","inst","590",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","NVLink","Logical Links","","0",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","NVLink","Physical Links","","0",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Block Size","","256",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Grid Size","","1",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Registers Per Thread","register/thread","48",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Threads","thread","256",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Launch Statistics","Waves Per SM","","0.00",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Block Limit SM","block","16",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Block Limit Registers","block","5",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Block Limit Shared Mem","block","88",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Block Limit Warps","block","6",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Achieved Occupancy","%","15.73",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.55",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Source Counters","Branch Instructions","inst","92",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Source Counters","Branch Efficiency","%","96.49",
"112","14446","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, int*, int*, int, cub::Sum, int>(int*, int*, int, cub::Sum, int)","2021-Feb-26 11:47:28","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,649,122,807.02",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,119,830,827.07",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,725",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","83.96",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.72",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.76",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Compute Workload Analysis","SM Busy","%","2.76",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","Memory Throughput","byte/second","52,631,578.95",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Scheduler Statistics","One or More Eligible","%","2.90",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Scheduler Statistics","No Eligible","%","97.10",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.53",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.35",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","20.53",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 80.3% of the total average of 34.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 20.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Instruction Statistics","Executed Instructions","inst","81",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Instruction Statistics","Issued Instructions","inst","97",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","NVLink","Logical Links","","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","NVLink","Physical Links","","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Block Size","","128",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Grid Size","","1",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Threads","thread","128",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Launch Statistics","Waves Per SM","","0.00",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Block Limit SM","block","16",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Block Limit Registers","block","32",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Block Limit Shared Mem","block","100",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Block Limit Warps","block","12",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Theoretical Occupancy","%","100",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Achieved Occupancy","%","7.98",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.83",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Source Counters","Branch Instructions","inst","5",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Source Counters","Branch Efficiency","%","0",
"113","14446","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:47:31","1","7","Source Counters","Avg. Divergent Branches","","0",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,098,765,432.10",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,194,554,673.72",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,196",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","Memory [%]","%","1.28",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","SOL DRAM","%","0.51",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","Duration","nsecond","5,184",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","6.14",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,296.06",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","GPU Speed Of Light","SM [%]","%","1.28",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.17",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.43",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.18",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Compute Workload Analysis","SM Busy","%","4.43",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,950,617,283.95",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","Max Bandwidth","%","1.28",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","66.36",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","L2 Hit Rate","%","78.36",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.28",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Scheduler Statistics","One or More Eligible","%","4.35",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Scheduler Statistics","No Eligible","%","95.65",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.05",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.05 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.01",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.50",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.90",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.53",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 8.9 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 37.0% of the total average of 24.0 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","54.00",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Instruction Statistics","Executed Instructions","inst","17,713",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","57.37",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Instruction Statistics","Issued Instructions","inst","18,817",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","NVLink","Logical Links","","0",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","NVLink","Physical Links","","0",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Block Size","","128",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Grid Size","","31",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Threads","thread","3,968",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Launch Statistics","Waves Per SM","","0.04",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 31 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Block Limit SM","block","16",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Block Limit Registers","block","10",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Block Limit Shared Mem","block","16",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Block Limit Warps","block","12",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Achieved Occupancy","%","8.66",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.15",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Source Counters","Branch Instructions","inst","1,225",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Source Counters","Branch Efficiency","%","99.70",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe10"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe20"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe30"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe40"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 116 sectors, got 580 (5.00x) at PC 0x7f6c9c86fe50"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 232 sectors, got 254 (1.09x) at PC 0x7f6c9c870200"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714c0"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714e0"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714f0"
"114","14446","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:47:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f6c9c8714d0"
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,235,658,914.73",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,210,818,106.31",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,334",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","Memory [%]","%","0.53",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","SOL DRAM","%","0.30",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","Duration","nsecond","6,880",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.83",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.53",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","154.29",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.17",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.43",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.18",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Compute Workload Analysis","SM Busy","%","4.49",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,362,790,697.67",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","Mem Busy","%","0.53",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","Max Bandwidth","%","0.30",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","3.84",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","76.32",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Scheduler Statistics","One or More Eligible","%","4.45",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Scheduler Statistics","No Eligible","%","95.55",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.26",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.91",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.26",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.04",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 12.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 53.9% of the total average of 22.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6.63",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Instruction Statistics","Executed Instructions","inst","2,176",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6.83",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Instruction Statistics","Issued Instructions","inst","2,240",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","NVLink","Logical Links","","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","NVLink","Physical Links","","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Block Size","","128",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Grid Size","","2",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Threads","thread","256",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Launch Statistics","Waves Per SM","","0.00",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Block Limit SM","block","16",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Block Limit Registers","block","16",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Block Limit Shared Mem","block","100",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Block Limit Warps","block","12",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Theoretical Occupancy","%","100",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Achieved Occupancy","%","8.23",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.95",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Source Counters","Branch Instructions","inst","256",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Source Counters","Branch Efficiency","%","100",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","Source Counters","Avg. Divergent Branches","","0",
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 64 sectors, got 72 (1.12x) at PC 0x7f6cae09ed20"
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 58 sectors, got 66 (1.14x) at PC 0x7f6cae0a2a00"
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 64 sectors, got 72 (1.12x) at PC 0x7f6cae0a0150"
"115","14446","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:47:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 64 sectors, got 72 (1.12x) at PC 0x7f6cae0a1570"
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,741,935,483.87",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,141,849,078.34",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,400",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","Memory [%]","%","1.13",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","SOL DRAM","%","0.73",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","Duration","nsecond","2,976",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","12.02",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.13",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","SM Active Cycles","cycle","74.89",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","GPU Speed Of Light","SM [%]","%","0.04",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.66",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Compute Workload Analysis","SM Busy","%","1.66",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","Memory Throughput","byte/second","5,462,365,591.40",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","Mem Busy","%","1.13",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","Max Bandwidth","%","0.73",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","42.42",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","L2 Hit Rate","%","72.62",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.04",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Scheduler Statistics","One or More Eligible","%","3.40",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Scheduler Statistics","No Eligible","%","96.60",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 29.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","29.45",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","32.83",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.74",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.81",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 13.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.7% of the total average of 29.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 11.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.3% of the total average of 29.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.12",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Instruction Statistics","Executed Instructions","inst","366",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.24",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Instruction Statistics","Issued Instructions","inst","408",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","NVLink","Logical Links","","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","NVLink","Physical Links","","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Block Size","","64",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Grid Size","","4",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Registers Per Thread","register/thread","26",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Threads","thread","256",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Launch Statistics","Waves Per SM","","0.00",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Block Limit SM","block","16",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Block Limit Registers","block","32",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Block Limit Shared Mem","block","100",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Block Limit Warps","block","24",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Achieved Occupancy","%","4.12",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Source Counters","Branch Instructions Ratio","%","0.09",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Source Counters","Branch Instructions","inst","34",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Source Counters","Branch Efficiency","%","100",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","Source Counters","Avg. Divergent Branches","","0",
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f6c84afc8d0"
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f6c84afc8e0"
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f6c84afc8f0"
"116","14446","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:47:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f6c84afc900"
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,840,319,361.28",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,154,031,223.27",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,170",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","Memory [%]","%","0.54",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","Duration","nsecond","5,344",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.46",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.54",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","SM Active Cycles","cycle","201.63",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","GPU Speed Of Light","SM [%]","%","0.10",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.29",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Compute Workload Analysis","SM Busy","%","3.10",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","Memory Throughput","byte/second","239,520,958.08",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","Mem Busy","%","0.54",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","Max Bandwidth","%","0.23",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.98",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.04",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Scheduler Statistics","One or More Eligible","%","4.61",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Scheduler Statistics","No Eligible","%","95.39",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.46",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.17",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.39",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.98",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 44.9% of the total average of 21.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4.47",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Instruction Statistics","Executed Instructions","inst","1,465",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.61",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Instruction Statistics","Issued Instructions","inst","1,513",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","NVLink","Logical Links","","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","NVLink","Physical Links","","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Block Size","","64",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Grid Size","","4",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Threads","thread","256",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Launch Statistics","Waves Per SM","","0.00",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Block Limit SM","block","16",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Block Limit Registers","block","32",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Block Limit Shared Mem","block","100",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Block Limit Warps","block","24",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Achieved Occupancy","%","4.12",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Source Counters","Branch Instructions","inst","393",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Source Counters","Branch Efficiency","%","99.71",
"117","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:46","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,540,229,885.06",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,107,784,277.50",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,170",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","Memory [%]","%","0.51",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","SOL DRAM","%","0.20",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","Duration","nsecond","5,568",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","25.73",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.51",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","48.62",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","GPU Speed Of Light","SM [%]","%","0.20",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","23.95",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.96",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Compute Workload Analysis","SM Busy","%","23.95",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,471,264,367.82",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","Mem Busy","%","0.51",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","Max Bandwidth","%","0.22",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.90",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","82.95",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.20",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Scheduler Statistics","One or More Eligible","%","24.34",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Scheduler Statistics","No Eligible","%","75.66",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.88",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.36",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.88 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","15.94",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.16",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.87",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.34",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 4.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 30.7% of the total average of 15.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","10.82",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Instruction Statistics","Executed Instructions","inst","3,548",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","11.65",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Instruction Statistics","Issued Instructions","inst","3,820",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","NVLink","Logical Links","","0",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","NVLink","Physical Links","","0",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Block Size","","512",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Grid Size","","1",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Threads","thread","512",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Launch Statistics","Waves Per SM","","0.00",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Block Limit SM","block","16",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Block Limit Registers","block","4",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Block Limit Shared Mem","block","19",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Block Limit Warps","block","3",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Theoretical Occupancy","%","100",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Achieved Occupancy","%","32.24",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.47",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Source Counters","Branch Instructions Ratio","%","0.13",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Source Counters","Branch Instructions","inst","477",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Source Counters","Branch Efficiency","%","99.75",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 125 sectors, got 250 (2.00x) at PC 0x7f6ca4905240"
"118","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 125 sectors, got 250 (2.00x) at PC 0x7f6ca4905250"
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,724,137,931.03",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,128,643,267.65",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,289",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","Memory [%]","%","2.44",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","SOL DRAM","%","0.51",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","Duration","nsecond","5,568",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.16",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.44",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","3,807.59",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","GPU Speed Of Light","SM [%]","%","1.92",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.05",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.34",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Compute Workload Analysis","SM Busy","%","3.16",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,770,114,942.53",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","Mem Busy","%","2.44",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","Max Bandwidth","%","2.02",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","91.54",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.78",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Scheduler Statistics","One or More Eligible","%","4.68",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Scheduler Statistics","No Eligible","%","95.32",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.15",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.85",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.95",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.46",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 45.1% of the total average of 21.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","86.18",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Instruction Statistics","Executed Instructions","inst","28,267",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","89.02",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Instruction Statistics","Issued Instructions","inst","29,200",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","NVLink","Logical Links","","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","NVLink","Physical Links","","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Block Size","","64",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Grid Size","","78",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Threads","thread","4,992",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Launch Statistics","Waves Per SM","","0.06",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Block Limit SM","block","16",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Block Limit Registers","block","32",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Block Limit Warps","block","24",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Achieved Occupancy","%","4.17",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Source Counters","Branch Instructions","inst","7,566",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Source Counters","Branch Efficiency","%","99.98",
"119","14446","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:47:53","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,401,709,401.71",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,236,821,771.98",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","12,359",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","Memory [%]","%","1.96",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","SOL DRAM","%","1.96",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","Duration","nsecond","9,984",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.98",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.06",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","120.28",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","GPU Speed Of Light","SM [%]","%","0.17",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.62",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","16.09",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.64",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Compute Workload Analysis","SM Busy","%","16.09",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","15,846,153,846.15",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","Mem Busy","%","1.06",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","Max Bandwidth","%","1.96",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.99",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","21.09",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.17",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Scheduler Statistics","One or More Eligible","%","15.62",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.16",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Scheduler Statistics","No Eligible","%","84.38",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.93",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.20",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.93 active warps per scheduler, but only an average of 0.20 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.13",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.29",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.83",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.01",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 14.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 55.6% of the total average of 25.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","18.49",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Instruction Statistics","Executed Instructions","inst","6,066",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","19.35",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Instruction Statistics","Issued Instructions","inst","6,346",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","NVLink","Logical Links","","0",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","NVLink","Physical Links","","0",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Block Size","","512",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Grid Size","","1",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Threads","thread","512",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Launch Statistics","Waves Per SM","","0.00",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Block Limit SM","block","16",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Block Limit Registers","block","4",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Block Limit Shared Mem","block","19",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Block Limit Warps","block","3",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Theoretical Occupancy","%","100",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Achieved Occupancy","%","33.88",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","16.26",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Source Counters","Branch Instructions","inst","641",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Source Counters","Branch Efficiency","%","99.64",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2465 sectors, got 4929 (2.00x) at PC 0x7f6ca4905240"
"120","14446","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:47:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2465 sectors, got 4929 (2.00x) at PC 0x7f6ca4905250"
