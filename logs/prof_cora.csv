TC_Blocks:	1268
Exp_Edges:	162304
Namespace(classes=7, dataset='cora', dim=1433, epochs=1, hidden=16, model='gcn', num_layers=2)
CSR (ms):	3.143
Prep. (ms):	2.098
Train (ms):	   nan	Test (ms):	   nan
==PROF== Connected to process 11868 (/home/yuke/anaconda3/envs/tcgnn/bin/python3.7)
==PROF== Disconnected from process 11868
"ID","Process ID","Process Name","Host Name","Kernel Name","Kernel Time","Context","Stream","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,347,227,582.66",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,372,579,188.95",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","113,853",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","Memory [%]","%","21.49",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","SOL DRAM","%","21.49",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","Duration","nsecond","82,912",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","30.06",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","12.07",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","58,264.48",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","GPU Speed Of Light","SM [%]","%","10.62",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details."
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.44",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.22",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.94",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.44",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Compute Workload Analysis","SM Busy","%","20.74",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","192,842,917,792.36",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","Mem Busy","%","15.39",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","Max Bandwidth","%","21.49",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","77.55",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","56.21",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","10.06",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Scheduler Statistics","One or More Eligible","%","21.85",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Scheduler Statistics","No Eligible","%","78.15",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.22",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.58",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.58",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.72",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 2.7 cycles being stalled waiting for a math execution pipeline to be available. This represents about 58.0% of the total average of 4.6 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6,369.51",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Instruction Statistics","Executed Instructions","inst","2,089,198",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6,374.75",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Instruction Statistics","Issued Instructions","inst","2,090,918",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","NVLink","Logical Links","","0",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","NVLink","Physical Links","","0",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Block Size","","64",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Grid Size","","43",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Threads","thread","2,752",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Launch Statistics","Waves Per SM","","0.52",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 43 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Block Limit SM","block","16",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Block Limit Registers","block","6",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Block Limit Shared Mem","block","1",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Block Limit Warps","block","24",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Achieved Occupancy","%","4.17",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Source Counters","Branch Instructions","inst","4,644",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Source Counters","Branch Efficiency","%","100",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","Source Counters","Avg. Divergent Branches","","0",
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 71400 (5.00x) at PC 0x7f595eb11e70"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 71400 (5.00x) at PC 0x7f595eb126d0"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 71400 (5.00x) at PC 0x7f595eb12f20"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14112 sectors, got 70560 (5.00x) at PC 0x7f595eb13190"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb11e30"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb11ff0"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb126b0"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb12790"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb12f10"
"0","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14112 sectors, got 67032 (4.75x) at PC 0x7f595eb12fc0"
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,172,413,793.10",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,053,109,605.91",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,934",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","Memory [%]","%","5.02",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","SOL DRAM","%","0.13",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","Duration","nsecond","2,784",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","8.16",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","SOL L2 Cache","%","5.02",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","SM Active Cycles","cycle","858.04",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","GPU Speed Of Light","SM [%]","%","0.78",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.65",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Compute Workload Analysis","SM Busy","%","2.65",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","Memory Throughput","byte/second","873,563,218.39",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","Mem Busy","%","5.02",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","Max Bandwidth","%","4.36",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.93",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.57",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Scheduler Statistics","One or More Eligible","%","3.08",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Scheduler Statistics","No Eligible","%","96.92",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 32.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.97",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.59",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 83.0% of the total average of 33.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17.63",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Instruction Statistics","Executed Instructions","inst","5,782",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","22.77",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Instruction Statistics","Issued Instructions","inst","7,470",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","NVLink","Logical Links","","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","NVLink","Physical Links","","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Block Size","","64",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Grid Size","","170",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Threads","thread","10,880",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Launch Statistics","Waves Per SM","","0.13",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Block Limit SM","block","16",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Block Limit Registers","block","64",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Block Limit Shared Mem","block","100",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Block Limit Warps","block","24",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Achieved Occupancy","%","7.89",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.79",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Source Counters","Branch Instructions","inst","682",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Source Counters","Branch Efficiency","%","100",
"1","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:20","1","7","Source Counters","Avg. Divergent Branches","","0",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,361,034,164.36",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,374,576,449.35",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","63,539",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","Memory [%]","%","2.66",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","SOL DRAM","%","0.74",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","Duration","nsecond","46,208",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.55",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.07",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22,355.01",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","GPU Speed Of Light","SM [%]","%","3.45",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.39",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.14",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.80",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.39",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Compute Workload Analysis","SM Busy","%","9.80",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,675,900,277.01",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","Mem Busy","%","2.20",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","Max Bandwidth","%","2.66",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","64.70",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.70",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.66",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Scheduler Statistics","One or More Eligible","%","9.89",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.10",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Scheduler Statistics","No Eligible","%","90.11",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.58",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.12",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.58 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.24",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.79",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.22",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.18",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 24.5 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 67.6% of the total average of 36.2 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2,158.08",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Instruction Statistics","Executed Instructions","inst","707,850",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2,191.14",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Instruction Statistics","Issued Instructions","inst","718,695",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","NVLink","Logical Links","","0",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","NVLink","Physical Links","","0",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Block Size","","256",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Grid Size","","170",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Threads","thread","43,520",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Launch Statistics","Waves Per SM","","0.35",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Block Limit SM","block","16",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Block Limit Registers","block","6",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Block Limit Shared Mem","block","47",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Block Limit Warps","block","6",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Theoretical Occupancy","%","100",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Achieved Occupancy","%","29.48",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.15",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Source Counters","Branch Instructions Ratio","%","0.23",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Source Counters","Branch Instructions","inst","166,189",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Source Counters","Branch Efficiency","%","94.80",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","Source Counters","Avg. Divergent Branches","","14.16",
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 19328 sectors, got 38332 (1.98x) at PC 0x7f59ce8cd300"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0c0"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0f0"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 11725 sectors, got 13982 (1.19x) at PC 0x7f59ce8cd020"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd440"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd460"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd480"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd4a0"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd4e0"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd500"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd520"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd530"
"2","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:24","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 2821 sectors, got 2875 (1.02x) at PC 0x7f59ce8cd190"
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,861,111,111.11",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,161,504,836.31",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,572",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","Memory [%]","%","7.71",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","SOL DRAM","%","7.71",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","Duration","nsecond","3,072",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.71",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","SOL L2 Cache","%","7.55",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,662.72",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","GPU Speed Of Light","SM [%]","%","0.99",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.12",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Compute Workload Analysis","SM Busy","%","2.12",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","Memory Throughput","byte/second","58,166,666,666.67",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","Mem Busy","%","7.55",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","Max Bandwidth","%","7.71",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","L2 Hit Rate","%","58.16",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.93",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Scheduler Statistics","One or More Eligible","%","2.44",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Scheduler Statistics","No Eligible","%","97.56",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.04",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 40.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.04 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","42.51",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","53.45",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.81",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 19.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.4% of the total average of 42.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 16.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 38.9% of the total average of 42.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","28.09",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Instruction Statistics","Executed Instructions","inst","9,212",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","35.31",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Instruction Statistics","Issued Instructions","inst","11,582",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","NVLink","Logical Links","","0",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","NVLink","Physical Links","","0",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Block Size","","64",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Grid Size","","170",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Threads","thread","10,880",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Launch Statistics","Waves Per SM","","0.13",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Block Limit SM","block","16",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Block Limit Registers","block","42",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Block Limit Shared Mem","block","100",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Block Limit Warps","block","24",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Achieved Occupancy","%","7.48",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.59",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Source Counters","Branch Instructions","inst","690",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Source Counters","Branch Efficiency","%","100",
"3","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:33:28","1","7","Source Counters","Avg. Divergent Branches","","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,827,956,989.25",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,140,408,986.18",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,535",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","Memory [%]","%","6.56",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","SOL DRAM","%","5.98",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","Duration","nsecond","3,968",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.29",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.56",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,790.82",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","GPU Speed Of Light","SM [%]","%","11.13",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.01",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.40",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Compute Workload Analysis","Issue Slots Busy","%","26.01",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.04",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Compute Workload Analysis","SM Busy","%","28.13",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","Memory Throughput","byte/second","44,935,483,870.97",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","Mem Busy","%","6.56",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","Max Bandwidth","%","5.98",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","L2 Hit Rate","%","62.49",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.28",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Scheduler Statistics","One or More Eligible","%","26.78",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Scheduler Statistics","No Eligible","%","73.22",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.17",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.54",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.17 active warps per scheduler, but only an average of 0.54 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","11.85",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","12.25",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.99",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.69",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 4.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 34.8% of the total average of 11.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","450.56",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Instruction Statistics","Executed Instructions","inst","147,785",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","465.75",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Instruction Statistics","Issued Instructions","inst","152,766",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","NVLink","Logical Links","","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","NVLink","Physical Links","","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Block Size","","256",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Grid Size","","170",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Registers Per Thread","register/thread","29",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Threads","thread","43,520",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Launch Statistics","Waves Per SM","","0.35",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Block Limit SM","block","16",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Block Limit Registers","block","8",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Block Limit Shared Mem","block","100",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Block Limit Warps","block","6",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Theoretical Occupancy","%","100",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Achieved Occupancy","%","26.09",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Occupancy","Achieved Active Warps Per SM","warp","12.52",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Source Counters","Branch Instructions Ratio","%","0.04",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Source Counters","Branch Instructions","inst","5,771",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Source Counters","Branch Efficiency","%","100",
"4","11868","python3.7","127.0.0.1","void at::native::(anonymous namespace)::fused_dropout_kernel_vec<float, float, unsigned int, 1, 4>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<unsigned char, unsigned int>, unsigned int, float, std::pair<unsigned long, unsigned long>)","2021-Feb-26 11:33:31","1","7","Source Counters","Avg. Divergent Branches","","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,209,150,326.80",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,204,519,307.72",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","13,777",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","Memory [%]","%","21.99",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","SOL DRAM","%","1.94",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","Duration","nsecond","11,424",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.60",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.93",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","SM Active Cycles","cycle","6,099.79",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","GPU Speed Of Light","SM [%]","%","7.44",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details."
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.39",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.17",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.94",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.40",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Compute Workload Analysis","SM Busy","%","9.94",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","Memory Throughput","byte/second","15,294,117,647.06",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","Mem Busy","%","21.99",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","Max Bandwidth","%","7.44",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","72.26",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","L2 Hit Rate","%","57.51",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Memory Workload Analysis","Mem Pipes Busy","%","7.44",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Scheduler Statistics","One or More Eligible","%","19.98",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.20",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Scheduler Statistics","No Eligible","%","80.02",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.20",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.20 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","5.04",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","5.09",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.83",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","601.21",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Instruction Statistics","Executed Instructions","inst","197,198",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","606.46",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Instruction Statistics","Issued Instructions","inst","198,918",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","NVLink","Logical Links","","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","NVLink","Physical Links","","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Block Size","","64",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Grid Size","","43",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Threads","thread","2,752",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Launch Statistics","Waves Per SM","","0.52",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 43 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Block Limit SM","block","16",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Block Limit Registers","block","6",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Block Limit Shared Mem","block","1",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Block Limit Warps","block","24",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Achieved Occupancy","%","4.15",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Source Counters","Branch Instructions","inst","860",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Source Counters","Branch Efficiency","%","100",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","Source Counters","Avg. Divergent Branches","","0",
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ec60"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0eca0"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ecd0"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ed00"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0edd0"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ee00"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ee30"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ee60"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0eee0"
"5","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:33:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ef00"
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,493,670,886.08",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,102,000,452.08",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,788",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","Memory [%]","%","2.87",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","SOL DRAM","%","0.06",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","Duration","nsecond","2,528",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.15",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.87",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","SM Active Cycles","cycle","794.06",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","GPU Speed Of Light","SM [%]","%","0.36",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.26",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Compute Workload Analysis","SM Busy","%","1.26",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","Memory Throughput","byte/second","455,696,202.53",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","Mem Busy","%","2.87",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","Max Bandwidth","%","2.17",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.87",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Scheduler Statistics","One or More Eligible","%","2.60",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Scheduler Statistics","No Eligible","%","97.40",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 38.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","38.02",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","49.20",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.06",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 30.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 79.3% of the total average of 38.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7.75",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Instruction Statistics","Executed Instructions","inst","2,543",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10.03",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Instruction Statistics","Issued Instructions","inst","3,291",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","NVLink","Logical Links","","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","NVLink","Physical Links","","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Block Size","","64",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Grid Size","","75",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Threads","thread","4,800",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Launch Statistics","Waves Per SM","","0.06",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 75 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Block Limit SM","block","16",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Block Limit Registers","block","64",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Block Limit Shared Mem","block","100",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Block Limit Warps","block","24",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Achieved Occupancy","%","4.15",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Source Counters","Branch Instructions","inst","301",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Source Counters","Branch Efficiency","%","100",
"6","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:33:39","1","7","Source Counters","Avg. Divergent Branches","","0",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,143,428,285.86",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,342,960,216.32",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","28,682",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","Memory [%]","%","3.12",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","SOL DRAM","%","0.72",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","Duration","nsecond","21,344",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","9.95",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.53",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","SM Active Cycles","cycle","8,988.20",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","GPU Speed Of Light","SM [%]","%","5.64",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.71",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.22",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Compute Workload Analysis","Issue Slots Busy","%","18.00",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.72",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Compute Workload Analysis","SM Busy","%","18.00",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,302,848,575.71",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","Mem Busy","%","2.07",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","Max Bandwidth","%","3.12",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","84.73",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","L2 Hit Rate","%","42.93",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.12",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Scheduler Statistics","One or More Eligible","%","18.06",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.18",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Scheduler Statistics","No Eligible","%","81.94",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.58",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.23",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.58 active warps per scheduler, but only an average of 0.23 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.83",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.25",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.30",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.33",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 8.3 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 42.0% of the total average of 19.8 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1,584.85",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Instruction Statistics","Executed Instructions","inst","519,832",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1,617.95",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Instruction Statistics","Issued Instructions","inst","530,689",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","NVLink","Logical Links","","0",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","NVLink","Physical Links","","0",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Block Size","","256",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Grid Size","","170",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Threads","thread","43,520",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Launch Statistics","Waves Per SM","","0.35",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Block Limit SM","block","16",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Block Limit Registers","block","6",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Block Limit Shared Mem","block","47",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Block Limit Warps","block","6",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Theoretical Occupancy","%","100",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Achieved Occupancy","%","29.97",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.38",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Source Counters","Branch Instructions Ratio","%","0.28",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Source Counters","Branch Instructions","inst","143,705",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Source Counters","Branch Efficiency","%","94.52",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","Source Counters","Avg. Divergent Branches","","12.37",
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0c0"
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0f0"
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 11725 sectors, got 13982 (1.19x) at PC 0x7f59ce8cd020"
"7","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:33:42","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 2821 sectors, got 2875 (1.02x) at PC 0x7f59ce8cd190"
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,922,330,097.09",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,154,949,722.61",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,811",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","Memory [%]","%","4.83",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","SOL DRAM","%","3.12",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","Duration","nsecond","3,296",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.97",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.83",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,872.27",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","GPU Speed Of Light","SM [%]","%","4.10",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 1% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.32",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.16",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.34",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.33",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Compute Workload Analysis","SM Busy","%","8.34",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","Memory Throughput","byte/second","23,728,155,339.81",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","Mem Busy","%","4.83",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","Max Bandwidth","%","4.06",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","53.03",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","L2 Hit Rate","%","73.23",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.92",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Scheduler Statistics","One or More Eligible","%","8.63",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Scheduler Statistics","No Eligible","%","91.37",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","11.94",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","12.42",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.70",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.47",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 4.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 38.4% of the total average of 11.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","150.09",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Instruction Statistics","Executed Instructions","inst","49,231",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","156.17",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Instruction Statistics","Issued Instructions","inst","51,223",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","NVLink","Logical Links","","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","NVLink","Physical Links","","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Block Size","","128",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Grid Size","","85",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Registers Per Thread","register/thread","17",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Threads","thread","10,880",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Launch Statistics","Waves Per SM","","0.09",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Block Limit SM","block","16",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Block Limit Registers","block","21",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Block Limit Shared Mem","block","100",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Block Limit Warps","block","12",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Theoretical Occupancy","%","100",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Achieved Occupancy","%","8.57",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.12",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Source Counters","Branch Instructions Ratio","%","0.03",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Source Counters","Branch Instructions","inst","1,357",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Source Counters","Branch Efficiency","%","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","Source Counters","Avg. Divergent Branches","","0",
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af68f180"
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af68f8f0"
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2369 (1.75x) at PC 0x7f59af68f130"
"8","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:33:46","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2369 (1.75x) at PC 0x7f59af68f6e0"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,247,619,047.62",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,215,114,795.92",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,445",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","Memory [%]","%","0.51",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","SOL DRAM","%","0.08",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","Duration","nsecond","4,480",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.41",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.51",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","44.15",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.35",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.04",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Compute Workload Analysis","SM Busy","%","9.04",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","628,571,428.57",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","Mem Busy","%","0.51",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.30",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.08",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Scheduler Statistics","One or More Eligible","%","9.22",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Scheduler Statistics","No Eligible","%","90.78",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.41",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.15",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 45.4% of the total average of 21.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","NVLink","Logical Links","","0",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","NVLink","Physical Links","","0",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Block Size","","256",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Grid Size","","1",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Threads","thread","256",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Launch Statistics","Waves Per SM","","0.00",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Block Limit SM","block","16",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Block Limit Registers","block","6",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Block Limit Shared Mem","block","88",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Block Limit Warps","block","6",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Theoretical Occupancy","%","100",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Achieved Occupancy","%","16.25",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.80",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Source Counters","Branch Instructions","inst","154",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Source Counters","Branch Efficiency","%","98.86",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"9","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:33:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,711,711,711.71",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,126,387,548.26",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,670",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","Memory [%]","%","0.97",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","Duration","nsecond","2,368",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","81.73",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.97",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.01",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.69",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Compute Workload Analysis","SM Busy","%","2.69",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","108,108,108.11",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","Mem Busy","%","0.97",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","Max Bandwidth","%","0.34",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Scheduler Statistics","One or More Eligible","%","2.76",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Scheduler Statistics","No Eligible","%","97.24",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 36.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.05",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.17",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 82.6% of the total average of 36.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Instruction Statistics","Executed Instructions","inst","81",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Instruction Statistics","Issued Instructions","inst","97",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","NVLink","Logical Links","","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","NVLink","Physical Links","","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Block Size","","128",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Grid Size","","1",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Threads","thread","128",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Launch Statistics","Waves Per SM","","0.00",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Block Limit SM","block","16",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Block Limit Registers","block","32",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Block Limit Warps","block","12",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Theoretical Occupancy","%","100",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Achieved Occupancy","%","8.27",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.97",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Source Counters","Branch Instructions","inst","5",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Source Counters","Branch Efficiency","%","0",
"10","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:33:53","1","7","Source Counters","Avg. Divergent Branches","","0",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,657,142,857.14",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,128,188,775.51",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,057",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","Memory [%]","%","0.62",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","Duration","nsecond","4,480",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.20",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.62",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","SM Active Cycles","cycle","169.73",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","GPU Speed Of Light","SM [%]","%","0.24",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.21",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.51",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.22",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Compute Workload Analysis","SM Busy","%","5.51",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","Memory Throughput","byte/second","742,857,142.86",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","Mem Busy","%","0.62",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","67.52",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.39",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.24",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Scheduler Statistics","One or More Eligible","%","5.57",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Scheduler Statistics","No Eligible","%","94.43",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 18.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","17.75",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","18.85",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.85",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.85",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 5.9 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 33.3% of the total average of 17.7 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8.80",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Instruction Statistics","Executed Instructions","inst","2,888",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.35",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Instruction Statistics","Issued Instructions","inst","3,067",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","NVLink","Logical Links","","0",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","NVLink","Physical Links","","0",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Block Size","","128",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Grid Size","","5",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Threads","thread","640",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Launch Statistics","Waves Per SM","","0.01",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Block Limit SM","block","16",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Block Limit Registers","block","10",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Block Limit Shared Mem","block","16",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Block Limit Warps","block","12",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Achieved Occupancy","%","8.85",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.25",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Source Counters","Branch Instructions","inst","192",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Source Counters","Branch Efficiency","%","99.06",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"11","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:33:56","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,224,299,065.42",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,206,400,200.27",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,264",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","Memory [%]","%","0.41",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","Duration","nsecond","6,848",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.84",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.41",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","SM Active Cycles","cycle","154.17",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","GPU Speed Of Light","SM [%]","%","0.09",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.19",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.97",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Compute Workload Analysis","SM Busy","%","4.97",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","Memory Throughput","byte/second","747,663,551.40",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","Mem Busy","%","0.41",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","8.14",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","L2 Hit Rate","%","90.25",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Scheduler Statistics","One or More Eligible","%","5.11",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Scheduler Statistics","No Eligible","%","94.89",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.51",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.08",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.62",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.41",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 48.2% of the total average of 19.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7.45",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Instruction Statistics","Executed Instructions","inst","2,442",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","7.66",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Instruction Statistics","Issued Instructions","inst","2,514",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","NVLink","Logical Links","","0",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","NVLink","Physical Links","","0",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Block Size","","128",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Grid Size","","2",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Threads","thread","256",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Launch Statistics","Waves Per SM","","0.00",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Block Limit SM","block","16",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Block Limit Registers","block","16",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Block Limit Shared Mem","block","100",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Block Limit Warps","block","12",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Theoretical Occupancy","%","100",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Achieved Occupancy","%","8.07",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.87",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Source Counters","Branch Instructions","inst","280",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Source Counters","Branch Efficiency","%","100",
"12","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:00","1","7","Source Counters","Avg. Divergent Branches","","0",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,202,531,645.57",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,204,198,688.97",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,091",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","Memory [%]","%","0.45",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","SOL DRAM","%","0.07",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","Duration","nsecond","5,056",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.04",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.45",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","SM Active Cycles","cycle","44.96",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","GPU Speed Of Light","SM [%]","%","0.07",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.34",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.88",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Compute Workload Analysis","SM Busy","%","8.88",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","Memory Throughput","byte/second","556,962,025.32",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","Mem Busy","%","0.45",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.30",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.07",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Scheduler Statistics","One or More Eligible","%","9.37",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Scheduler Statistics","No Eligible","%","90.63",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.98",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.98 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.13",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.87",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 45.8% of the total average of 21.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","NVLink","Logical Links","","0",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","NVLink","Physical Links","","0",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Block Size","","256",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Grid Size","","1",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Threads","thread","256",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Launch Statistics","Waves Per SM","","0.00",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Block Limit SM","block","16",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Block Limit Registers","block","6",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Block Limit Shared Mem","block","88",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Block Limit Warps","block","6",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Theoretical Occupancy","%","100",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Achieved Occupancy","%","16.68",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Occupancy","Achieved Active Warps Per SM","warp","8.01",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Source Counters","Branch Instructions","inst","154",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Source Counters","Branch Efficiency","%","98.86",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"13","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:03","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,639,639,639.64",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,134,350,868.73",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,689",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","Memory [%]","%","0.97",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","Duration","nsecond","2,368",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","79.78",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.97",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.28",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.62",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Compute Workload Analysis","SM Busy","%","2.62",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","Memory Throughput","byte/second","108,108,108.11",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","Mem Busy","%","0.97",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","Max Bandwidth","%","0.34",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Scheduler Statistics","One or More Eligible","%","2.80",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Scheduler Statistics","No Eligible","%","97.20",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 35.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.01",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.12",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 83.1% of the total average of 36.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Instruction Statistics","Executed Instructions","inst","81",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Instruction Statistics","Issued Instructions","inst","97",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","NVLink","Logical Links","","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","NVLink","Physical Links","","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Block Size","","128",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Grid Size","","1",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Threads","thread","128",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Launch Statistics","Waves Per SM","","0.00",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Block Limit SM","block","16",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Block Limit Registers","block","32",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Block Limit Shared Mem","block","100",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Block Limit Warps","block","12",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Theoretical Occupancy","%","100",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Achieved Occupancy","%","8.06",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.87",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Source Counters","Branch Instructions","inst","5",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Source Counters","Branch Efficiency","%","0",
"14","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:07","1","7","Source Counters","Avg. Divergent Branches","","0",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,167,848,699.76",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,051,956,686.93",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,747",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","Memory [%]","%","0.66",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","Duration","nsecond","4,512",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","6.48",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.66",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","SM Active Cycles","cycle","187.90",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","GPU Speed Of Light","SM [%]","%","0.26",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.19",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.98",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Compute Workload Analysis","SM Busy","%","4.98",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","Memory Throughput","byte/second","680,851,063.83",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","Mem Busy","%","0.66",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","Max Bandwidth","%","0.26",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","67.52",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.45",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Scheduler Statistics","One or More Eligible","%","5.50",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Scheduler Statistics","No Eligible","%","94.50",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 18.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.02",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.14",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.85",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.71",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 5.7 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 31.6% of the total average of 18.0 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8.80",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Instruction Statistics","Executed Instructions","inst","2,888",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.35",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Instruction Statistics","Issued Instructions","inst","3,067",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","NVLink","Logical Links","","0",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","NVLink","Physical Links","","0",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Block Size","","128",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Grid Size","","5",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Threads","thread","640",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Launch Statistics","Waves Per SM","","0.01",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Block Limit SM","block","16",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Block Limit Registers","block","10",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Block Limit Shared Mem","block","16",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Block Limit Warps","block","12",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Achieved Occupancy","%","7.53",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.61",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Source Counters","Branch Instructions","inst","189",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Source Counters","Branch Efficiency","%","99.07",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"15","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:10","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,370,860,927.15",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,231,315,042.57",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,953",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","Memory [%]","%","0.48",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","SOL DRAM","%","0.06",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","Duration","nsecond","4,832",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.05",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.48",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","SM Active Cycles","cycle","47.26",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.10",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.75",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Compute Workload Analysis","SM Busy","%","2.75",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","Memory Throughput","byte/second","476,821,192.05",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","Mem Busy","%","0.48",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","L2 Hit Rate","%","94.87",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Scheduler Statistics","One or More Eligible","%","3.56",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Scheduler Statistics","No Eligible","%","96.44",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 28.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","28.45",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","30.76",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.82",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.34",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 32.3% of the total average of 28.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.20",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Instruction Statistics","Executed Instructions","inst","394",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.30",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Instruction Statistics","Issued Instructions","inst","426",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","NVLink","Logical Links","","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","NVLink","Physical Links","","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Block Size","","128",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Grid Size","","1",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Threads","thread","128",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Launch Statistics","Waves Per SM","","0.00",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Block Limit SM","block","16",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Block Limit Registers","block","16",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Block Limit Shared Mem","block","100",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Block Limit Warps","block","12",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Theoretical Occupancy","%","100",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Achieved Occupancy","%","6.49",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.12",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Source Counters","Branch Instructions Ratio","%","0.15",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Source Counters","Branch Instructions","inst","59",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Source Counters","Branch Efficiency","%","97.30",
"16","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:34:14","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,767,295,597.48",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,288,809,804.58",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,745",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","Memory [%]","%","0.34",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","Duration","nsecond","6,784",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","10.85",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.34",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","83.10",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.22",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Compute Workload Analysis","SM Busy","%","1.22",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","716,981,132.08",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","Mem Busy","%","0.34",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","Max Bandwidth","%","0.13",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","2.07",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","89.42",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Scheduler Statistics","One or More Eligible","%","4.94",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Scheduler Statistics","No Eligible","%","95.06",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 20.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.51",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.88",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","18.38",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","15.89",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 13.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 65.4% of the total average of 20.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 18.4 threads being active per cycle. This is further reduced to 15.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.99",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Instruction Statistics","Executed Instructions","inst","326",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.01",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Instruction Statistics","Issued Instructions","inst","332",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","NVLink","Logical Links","","0",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","NVLink","Physical Links","","0",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Block Size","","32",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Grid Size","","1",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Registers Per Thread","register/thread","34",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","256",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Threads","thread","32",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Launch Statistics","Waves Per SM","","0.00",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Block Limit SM","block","16",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Block Limit Registers","block","48",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Block Limit Shared Mem","block","80",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Block Limit Warps","block","48",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Achieved Occupancy","%","2.07",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","0.99",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Source Counters","Branch Instructions","inst","37",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Source Counters","Branch Efficiency","%","94.44",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 26 (6.50x) at PC 0x7f597af52340"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 26 (6.50x) at PC 0x7f597af52660"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 25 (6.25x) at PC 0x7f597af52970"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 25 (6.25x) at PC 0x7f597af52c80"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2 sectors, got 10 (5.00x) at PC 0x7f597af51f40"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af52a60"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af52130"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af52440"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af52750"
"17","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)","2021-Feb-26 11:34:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2 sectors, got 3 (1.50x) at PC 0x7f597af51d30"
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,578,947,368.42",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,114,191,729.32",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,713",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","68.21",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","SM Active Cycles","cycle","13.20",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","GPU Speed Of Light","SM [%]","%","0.00",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.02",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.81",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.03",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Compute Workload Analysis","SM Busy","%","0.81",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","Memory Throughput","byte/second","52,631,578.95",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.59",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.00",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Scheduler Statistics","One or More Eligible","%","2.11",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Scheduler Statistics","No Eligible","%","97.89",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.05",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 47.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.05 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","49.66",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","64.37",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","21.67",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","21.63",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 38.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 78.3% of the total average of 49.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 21.7 threads being active per cycle. This is further reduced to 21.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.08",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Instruction Statistics","Executed Instructions","inst","27",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.11",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Instruction Statistics","Issued Instructions","inst","35",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","NVLink","Logical Links","","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","NVLink","Physical Links","","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Block Size","","64",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Grid Size","","1",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Threads","thread","64",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Launch Statistics","Waves Per SM","","0.00",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Block Limit SM","block","16",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Block Limit Registers","block","64",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Block Limit Shared Mem","block","100",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Block Limit Warps","block","24",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Achieved Occupancy","%","3.32",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.59",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Source Counters","Branch Instructions Ratio","%","0.19",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Source Counters","Branch Instructions","inst","5",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Source Counters","Branch Efficiency","%","100",
"18","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:21","1","7","Source Counters","Avg. Divergent Branches","","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,666,666,666.67",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,272,269,518.27",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","7,005",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","Memory [%]","%","0.42",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","Duration","nsecond","5,504",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","14.18",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.42",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","63.48",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.00",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Compute Workload Analysis","SM Busy","%","1.00",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","255,813,953.49",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","Mem Busy","%","0.42",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","6.04",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.15",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Scheduler Statistics","One or More Eligible","%","4.03",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Scheduler Statistics","No Eligible","%","95.97",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.62",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","25.48",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.36",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.02",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 15.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 63.9% of the total average of 24.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.61",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Instruction Statistics","Executed Instructions","inst","201",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.63",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Instruction Statistics","Issued Instructions","inst","208",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","NVLink","Logical Links","","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","NVLink","Physical Links","","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Block Size","","32",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Grid Size","","1",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Threads","thread","32",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Launch Statistics","Waves Per SM","","0.00",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Block Limit SM","block","16",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Block Limit Registers","block","64",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Block Limit Shared Mem","block","100",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Block Limit Warps","block","48",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Achieved Occupancy","%","2.08",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.00",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Source Counters","Branch Instructions Ratio","%","0.16",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Source Counters","Branch Instructions","inst","32",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Source Counters","Branch Efficiency","%","93.33",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 26 (6.50x) at PC 0x7f597af4df40"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 26 (6.50x) at PC 0x7f597af4e220"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 25 (6.25x) at PC 0x7f597af4e500"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 25 (6.25x) at PC 0x7f597af4e7e0"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2 sectors, got 10 (5.00x) at PC 0x7f597af4db70"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af4df70"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af4e250"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af4e530"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 8 (2.00x) at PC 0x7f597af4dc90"
"19","11868","python3.7","127.0.0.1","void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)","2021-Feb-26 11:34:24","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2 sectors, got 3 (1.50x) at PC 0x7f597af4d8c0"
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,976,744,186.05",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,032,495,847.18",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,842",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","Memory [%]","%","2.92",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","Duration","nsecond","2,752",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.33",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.92",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","SM Active Cycles","cycle","760.80",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","GPU Speed Of Light","SM [%]","%","0.35",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.32",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Compute Workload Analysis","SM Busy","%","1.32",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","Memory Throughput","byte/second","232,558,139.53",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","Mem Busy","%","2.92",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","Max Bandwidth","%","2.23",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.85",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Scheduler Statistics","One or More Eligible","%","2.61",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Scheduler Statistics","No Eligible","%","97.39",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.96",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 38.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.96 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.71",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","47.51",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.06",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 30.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 83.6% of the total average of 36.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7.75",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Instruction Statistics","Executed Instructions","inst","2,543",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10.03",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Instruction Statistics","Issued Instructions","inst","3,291",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","NVLink","Logical Links","","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","NVLink","Physical Links","","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Block Size","","64",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Grid Size","","75",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Threads","thread","4,800",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Launch Statistics","Waves Per SM","","0.06",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 75 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Block Limit SM","block","16",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Block Limit Registers","block","64",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Block Limit Shared Mem","block","100",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Block Limit Warps","block","24",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Achieved Occupancy","%","4.30",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.06",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Source Counters","Branch Instructions","inst","301",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Source Counters","Branch Efficiency","%","100",
"20","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:34:28","1","7","Source Counters","Avg. Divergent Branches","","0",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,394,366,197.18",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,239,656,690.14",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,636",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","Memory [%]","%","0.49",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","SOL DRAM","%","0.08",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.43",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.49",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","SM Active Cycles","cycle","44.11",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.35",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.05",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Compute Workload Analysis","SM Busy","%","9.05",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","Memory Throughput","byte/second","619,718,309.86",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","Mem Busy","%","0.49",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.15",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.08",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Scheduler Statistics","One or More Eligible","%","9.34",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Scheduler Statistics","No Eligible","%","90.66",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.02",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.02 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.68",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.44",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 44.5% of the total average of 21.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","NVLink","Logical Links","","0",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","NVLink","Physical Links","","0",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Block Size","","256",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Grid Size","","1",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Threads","thread","256",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Launch Statistics","Waves Per SM","","0.00",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Block Limit SM","block","16",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Block Limit Registers","block","6",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Block Limit Shared Mem","block","88",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Block Limit Warps","block","6",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Theoretical Occupancy","%","100",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Achieved Occupancy","%","16.22",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.79",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Source Counters","Branch Instructions","inst","154",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Source Counters","Branch Efficiency","%","98.86",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"21","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:34:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,680,000,000",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,128,869,047.62",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,710",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","81.46",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.05",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.68",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Compute Workload Analysis","SM Busy","%","2.68",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Scheduler Statistics","One or More Eligible","%","2.81",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Scheduler Statistics","No Eligible","%","97.19",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 35.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.04",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.16",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.6% of the total average of 36.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Instruction Statistics","Executed Instructions","inst","81",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Instruction Statistics","Issued Instructions","inst","97",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","NVLink","Logical Links","","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","NVLink","Physical Links","","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Block Size","","128",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Grid Size","","1",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Threads","thread","128",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Launch Statistics","Waves Per SM","","0.00",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Block Limit SM","block","16",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Block Limit Registers","block","32",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Block Limit Shared Mem","block","100",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Block Limit Warps","block","12",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Theoretical Occupancy","%","100",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Achieved Occupancy","%","8.11",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.89",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Source Counters","Branch Instructions","inst","5",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Source Counters","Branch Efficiency","%","0",
"22","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:34:35","1","7","Source Counters","Avg. Divergent Branches","","0",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,716,312,056.74",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,127,501,266.46",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,089",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","Duration","nsecond","4,512",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.04",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","172.87",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","GPU Speed Of Light","SM [%]","%","0.24",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.20",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.41",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.22",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Compute Workload Analysis","SM Busy","%","5.41",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","680,851,063.83",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","67.52",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.39",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.24",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Scheduler Statistics","One or More Eligible","%","5.54",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Scheduler Statistics","No Eligible","%","94.46",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.04",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 18.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.04 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.76",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.83",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.71",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.59",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 6.0 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 32.1% of the total average of 18.8 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8.84",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Instruction Statistics","Executed Instructions","inst","2,901",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.35",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Instruction Statistics","Issued Instructions","inst","3,067",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","NVLink","Logical Links","","0",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","NVLink","Physical Links","","0",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Block Size","","128",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Grid Size","","5",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Threads","thread","640",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Launch Statistics","Waves Per SM","","0.01",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Block Limit SM","block","16",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Block Limit Registers","block","10",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Block Limit Shared Mem","block","16",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Block Limit Warps","block","12",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Achieved Occupancy","%","8.25",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.96",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Source Counters","Branch Instructions","inst","189",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Source Counters","Branch Efficiency","%","99.06",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"23","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:34:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,876,543,209.88",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,163,235,780.42",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,022",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","Memory [%]","%","0.68",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","Duration","nsecond","3,456",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","31.74",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.68",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","SM Active Cycles","cycle","28.35",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.12",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.25",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.13",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Compute Workload Analysis","SM Busy","%","3.25",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","Memory Throughput","byte/second","370,370,370.37",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","Mem Busy","%","0.68",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","Max Bandwidth","%","0.22",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","L2 Hit Rate","%","96.80",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Scheduler Statistics","One or More Eligible","%","7.14",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Scheduler Statistics","No Eligible","%","92.86",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.07",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.07",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 14.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.07 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","14.93",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","15.65",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.92",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 34.4% of the total average of 14.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.88",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Instruction Statistics","Executed Instructions","inst","288",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.92",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Instruction Statistics","Issued Instructions","inst","302",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","NVLink","Logical Links","","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","NVLink","Physical Links","","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Block Size","","64",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Grid Size","","1",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Threads","thread","64",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Launch Statistics","Waves Per SM","","0.00",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Block Limit SM","block","16",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Block Limit Registers","block","32",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Block Limit Shared Mem","block","100",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Block Limit Warps","block","24",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Achieved Occupancy","%","3.84",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.84",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Source Counters","Branch Instructions","inst","48",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Source Counters","Branch Efficiency","%","91.67",
"24","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::remainder_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long, long)#1}>, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:42","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,912,087,912.09",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,164,246,467.82",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,394",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","Memory [%]","%","0.80",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","SOL DRAM","%","0.06",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","Duration","nsecond","2,912",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","45.87",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.80",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","SM Active Cycles","cycle","19.62",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.08",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Compute Workload Analysis","SM Busy","%","2.08",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","Memory Throughput","byte/second","439,560,439.56",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","Mem Busy","%","0.80",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.02",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Scheduler Statistics","One or More Eligible","%","4.31",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Scheduler Statistics","No Eligible","%","95.69",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.28",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.03",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.70",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 50.3% of the total average of 23.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.37",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Instruction Statistics","Executed Instructions","inst","120",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.41",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Instruction Statistics","Issued Instructions","inst","134",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","NVLink","Logical Links","","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","NVLink","Physical Links","","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Block Size","","64",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Grid Size","","1",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Registers Per Thread","register/thread","22",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Threads","thread","64",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Launch Statistics","Waves Per SM","","0.00",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Block Limit SM","block","16",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Block Limit Registers","block","42",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Block Limit Shared Mem","block","100",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Block Limit Warps","block","24",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Achieved Occupancy","%","4.10",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.97",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Source Counters","Branch Instructions","inst","21",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Source Counters","Branch Efficiency","%","90.91",
"25","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::MulFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:46","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,950,617,283.95",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,170,510,912.70",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,047",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","Memory [%]","%","0.67",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","Duration","nsecond","3,456",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","32.90",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.67",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","SM Active Cycles","cycle","27.35",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.11",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.83",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Compute Workload Analysis","SM Busy","%","2.83",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","Memory Throughput","byte/second","407,407,407.41",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","Mem Busy","%","0.67",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","Max Bandwidth","%","0.22",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.02",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Scheduler Statistics","One or More Eligible","%","6.07",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Scheduler Statistics","No Eligible","%","93.93",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 16.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.73",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.71",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.97",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.04",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 6.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 36.4% of the total average of 16.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.73",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Instruction Statistics","Executed Instructions","inst","240",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.77",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Instruction Statistics","Issued Instructions","inst","254",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","NVLink","Logical Links","","0",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","NVLink","Physical Links","","0",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Block Size","","64",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Grid Size","","1",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Threads","thread","64",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Launch Statistics","Waves Per SM","","0.00",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Block Limit SM","block","16",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Block Limit Registers","block","32",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Block Limit Shared Mem","block","100",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Block Limit Warps","block","24",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Achieved Occupancy","%","3.92",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.88",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Source Counters","Branch Instructions","inst","43",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Source Counters","Branch Efficiency","%","91.67",
"26","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::DivFunctor<long> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:34:50","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,459,915,611.81",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,093,015,370.71",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,766",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","Duration","nsecond","2,528",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","81.19",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.09",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.25",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","7.37",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.29",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Compute Workload Analysis","SM Busy","%","8.36",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","101,265,822.78",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.69",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Scheduler Statistics","One or More Eligible","%","7.46",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Scheduler Statistics","No Eligible","%","92.54",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 13.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","26.45",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","31.65",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.30",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 19.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 75.1% of the total average of 26.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.68",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Instruction Statistics","Executed Instructions","inst","224",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.82",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Instruction Statistics","Issued Instructions","inst","268",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","NVLink","Logical Links","","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","NVLink","Physical Links","","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Block Size","","256",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Grid Size","","1",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Threads","thread","256",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Launch Statistics","Waves Per SM","","0.00",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Block Limit SM","block","16",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Block Limit Registers","block","16",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Block Limit Warps","block","6",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Theoretical Occupancy","%","100",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Achieved Occupancy","%","16.35",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.85",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Source Counters","Branch Instructions","inst","16",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Source Counters","Branch Efficiency","%","100",
"27","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long>(thrust::cuda_cub::__transform::unary_transform_f<thrust::counting_iterator<long, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<long>, thrust::cuda_cub::__transform::always_true_predicate>, long)","2021-Feb-26 11:34:53","1","7","Source Counters","Avg. Divergent Branches","","0",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,660,587,639.31",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,272,619,952.24",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","26,798",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","Memory [%]","%","0.79",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","Duration","nsecond","21,056",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","69.45",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.11",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","304.83",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","GPU Speed Of Light","SM [%]","%","0.27",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.72",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","17.96",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.72",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Compute Workload Analysis","SM Busy","%","19.71",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","121,580,547.11",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","Mem Busy","%","0.79",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","55.13",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","94.94",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.27",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Scheduler Statistics","One or More Eligible","%","17.95",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.18",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Scheduler Statistics","No Eligible","%","82.05",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.00",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.24",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.00 active warps per scheduler, but only an average of 0.24 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","11.17",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","11.19",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.68",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.28",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 3.6 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 32.1% of the total average of 11.2 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","54.65",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Instruction Statistics","Executed Instructions","inst","17,926",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","54.76",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Instruction Statistics","Issued Instructions","inst","17,962",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","NVLink","Logical Links","","0",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","NVLink","Physical Links","","0",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Block Size","","256",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Grid Size","","1",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Registers Per Thread","register/thread","92",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","16,912",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Threads","thread","256",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Launch Statistics","Waves Per SM","","0.01",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Block Limit SM","block","16",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Block Limit Registers","block","2",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Block Limit Shared Mem","block","5",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Block Limit Warps","block","6",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Achieved Occupancy","%","16.61",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.97",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Source Counters","Branch Instructions Ratio","%","0.01",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Source Counters","Branch Instructions","inst","186",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Source Counters","Branch Efficiency","%","87.13",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","Source Counters","Avg. Divergent Branches","","0.04",
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 132 (8.25x) at PC 0x7f59905eeba0"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef3c0"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef400"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef430"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef470"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef4b0"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef4e0"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef540"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 128 (8.00x) at PC 0x7f59905ef570"
"28","11868","python3.7","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__merge_sort::BlockSortAgent<thrust::device_ptr<long>, thrust::device_ptr<long>, long, ThrustLTOp<long, false>, thrust::detail::integral_constant<bool, true>, thrust::detail::integral_constant<bool, false> >, bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false> >(bool, thrust::device_ptr<long>, thrust::device_ptr<long>, long, long*, long*, ThrustLTOp<long, false>)","2021-Feb-26 11:34:57","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 16 sectors, got 124 (7.75x) at PC 0x7f59905eecb0"
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,325,203,252.03",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,223,577,235.77",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,819",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","Memory [%]","%","1.35",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","SOL DRAM","%","0.33",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","Duration","nsecond","3,936",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.36",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.89",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,209.56",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","GPU Speed Of Light","SM [%]","%","1.46",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.22",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.06",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.82",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.23",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Compute Workload Analysis","SM Busy","%","5.82",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,601,626,016.26",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","Mem Busy","%","0.89",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","Max Bandwidth","%","1.35",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","70.38",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","L2 Hit Rate","%","84.60",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Memory Workload Analysis","Mem Pipes Busy","%","1.35",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Scheduler Statistics","One or More Eligible","%","6.27",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Scheduler Statistics","No Eligible","%","93.73",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.06",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 16.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.06 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.87",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.73",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","17.45",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","14.80",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 46.3% of the total average of 16.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 17.5 threads being active per cycle. This is further reduced to 14.8 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","66.96",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Instruction Statistics","Executed Instructions","inst","21,962",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","70.37",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Instruction Statistics","Issued Instructions","inst","23,082",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","NVLink","Logical Links","","0",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","NVLink","Physical Links","","0",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Block Size","","128",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Grid Size","","35",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Registers Per Thread","register/thread","39",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Threads","thread","4,480",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Launch Statistics","Waves Per SM","","0.04",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 35 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Block Limit SM","block","16",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Block Limit Registers","block","12",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Block Limit Shared Mem","block","100",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Block Limit Warps","block","12",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Theoretical Occupancy","%","100",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Achieved Occupancy","%","7.69",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.69",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Source Counters","Branch Instructions","inst","2,097",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Source Counters","Branch Efficiency","%","87.48",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","Source Counters","Avg. Divergent Branches","","0.43",
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 140 sectors, got 245 (1.75x) at PC 0x7f5977930a70"
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 140 sectors, got 245 (1.75x) at PC 0x7f5977930930"
"29","11868","python3.7","127.0.0.1","void (anonymous namespace)::indexing_backward_kernel<float, 4>(long*, long*, float*, float*, long, long, long, long)","2021-Feb-26 11:35:01","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 140 sectors, got 245 (1.75x) at PC 0x7f5977930970"
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,025,641,025.64",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,175,781,250",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,918",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","Memory [%]","%","6.04",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","SOL DRAM","%","5.99",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","Duration","nsecond","3,328",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","6.49",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.04",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,787.71",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","GPU Speed Of Light","SM [%]","%","2.97",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.20",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.09",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.50",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.22",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Compute Workload Analysis","SM Busy","%","5.50",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","Memory Throughput","byte/second","46,153,846,153.85",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","Mem Busy","%","6.04",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","Max Bandwidth","%","5.99",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","51.78",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","L2 Hit Rate","%","58.41",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.97",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Scheduler Statistics","One or More Eligible","%","5.71",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Scheduler Statistics","No Eligible","%","94.29",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 17.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.02",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.68",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.01",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.38",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 7.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 42.5% of the total average of 18.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","90.05",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Instruction Statistics","Executed Instructions","inst","29,537",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","98.34",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Instruction Statistics","Issued Instructions","inst","32,257",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","NVLink","Logical Links","","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","NVLink","Physical Links","","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Block Size","","128",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Grid Size","","85",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Registers Per Thread","register/thread","21",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Threads","thread","10,880",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Launch Statistics","Waves Per SM","","0.09",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Block Limit SM","block","16",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Block Limit Registers","block","21",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Block Limit Shared Mem","block","100",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Block Limit Warps","block","12",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Theoretical Occupancy","%","100",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Achieved Occupancy","%","8.36",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.01",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Source Counters","Branch Instructions","inst","2,035",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Source Counters","Branch Efficiency","%","0",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","Source Counters","Avg. Divergent Branches","","1.03",
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af61fca0"
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af61fd00"
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af620040"
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2369 (1.75x) at PC 0x7f59af61fce0"
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2369 (1.75x) at PC 0x7f59af61ff00"
"30","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_backward<float, float, float, 3, true>(float*, float const*, float const*, int, int, int)","2021-Feb-26 11:35:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1351 sectors, got 2363 (1.75x) at PC 0x7f59af61fc90"
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,623,931,623.93",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,128,491,300.37",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,820",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","Memory [%]","%","2.94",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","Duration","nsecond","2,496",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.38",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.94",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","SM Active Cycles","cycle","751.39",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","GPU Speed Of Light","SM [%]","%","0.36",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.34",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Compute Workload Analysis","SM Busy","%","1.34",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","Memory Throughput","byte/second","102,564,102.56",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","Mem Busy","%","2.94",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","Max Bandwidth","%","2.25",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.87",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Scheduler Statistics","One or More Eligible","%","2.72",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Scheduler Statistics","No Eligible","%","97.28",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 36.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.29",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.96",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.06",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 29.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 80.5% of the total average of 36.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7.75",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Instruction Statistics","Executed Instructions","inst","2,543",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10.03",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Instruction Statistics","Issued Instructions","inst","3,291",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","NVLink","Logical Links","","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","NVLink","Physical Links","","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Block Size","","64",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Grid Size","","75",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Threads","thread","4,800",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Launch Statistics","Waves Per SM","","0.06",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 75 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Block Limit SM","block","16",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Block Limit Registers","block","64",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Block Limit Shared Mem","block","100",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Block Limit Warps","block","24",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Achieved Occupancy","%","4.14",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Source Counters","Branch Instructions","inst","301",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Source Counters","Branch Efficiency","%","100",
"31","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:08","1","7","Source Counters","Avg. Divergent Branches","","0",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,065,868,263.47",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,331,526,678.79",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","28,485",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","Memory [%]","%","3.14",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","SOL DRAM","%","0.72",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","Duration","nsecond","21,376",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","9.89",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.53",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","SM Active Cycles","cycle","9,046.45",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","GPU Speed Of Light","SM [%]","%","5.68",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.70",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.22",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Compute Workload Analysis","Issue Slots Busy","%","17.88",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.72",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Compute Workload Analysis","SM Busy","%","17.88",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,275,449,101.80",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","Mem Busy","%","2.08",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","Max Bandwidth","%","3.14",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","84.74",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","L2 Hit Rate","%","92.38",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.14",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Scheduler Statistics","One or More Eligible","%","17.93",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.18",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Scheduler Statistics","No Eligible","%","82.07",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.57",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.23",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.57 active warps per scheduler, but only an average of 0.23 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.94",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.36",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.30",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.33",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 8.3 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 41.8% of the total average of 19.9 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1,584.85",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Instruction Statistics","Executed Instructions","inst","519,832",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1,617.88",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Instruction Statistics","Issued Instructions","inst","530,666",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","NVLink","Logical Links","","0",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","NVLink","Physical Links","","0",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Block Size","","256",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Grid Size","","170",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Threads","thread","43,520",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Launch Statistics","Waves Per SM","","0.35",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Block Limit SM","block","16",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Block Limit Registers","block","6",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Block Limit Shared Mem","block","47",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Block Limit Warps","block","6",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Theoretical Occupancy","%","100",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Achieved Occupancy","%","29.75",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.28",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Source Counters","Branch Instructions Ratio","%","0.28",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Source Counters","Branch Instructions","inst","143,705",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Source Counters","Branch Efficiency","%","94.52",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","Source Counters","Avg. Divergent Branches","","12.37",
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0c0"
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0f0"
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 11725 sectors, got 13982 (1.19x) at PC 0x7f59ce8cd020"
"32","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:11","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 2821 sectors, got 2875 (1.02x) at PC 0x7f59ce8cd190"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,355,029,585.80",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,226,463,440.41",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","13,286",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","Memory [%]","%","22.08",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","SOL DRAM","%","0.91",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","Duration","nsecond","10,816",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.59",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.63",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","SM Active Cycles","cycle","5,907.05",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","GPU Speed Of Light","SM [%]","%","7.24",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details."
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.41",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.18",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.28",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.41",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Compute Workload Analysis","SM Busy","%","10.28",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","Memory Throughput","byte/second","7,266,272,189.35",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","Mem Busy","%","22.08",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","Max Bandwidth","%","7.24",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","67.84",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","L2 Hit Rate","%","86.80",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Memory Workload Analysis","Mem Pipes Busy","%","7.24",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Scheduler Statistics","One or More Eligible","%","20.65",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Scheduler Statistics","No Eligible","%","79.35",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.85",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.89",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.89",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","602",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Instruction Statistics","Executed Instructions","inst","197,456",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","607.51",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Instruction Statistics","Issued Instructions","inst","199,262",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","NVLink","Logical Links","","0",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","NVLink","Physical Links","","0",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Block Size","","64",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Grid Size","","43",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Threads","thread","2,752",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Launch Statistics","Waves Per SM","","0.52",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 43 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Block Limit SM","block","16",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Block Limit Registers","block","6",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Block Limit Shared Mem","block","1",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Block Limit Warps","block","24",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Achieved Occupancy","%","4.15",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Source Counters","Branch Instructions","inst","860",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Source Counters","Branch Efficiency","%","100",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","Source Counters","Avg. Divergent Branches","","0",
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf4390"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf43c0"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf43f0"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf4420"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf44b0"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf44c0"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf44d0"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf44e0"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf4500"
"33","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_tn_align1::Params)","2021-Feb-26 11:35:15","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 85 sectors, got 340 (4.00x) at PC 0x7f595eaf4530"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,488,972,282.57",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,394,328,778.82",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","225,905",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","Memory [%]","%","0.30",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","SOL DRAM","%","0.17",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","Duration","nsecond","162,016",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","25.12",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.09",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","SM Active Cycles","cycle","2,733",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","GPU Speed Of Light","SM [%]","%","0.30",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.42",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.48",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.42",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Compute Workload Analysis","SM Busy","%","19.42",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,574,560,537.23",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","Mem Busy","%","0.21",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","Max Bandwidth","%","0.30",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","76.74",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","L2 Hit Rate","%","34.38",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.30",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Scheduler Statistics","One or More Eligible","%","21.15",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Scheduler Statistics","No Eligible","%","78.85",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.77",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.77",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.06",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 2.6 cycles being stalled waiting for a math execution pipeline to be available. This represents about 53.6% of the total average of 4.8 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","286.26",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Instruction Statistics","Executed Instructions","inst","93,894",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","286.38",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Instruction Statistics","Issued Instructions","inst","93,934",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","NVLink","Logical Links","","0",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","NVLink","Physical Links","","0",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Block Size","","64",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Grid Size","","1",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Threads","thread","64",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Launch Statistics","Waves Per SM","","0.01",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Block Limit SM","block","16",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Block Limit Registers","block","6",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Block Limit Shared Mem","block","1",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Block Limit Warps","block","24",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Achieved Occupancy","%","4.17",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Source Counters","Branch Instructions","inst","190",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Source Counters","Branch Efficiency","%","100",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","Source Counters","Avg. Divergent Branches","","0",
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04a30"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04a40"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04a50"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04a60"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04f80"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04f90"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04fc0"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb04fe0"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb05760"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 328 sectors, got 1312 (4.00x) at PC 0x7f595eb05770"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04a30"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04a40"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04a50"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04a60"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04f80"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04f90"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04fc0"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb04fe0"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb05760"
"34","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:20","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 164 sectors, got 492 (3.00x) at PC 0x7f595eb05770"
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,247,422,680.41",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,207,198,085.42",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,751",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","Memory [%]","%","8.93",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","SOL DRAM","%","8.93",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","Duration","nsecond","3,104",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.92",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","8.12",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,677.49",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","GPU Speed Of Light","SM [%]","%","1.16",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.60",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Compute Workload Analysis","SM Busy","%","2.60",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","70,721,649,484.54",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","Mem Busy","%","8.12",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","Max Bandwidth","%","8.93",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","53.64",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.89",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Scheduler Statistics","One or More Eligible","%","2.89",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Scheduler Statistics","No Eligible","%","97.11",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.91",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","45.69",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.97",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 15.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 43.3% of the total average of 34.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 14.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 41.0% of the total average of 34.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","33.27",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Instruction Statistics","Executed Instructions","inst","10,914",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","43.55",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Instruction Statistics","Issued Instructions","inst","14,284",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","NVLink","Logical Links","","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","NVLink","Physical Links","","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Block Size","","64",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Grid Size","","170",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Threads","thread","10,880",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Launch Statistics","Waves Per SM","","0.13",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Block Limit SM","block","16",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Block Limit Registers","block","42",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Block Limit Shared Mem","block","100",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Block Limit Warps","block","24",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Achieved Occupancy","%","7.72",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.71",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Source Counters","Branch Instructions","inst","690",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Source Counters","Branch Efficiency","%","100",
"35","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3> >(int, at::native::(anonymous namespace)::masked_scale_kernel<float, float>(at::Tensor&, at::Tensor, at::Tensor, float)::{lambda(float, unsigned char)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:24","1","7","Source Counters","Avg. Divergent Branches","","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,950,155,763.24",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,174,774,699.60",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,027",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","Memory [%]","%","13.44",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","SOL DRAM","%","13.44",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","Duration","nsecond","3,424",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.17",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","SOL L2 Cache","%","9.62",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,842.90",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","GPU Speed Of Light","SM [%]","%","0.88",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.92",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Compute Workload Analysis","SM Busy","%","1.92",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","Memory Throughput","byte/second","102,542,056,074.77",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","Mem Busy","%","9.62",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","Max Bandwidth","%","13.44",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","L2 Hit Rate","%","41.63",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.82",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Scheduler Statistics","One or More Eligible","%","2.11",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Scheduler Statistics","No Eligible","%","97.89",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 47.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","47.78",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","60.07",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.81",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 18.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 38.3% of the total average of 47.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 22.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 47.0% of the total average of 47.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","28.09",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Instruction Statistics","Executed Instructions","inst","9,212",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","35.31",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Instruction Statistics","Issued Instructions","inst","11,582",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","NVLink","Logical Links","","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","NVLink","Physical Links","","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Block Size","","64",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Grid Size","","170",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Threads","thread","10,880",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Launch Statistics","Waves Per SM","","0.13",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Block Limit SM","block","16",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Block Limit Registers","block","42",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Block Limit Shared Mem","block","100",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Block Limit Warps","block","24",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Achieved Occupancy","%","7.74",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.71",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Source Counters","Branch Instructions","inst","690",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Source Counters","Branch Efficiency","%","100",
"36","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:28","1","7","Source Counters","Avg. Divergent Branches","","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,703,703,703.70",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,132,440,476.19",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,937",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","Memory [%]","%","5.01",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","Duration","nsecond","2,592",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","8.19",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","SOL L2 Cache","%","5.01",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","SM Active Cycles","cycle","855.67",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","GPU Speed Of Light","SM [%]","%","0.78",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.66",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Compute Workload Analysis","SM Busy","%","2.66",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","Memory Throughput","byte/second","246,913,580.25",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","Mem Busy","%","5.01",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","Max Bandwidth","%","4.35",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.93",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.57",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Scheduler Statistics","One or More Eligible","%","3.11",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Scheduler Statistics","No Eligible","%","96.89",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 32.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.15",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.83",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 25.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 77.9% of the total average of 33.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17.63",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Instruction Statistics","Executed Instructions","inst","5,782",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","22.77",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Instruction Statistics","Issued Instructions","inst","7,470",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","NVLink","Logical Links","","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","NVLink","Physical Links","","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Block Size","","64",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Grid Size","","170",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Threads","thread","10,880",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Launch Statistics","Waves Per SM","","0.13",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Block Limit SM","block","16",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Block Limit Registers","block","64",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Block Limit Shared Mem","block","100",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Block Limit Warps","block","24",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Achieved Occupancy","%","7.57",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.63",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Source Counters","Branch Instructions","inst","682",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Source Counters","Branch Efficiency","%","100",
"37","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:31","1","7","Source Counters","Avg. Divergent Branches","","0",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,393,476,752.26",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,379,275,304.85",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","63,623",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","Memory [%]","%","2.65",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","SOL DRAM","%","0.75",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","Duration","nsecond","46,112",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.56",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.07",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22,339.60",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","GPU Speed Of Light","SM [%]","%","3.44",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.39",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.14",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.81",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.39",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Compute Workload Analysis","SM Busy","%","9.81",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,775,850,104.09",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","Mem Busy","%","2.20",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","Max Bandwidth","%","2.65",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","64.63",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","L2 Hit Rate","%","74.38",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.65",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Scheduler Statistics","One or More Eligible","%","9.90",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.10",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Scheduler Statistics","No Eligible","%","90.10",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.57",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.12",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.57 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.08",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.63",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.22",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.18",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 24.5 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 68.0% of the total average of 36.1 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2,158.08",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Instruction Statistics","Executed Instructions","inst","707,850",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2,191.06",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Instruction Statistics","Issued Instructions","inst","718,667",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","NVLink","Logical Links","","0",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","NVLink","Physical Links","","0",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Block Size","","256",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Grid Size","","170",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Threads","thread","43,520",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Launch Statistics","Waves Per SM","","0.35",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Block Limit SM","block","16",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Block Limit Registers","block","6",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Block Limit Shared Mem","block","47",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Block Limit Warps","block","6",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Theoretical Occupancy","%","100",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Achieved Occupancy","%","29.54",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.18",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Source Counters","Branch Instructions Ratio","%","0.23",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Source Counters","Branch Instructions","inst","166,189",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Source Counters","Branch Efficiency","%","94.80",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","Source Counters","Avg. Divergent Branches","","14.16",
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 19328 sectors, got 38332 (1.98x) at PC 0x7f59ce8cd300"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0c0"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0f0"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 11725 sectors, got 13982 (1.19x) at PC 0x7f59ce8cd020"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd440"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd460"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd480"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd4a0"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd4e0"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd500"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd520"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd530"
"38","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:35:34","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 2821 sectors, got 2875 (1.02x) at PC 0x7f59ce8cd190"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,379,310,344.83",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,348,800,254.25",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","38,896",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","Memory [%]","%","50.81",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","SOL DRAM","%","50.81",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","Duration","nsecond","28,768",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","46.90",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","35.05",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","31,962.20",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","GPU Speed Of Light","SM [%]","%","38.63",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons."
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 18% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.63",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.34",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","40.83",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.63",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Compute Workload Analysis","SM Busy","%","40.83",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","457,539,488,320.36",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","Mem Busy","%","34.27",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","Max Bandwidth","%","50.81",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","30.47",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.32",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","38.63",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Scheduler Statistics","One or More Eligible","%","40.60",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.41",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Scheduler Statistics","No Eligible","%","59.40",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.44",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.84",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.44 active warps per scheduler, but only an average of 0.84 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","8.47",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","8.50",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.27",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","13,006.83",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Instruction Statistics","Executed Instructions","inst","4,266,240",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13,050.85",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Instruction Statistics","Issued Instructions","inst","4,280,679",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","NVLink","Logical Links","","0",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","NVLink","Physical Links","","0",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Block Size","","256",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Grid Size","","264",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Registers Per Thread","register/thread","118",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16,896",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Threads","thread","67,584",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Launch Statistics","Waves Per SM","","1.61",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Block Limit SM","block","16",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Block Limit Registers","block","2",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Block Limit Shared Mem","block","5",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Block Limit Warps","block","6",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","16",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Theoretical Occupancy","%","33.33",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Achieved Occupancy","%","29.19",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.01",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Source Counters","Branch Instructions Ratio","%","0.03",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Source Counters","Branch Instructions","inst","109,824",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Source Counters","Branch Efficiency","%","100",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","Source Counters","Avg. Divergent Branches","","0",
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 62192 sectors, got 77064 (1.24x) at PC 0x7f59663834a0"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 59488 sectors, got 74360 (1.25x) at PC 0x7f59663834b0"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 62192 sectors, got 77064 (1.24x) at PC 0x7f5966383530"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 59488 sectors, got 74360 (1.25x) at PC 0x7f5966383540"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 59840 sectors, got 71060 (1.19x) at PC 0x7f5966383470"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 62560 sectors, got 73780 (1.18x) at PC 0x7f5966383460"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 62192 sectors, got 73346 (1.18x) at PC 0x7f59663834f0"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 59488 sectors, got 70642 (1.19x) at PC 0x7f5966383500"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 67584 sectors, got 135168 (2.00x) at PC 0x7f5966383060"
"39","11868","python3.7","127.0.0.1","ampere_sgemm_128x128_tn","2021-Feb-26 11:35:38","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 67584 sectors, got 135168 (2.00x) at PC 0x7f5966383070"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,397,251,265.86",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,381,123,530.99",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","224,040",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","Memory [%]","%","10.73",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","SOL DRAM","%","10.73",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","Duration","nsecond","162,208",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","27.05",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.23",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","SM Active Cycles","cycle","62,032.43",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","GPU Speed Of Light","SM [%]","%","7.05",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.42",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.12",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.62",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.42",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Compute Workload Analysis","SM Busy","%","19.68",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","Memory Throughput","byte/second","96,812,783,586.51",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","Mem Busy","%","7.49",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","Max Bandwidth","%","10.73",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","76.06",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","L2 Hit Rate","%","55.34",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Memory Workload Analysis","Mem Pipes Busy","%","7.05",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Scheduler Statistics","One or More Eligible","%","21.26",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Scheduler Statistics","No Eligible","%","78.74",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.21",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.21 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.71",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.71",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.73",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 2.5 cycles being stalled waiting for a math execution pipeline to be available. This represents about 53.2% of the total average of 4.7 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6,584.03",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Instruction Statistics","Executed Instructions","inst","2,159,562",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6,586.84",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Instruction Statistics","Issued Instructions","inst","2,160,482",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","NVLink","Logical Links","","0",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","NVLink","Physical Links","","0",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Block Size","","64",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Grid Size","","23",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Threads","thread","1,472",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Launch Statistics","Waves Per SM","","0.28",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 23 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Block Limit SM","block","16",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Block Limit Registers","block","6",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Block Limit Shared Mem","block","1",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Block Limit Warps","block","24",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Achieved Occupancy","%","4.17",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Source Counters","Branch Instructions","inst","4,370",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Source Counters","Branch Efficiency","%","100",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","Source Counters","Avg. Divergent Branches","","0",
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 15088 sectors, got 74784 (4.96x) at PC 0x7f595eb04a30"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 15088 sectors, got 74784 (4.96x) at PC 0x7f595eb057f0"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14432 sectors, got 72160 (5.00x) at PC 0x7f595eb04a80"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14432 sectors, got 72160 (5.00x) at PC 0x7f595eb05860"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14924 sectors, got 71012 (4.76x) at PC 0x7f595eb04a40"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14924 sectors, got 71012 (4.76x) at PC 0x7f595eb04f80"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14924 sectors, got 71012 (4.76x) at PC 0x7f595eb05770"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14924 sectors, got 71012 (4.76x) at PC 0x7f595eb05a60"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14432 sectors, got 68552 (4.75x) at PC 0x7f595eb04ac0"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14432 sectors, got 68552 (4.75x) at PC 0x7f595eb050d0"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 15088 (4.00x) at PC 0x7f595eb04a30"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 15088 (4.00x) at PC 0x7f595eb04fe0"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 15088 (4.00x) at PC 0x7f595eb057f0"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 15088 (4.00x) at PC 0x7f595eb05a70"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 14924 (3.96x) at PC 0x7f595eb04a40"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 14924 (3.96x) at PC 0x7f595eb04a50"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 14924 (3.96x) at PC 0x7f595eb04a60"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 14924 (3.96x) at PC 0x7f595eb04f80"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 14924 (3.96x) at PC 0x7f595eb04f90"
"40","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nt_align1::Params)","2021-Feb-26 11:35:43","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 3772 sectors, got 14924 (3.96x) at PC 0x7f595eb04fc0"
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,633,333,333.33",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,120,089,285.71",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,869",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","Memory [%]","%","3.29",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","Duration","nsecond","2,560",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.77",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.29",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","SM Active Cycles","cycle","816.89",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","GPU Speed Of Light","SM [%]","%","0.42",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.48",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Compute Workload Analysis","SM Busy","%","1.48",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","Memory Throughput","byte/second","250,000,000",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","Mem Busy","%","3.29",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","Max Bandwidth","%","2.61",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.89",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.31",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Scheduler Statistics","One or More Eligible","%","2.79",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Scheduler Statistics","No Eligible","%","97.21",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 35.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","35.63",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.05",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.97",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.08",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 78.3% of the total average of 35.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9.38",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Instruction Statistics","Executed Instructions","inst","3,077",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.12",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Instruction Statistics","Issued Instructions","inst","3,977",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","NVLink","Logical Links","","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","NVLink","Physical Links","","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Block Size","","64",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Grid Size","","90",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Threads","thread","5,760",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Launch Statistics","Waves Per SM","","0.07",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Block Limit SM","block","16",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Block Limit Registers","block","64",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Block Limit Shared Mem","block","100",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Block Limit Warps","block","24",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Achieved Occupancy","%","4.59",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.20",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Source Counters","Branch Instructions","inst","365",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Source Counters","Branch Efficiency","%","100",
"41","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:46","1","7","Source Counters","Avg. Divergent Branches","","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,794,871,794.87",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,140,682,234.43",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,851",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","Memory [%]","%","3.29",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","SOL DRAM","%","0.04",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","Duration","nsecond","2,496",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.75",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","SOL L2 Cache","%","3.29",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","SM Active Cycles","cycle","820.17",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","GPU Speed Of Light","SM [%]","%","0.43",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.48",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Compute Workload Analysis","SM Busy","%","1.48",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","Memory Throughput","byte/second","307,692,307.69",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","Mem Busy","%","3.29",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","Max Bandwidth","%","2.60",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.87",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.31",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Scheduler Statistics","One or More Eligible","%","2.76",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Scheduler Statistics","No Eligible","%","97.24",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 36.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.31",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.94",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.97",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.08",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 78.0% of the total average of 36.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9.38",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Instruction Statistics","Executed Instructions","inst","3,077",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.12",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Instruction Statistics","Issued Instructions","inst","3,977",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","NVLink","Logical Links","","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","NVLink","Physical Links","","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Block Size","","64",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Grid Size","","90",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Threads","thread","5,760",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Launch Statistics","Waves Per SM","","0.07",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Block Limit SM","block","16",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Block Limit Registers","block","64",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Block Limit Shared Mem","block","100",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Block Limit Warps","block","24",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Achieved Occupancy","%","4.57",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.19",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Source Counters","Branch Instructions","inst","365",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Source Counters","Branch Efficiency","%","100",
"42","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:35:50","1","7","Source Counters","Avg. Divergent Branches","","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,113,475,177.30",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,179,473,784.19",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,551",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","Memory [%]","%","7.91",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","SOL DRAM","%","7.91",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","Duration","nsecond","3,008",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.52",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.34",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,547.37",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","GPU Speed Of Light","SM [%]","%","0.50",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.01",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Compute Workload Analysis","SM Busy","%","1.01",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","61,617,021,276.60",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","Mem Busy","%","6.34",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","Max Bandwidth","%","7.91",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","47.02",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.50",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Scheduler Statistics","One or More Eligible","%","1.97",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Scheduler Statistics","No Eligible","%","98.03",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 50.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","50.68",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","61.55",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.92",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.49",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 21.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 42.8% of the total average of 50.7 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 22.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 43.5% of the total average of 50.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12.82",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Instruction Statistics","Executed Instructions","inst","4,205",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15.57",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Instruction Statistics","Issued Instructions","inst","5,107",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","NVLink","Logical Links","","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","NVLink","Physical Links","","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Block Size","","64",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Grid Size","","90",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Registers Per Thread","register/thread","20",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Threads","thread","5,760",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Launch Statistics","Waves Per SM","","0.07",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Block Limit SM","block","16",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Block Limit Registers","block","42",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Block Limit Warps","block","24",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Achieved Occupancy","%","4.51",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.17",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Source Counters","Branch Instructions Ratio","%","0.09",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Source Counters","Branch Instructions","inst","377",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Source Counters","Branch Efficiency","%","99.47",
"43","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:35:53","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,897,435,897.44",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,010,173,420.33",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,366",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","Memory [%]","%","4.59",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","SOL DRAM","%","4.22",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","Duration","nsecond","3,328",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.08",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.59",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,404.78",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","GPU Speed Of Light","SM [%]","%","0.41",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.99",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Compute Workload Analysis","SM Busy","%","0.99",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","27,961,538,461.54",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","Mem Busy","%","4.59",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","Max Bandwidth","%","4.22",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","61.23",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.40",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Scheduler Statistics","One or More Eligible","%","1.88",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Scheduler Statistics","No Eligible","%","98.12",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 53.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","53.05",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","66.14",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.30",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 24.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 46.9% of the total average of 53.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 19.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.5% of the total average of 53.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.15",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Instruction Statistics","Executed Instructions","inst","3,658",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.90",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Instruction Statistics","Issued Instructions","inst","4,560",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","NVLink","Logical Links","","0",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","NVLink","Physical Links","","0",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Block Size","","64",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Grid Size","","90",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Threads","thread","5,760",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Launch Statistics","Waves Per SM","","0.07",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Block Limit SM","block","16",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Block Limit Registers","block","64",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Block Limit Shared Mem","block","100",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Block Limit Warps","block","24",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Achieved Occupancy","%","4.37",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.10",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Source Counters","Branch Instructions","inst","377",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Source Counters","Branch Efficiency","%","99.47",
"44","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:35:57","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,056,737,588.65",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,185,600,303.95",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,570",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","Memory [%]","%","7.96",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","SOL DRAM","%","7.96",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","Duration","nsecond","3,008",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.50",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.15",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,551.89",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","GPU Speed Of Light","SM [%]","%","0.50",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.00",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Compute Workload Analysis","SM Busy","%","1.00",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","Memory Throughput","byte/second","61,531,914,893.62",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","Mem Busy","%","6.15",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","Max Bandwidth","%","7.96",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","L2 Hit Rate","%","45.56",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.50",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Scheduler Statistics","One or More Eligible","%","1.98",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Scheduler Statistics","No Eligible","%","98.02",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 50.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","51.01",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","61.96",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.92",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.49",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 21.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 42.4% of the total average of 51.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 21.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 42.2% of the total average of 51.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","12.82",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Instruction Statistics","Executed Instructions","inst","4,205",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","15.57",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Instruction Statistics","Issued Instructions","inst","5,107",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","NVLink","Logical Links","","0",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","NVLink","Physical Links","","0",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Block Size","","64",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Grid Size","","90",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Registers Per Thread","register/thread","20",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Threads","thread","5,760",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Launch Statistics","Waves Per SM","","0.07",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Block Limit SM","block","16",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Block Limit Registers","block","42",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Block Limit Shared Mem","block","100",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Block Limit Warps","block","24",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Achieved Occupancy","%","4.39",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.11",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Source Counters","Branch Instructions Ratio","%","0.09",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Source Counters","Branch Instructions","inst","377",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Source Counters","Branch Efficiency","%","99.47",
"45","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:01","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,532,110,091.74",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","SM Frequency","cycle/second","963,016,055.05",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,362",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","Memory [%]","%","4.57",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","SOL DRAM","%","4.21",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","Duration","nsecond","3,488",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.03",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.57",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,432.34",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","GPU Speed Of Light","SM [%]","%","0.41",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Compute Workload Analysis","Issue Slots Busy","%","0.97",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Compute Workload Analysis","SM Busy","%","0.97",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","Memory Throughput","byte/second","26,385,321,100.92",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","Mem Busy","%","4.57",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","Max Bandwidth","%","4.21",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","L2 Hit Rate","%","61.06",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.40",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Scheduler Statistics","One or More Eligible","%","1.85",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Scheduler Statistics","No Eligible","%","98.15",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 54.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","53.02",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","66.09",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.30",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 24.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 46.2% of the total average of 53.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 19.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.9% of the total average of 53.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.15",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Instruction Statistics","Executed Instructions","inst","3,658",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.90",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Instruction Statistics","Issued Instructions","inst","4,560",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","NVLink","Logical Links","","0",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","NVLink","Physical Links","","0",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Block Size","","64",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Grid Size","","90",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Threads","thread","5,760",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Launch Statistics","Waves Per SM","","0.07",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Block Limit SM","block","16",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Block Limit Registers","block","64",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Block Limit Shared Mem","block","100",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Block Limit Warps","block","24",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Achieved Occupancy","%","4.24",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.04",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Source Counters","Branch Instructions","inst","377",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Source Counters","Branch Efficiency","%","99.47",
"46","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:05","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,112,280,701.75",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,188,956,766.92",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,617",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","Memory [%]","%","7.89",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","SOL DRAM","%","7.89",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","Duration","nsecond","3,040",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.40",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.02",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,589.35",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","GPU Speed Of Light","SM [%]","%","0.62",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.29",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Compute Workload Analysis","SM Busy","%","1.29",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","Memory Throughput","byte/second","61,431,578,947.37",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","Mem Busy","%","6.02",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","Max Bandwidth","%","7.89",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","L2 Hit Rate","%","45.14",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.62",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Scheduler Statistics","One or More Eligible","%","2.49",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Scheduler Statistics","No Eligible","%","97.51",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 40.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.18",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","49.45",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.92",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.82",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 17.8 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 44.3% of the total average of 40.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 16.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 39.9% of the total average of 40.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","16.67",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Instruction Statistics","Executed Instructions","inst","5,467",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","20.52",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Instruction Statistics","Issued Instructions","inst","6,729",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","NVLink","Logical Links","","0",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","NVLink","Physical Links","","0",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Block Size","","64",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Grid Size","","90",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Registers Per Thread","register/thread","24",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Threads","thread","5,760",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Launch Statistics","Waves Per SM","","0.07",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Block Limit SM","block","16",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Block Limit Registers","block","42",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Block Limit Shared Mem","block","100",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Block Limit Warps","block","24",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Achieved Occupancy","%","4.55",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.18",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Source Counters","Branch Instructions","inst","377",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Source Counters","Branch Efficiency","%","99.47",
"47","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:08","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,840,000,000",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,154,464,285.71",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,700",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","Memory [%]","%","4.33",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","SOL DRAM","%","3.87",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","Duration","nsecond","3,200",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.61",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.33",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,661.66",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","GPU Speed Of Light","SM [%]","%","0.98",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.04",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.19",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Compute Workload Analysis","SM Busy","%","2.19",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","Memory Throughput","byte/second","29,120,000,000",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","Mem Busy","%","4.33",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","Max Bandwidth","%","3.87",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","L2 Hit Rate","%","62.62",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.36",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Scheduler Statistics","One or More Eligible","%","4.06",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Scheduler Statistics","No Eligible","%","95.94",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 24.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","24.34",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.33",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.97",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.41",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 38.4% of the total average of 24.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 30.9% of the total average of 24.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","33.63",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Instruction Statistics","Executed Instructions","inst","11,030",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","36.38",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Instruction Statistics","Issued Instructions","inst","11,932",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","NVLink","Logical Links","","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","NVLink","Physical Links","","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Block Size","","64",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Grid Size","","90",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Registers Per Thread","register/thread","21",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Threads","thread","5,760",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Launch Statistics","Waves Per SM","","0.07",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Block Limit SM","block","16",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Block Limit Registers","block","42",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Block Limit Shared Mem","block","100",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Block Limit Warps","block","24",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Achieved Occupancy","%","4.51",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.17",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Source Counters","Branch Instructions","inst","1,823",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Source Counters","Branch Efficiency","%","99.78",
"48","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:13","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,913,978,494.62",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,162,298,387.10",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,463",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","Memory [%]","%","4.64",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","SOL DRAM","%","4.13",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","Duration","nsecond","2,976",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.16",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.64",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,370.11",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","GPU Speed Of Light","SM [%]","%","0.40",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.01",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Compute Workload Analysis","SM Busy","%","1.01",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","31,397,849,462.37",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","Mem Busy","%","4.64",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","Max Bandwidth","%","4.13",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","62.76",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.39",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Scheduler Statistics","One or More Eligible","%","1.94",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Scheduler Statistics","No Eligible","%","98.06",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 51.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","51.56",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","64.27",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.30",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 24.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.9% of the total average of 51.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 20.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 38.7% of the total average of 51.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.15",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Instruction Statistics","Executed Instructions","inst","3,658",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","13.90",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Instruction Statistics","Issued Instructions","inst","4,560",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","NVLink","Logical Links","","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","NVLink","Physical Links","","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Block Size","","64",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Grid Size","","90",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Threads","thread","5,760",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Launch Statistics","Waves Per SM","","0.07",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Block Limit SM","block","16",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Block Limit Registers","block","64",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Block Limit Shared Mem","block","100",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Block Limit Warps","block","24",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Achieved Occupancy","%","4.47",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.15",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Source Counters","Branch Instructions","inst","377",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Source Counters","Branch Efficiency","%","99.47",
"49","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:17","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,029,629,629.63",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,181,001,984.13",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,404",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","Memory [%]","%","4.53",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","SOL DRAM","%","4.14",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","Duration","nsecond","2,880",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","3.16",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.53",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,372.96",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","GPU Speed Of Light","SM [%]","%","0.42",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.03",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.05",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.04",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Compute Workload Analysis","SM Busy","%","1.05",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","Memory Throughput","byte/second","31,911,111,111.11",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","Mem Busy","%","4.53",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","Max Bandwidth","%","4.14",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","L2 Hit Rate","%","61.17",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.39",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Scheduler Statistics","One or More Eligible","%","2.01",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Scheduler Statistics","No Eligible","%","97.99",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 49.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","49.87",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","61.59",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.37",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 22.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.9% of the total average of 49.9 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 19.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 38.4% of the total average of 49.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.70",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Instruction Statistics","Executed Instructions","inst","3,839",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","14.45",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Instruction Statistics","Issued Instructions","inst","4,741",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","NVLink","Logical Links","","0",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","NVLink","Physical Links","","0",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Block Size","","64",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Grid Size","","90",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Threads","thread","5,760",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Launch Statistics","Waves Per SM","","0.07",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Block Limit SM","block","16",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Block Limit Registers","block","64",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Block Limit Shared Mem","block","100",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Block Limit Warps","block","24",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Achieved Occupancy","%","4.46",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.14",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Source Counters","Branch Instructions Ratio","%","0.10",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Source Counters","Branch Instructions","inst","377",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Source Counters","Branch Efficiency","%","99.47",
"50","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:20","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,448,377,581.12",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,242,770,227.56",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,501",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","Memory [%]","%","9.42",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","SOL DRAM","%","9.42",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","Duration","nsecond","3,616",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.54",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.35",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","2,311.90",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","GPU Speed Of Light","SM [%]","%","1.37",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.10",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.05",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.66",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Compute Workload Analysis","SM Busy","%","2.66",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","76,424,778,761.06",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","Mem Busy","%","6.35",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","Max Bandwidth","%","9.42",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","25",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","37.15",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.58",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Scheduler Statistics","One or More Eligible","%","5.01",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Scheduler Statistics","No Eligible","%","94.99",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 20.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.27",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.62",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.96",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.43",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 32.1% of the total average of 20.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","57.74",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Instruction Statistics","Executed Instructions","inst","18,938",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","61.59",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Instruction Statistics","Issued Instructions","inst","20,200",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","NVLink","Logical Links","","0",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","NVLink","Physical Links","","0",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Block Size","","64",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Grid Size","","90",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Registers Per Thread","register/thread","28",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Threads","thread","5,760",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Launch Statistics","Waves Per SM","","0.07",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Block Limit SM","block","16",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Block Limit Registers","block","32",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Block Limit Shared Mem","block","100",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Block Limit Warps","block","24",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Achieved Occupancy","%","4.48",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.15",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Source Counters","Branch Instructions","inst","3,262",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Source Counters","Branch Efficiency","%","99.91",
"51","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:24","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,776,470,588.24",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","SM Frequency","cycle/second","997,636,554.62",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,716",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","Memory [%]","%","0.96",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","Duration","nsecond","2,720",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","79.96",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.96",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.26",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.52",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Compute Workload Analysis","SM Busy","%","1.52",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","Memory Throughput","byte/second","47,058,823.53",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","Mem Busy","%","0.96",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.68",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Scheduler Statistics","One or More Eligible","%","3.06",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Scheduler Statistics","No Eligible","%","96.94",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 32.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.18",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.17",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.26",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.83",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 24.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.2% of the total average of 32.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.14",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Instruction Statistics","Executed Instructions","inst","46",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.17",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Instruction Statistics","Issued Instructions","inst","56",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","NVLink","Logical Links","","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","NVLink","Physical Links","","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Block Size","","64",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Grid Size","","1",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Threads","thread","64",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Launch Statistics","Waves Per SM","","0.00",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Block Limit SM","block","16",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Block Limit Registers","block","64",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Block Limit Shared Mem","block","100",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Block Limit Warps","block","24",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Achieved Occupancy","%","4.03",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.94",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Source Counters","Branch Instructions","inst","8",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Source Counters","Branch Efficiency","%","100",
"52","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:27","1","7","Source Counters","Avg. Divergent Branches","","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,807,843,137.25",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","SM Frequency","cycle/second","999,474,789.92",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,722",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","Duration","nsecond","2,720",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","79.78",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","SM Active Cycles","cycle","11.28",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.05",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.51",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.06",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Compute Workload Analysis","SM Busy","%","1.51",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","Memory Throughput","byte/second","94,117,647.06",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Scheduler Statistics","One or More Eligible","%","3.06",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Scheduler Statistics","No Eligible","%","96.94",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 32.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","32.21",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","39.22",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.26",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.83",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 24.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.2% of the total average of 32.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.14",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Instruction Statistics","Executed Instructions","inst","46",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.17",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Instruction Statistics","Issued Instructions","inst","56",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","NVLink","Logical Links","","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","NVLink","Physical Links","","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Block Size","","64",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Grid Size","","1",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Threads","thread","64",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Launch Statistics","Waves Per SM","","0.00",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Block Limit SM","block","16",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Block Limit Registers","block","64",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Block Limit Shared Mem","block","100",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Block Limit Warps","block","24",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Achieved Occupancy","%","4.03",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.93",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Source Counters","Branch Instructions","inst","8",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Source Counters","Branch Efficiency","%","100",
"53","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:36:30","1","7","Source Counters","Avg. Divergent Branches","","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,706,270,627.06",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","SM Frequency","cycle/second","992,795,261.67",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,213",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","Memory [%]","%","0.82",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","Duration","nsecond","3,232",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","52.90",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.82",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","SM Active Cycles","cycle","17.01",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.83",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Compute Workload Analysis","SM Busy","%","1.83",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","Memory Throughput","byte/second","158,415,841.58",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","Mem Busy","%","0.82",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.50",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Scheduler Statistics","One or More Eligible","%","3.65",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Scheduler Statistics","No Eligible","%","96.35",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 27.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","27.42",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","31.08",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.80",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.31",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 13.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 48.3% of the total average of 27.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.27",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Instruction Statistics","Executed Instructions","inst","90",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.31",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Instruction Statistics","Issued Instructions","inst","102",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","NVLink","Logical Links","","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","NVLink","Physical Links","","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Block Size","","64",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Grid Size","","1",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Threads","thread","64",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Launch Statistics","Waves Per SM","","0.00",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Block Limit SM","block","16",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Block Limit Registers","block","64",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Block Limit Shared Mem","block","100",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Block Limit Warps","block","24",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Achieved Occupancy","%","4.04",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.94",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Source Counters","Branch Instructions Ratio","%","0.22",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Source Counters","Branch Instructions","inst","20",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Source Counters","Branch Efficiency","%","90.91",
"54","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:34","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,976,430,976.43",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,032,693,001.44",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,272",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","Memory [%]","%","0.81",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","SOL DRAM","%","0.04",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","Duration","nsecond","3,168",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","52.19",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.81",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","SM Active Cycles","cycle","17.24",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.96",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Compute Workload Analysis","SM Busy","%","1.96",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","Memory Throughput","byte/second","282,828,282.83",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","Mem Busy","%","0.81",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.50",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Scheduler Statistics","One or More Eligible","%","3.96",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Scheduler Statistics","No Eligible","%","96.04",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 25.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.40",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","28.47",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.61",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.34",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 12.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 47.7% of the total average of 25.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.30",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Instruction Statistics","Executed Instructions","inst","99",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.34",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Instruction Statistics","Issued Instructions","inst","111",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","NVLink","Logical Links","","0",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","NVLink","Physical Links","","0",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Block Size","","64",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Grid Size","","1",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Registers Per Thread","register/thread","20",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Threads","thread","64",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Launch Statistics","Waves Per SM","","0.00",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Block Limit SM","block","16",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Block Limit Registers","block","42",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Block Limit Shared Mem","block","100",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Block Limit Warps","block","24",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Achieved Occupancy","%","4.06",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.95",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Source Counters","Branch Instructions Ratio","%","0.20",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Source Counters","Branch Instructions","inst","20",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Source Counters","Branch Efficiency","%","90.91",
"55","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::AddFunctor<float>, at::detail::Array<char*, 3> >(int, at::native::AddFunctor<float>, at::detail::Array<char*, 3>)","2021-Feb-26 11:36:38","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,679,867,986.80",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","SM Frequency","cycle/second","980,684,229.14",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,174",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","Memory [%]","%","0.83",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","Duration","nsecond","3,232",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","51.04",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.83",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","SM Active Cycles","cycle","17.63",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.76",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Compute Workload Analysis","SM Busy","%","1.76",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","Memory Throughput","byte/second","198,019,801.98",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","Mem Busy","%","0.83",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.50",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Scheduler Statistics","One or More Eligible","%","3.76",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Scheduler Statistics","No Eligible","%","96.24",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 26.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","26.79",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","30.37",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.80",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.31",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 13.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 49.5% of the total average of 26.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.27",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Instruction Statistics","Executed Instructions","inst","90",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.31",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Instruction Statistics","Issued Instructions","inst","102",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","NVLink","Logical Links","","0",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","NVLink","Physical Links","","0",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Block Size","","64",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Grid Size","","1",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Threads","thread","64",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Launch Statistics","Waves Per SM","","0.00",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Block Limit SM","block","16",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Block Limit Registers","block","64",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Block Limit Shared Mem","block","100",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Block Limit Warps","block","24",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Achieved Occupancy","%","3.88",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.86",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Source Counters","Branch Instructions Ratio","%","0.22",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Source Counters","Branch Instructions","inst","20",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Source Counters","Branch Efficiency","%","90.91",
"56","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:42","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,886,731,391.59",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,006,371,359.22",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,319",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","Memory [%]","%","0.80",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","Duration","nsecond","3,296",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","48.43",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.80",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","SM Active Cycles","cycle","18.59",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.12",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Compute Workload Analysis","SM Busy","%","2.12",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","Memory Throughput","byte/second","349,514,563.11",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","Mem Busy","%","0.80",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.57",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Scheduler Statistics","One or More Eligible","%","4.29",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Scheduler Statistics","No Eligible","%","95.71",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 23.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","23.19",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","26.47",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.32",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.34",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 48.8% of the total average of 23.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.34",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Instruction Statistics","Executed Instructions","inst","113",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.39",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Instruction Statistics","Issued Instructions","inst","129",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","NVLink","Logical Links","","0",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","NVLink","Physical Links","","0",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Block Size","","64",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Grid Size","","1",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Registers Per Thread","register/thread","24",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Threads","thread","64",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Launch Statistics","Waves Per SM","","0.00",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Block Limit SM","block","16",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Block Limit Registers","block","42",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Block Limit Shared Mem","block","100",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Block Limit Warps","block","24",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Achieved Occupancy","%","4.00",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.92",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Source Counters","Branch Instructions","inst","20",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Source Counters","Branch Efficiency","%","90.91",
"57","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcmul_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:36:45","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,970,760,233.92",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,019,345,238.10",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,721",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","Memory [%]","%","0.71",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","Duration","nsecond","3,648",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","39.94",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.71",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22.54",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.10",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.67",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Compute Workload Analysis","SM Busy","%","2.67",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","175,438,596.49",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","Mem Busy","%","0.71",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.50",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Scheduler Statistics","One or More Eligible","%","5.38",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Scheduler Statistics","No Eligible","%","94.62",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 18.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.36",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.55",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.06",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.64",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 7.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 38.1% of the total average of 18.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.56",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Instruction Statistics","Executed Instructions","inst","185",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.60",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Instruction Statistics","Issued Instructions","inst","197",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","NVLink","Logical Links","","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","NVLink","Physical Links","","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Block Size","","64",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Grid Size","","1",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Registers Per Thread","register/thread","21",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Threads","thread","64",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Launch Statistics","Waves Per SM","","0.00",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Block Limit SM","block","16",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Block Limit Registers","block","42",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Block Limit Shared Mem","block","100",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Block Limit Warps","block","24",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Achieved Occupancy","%","4.05",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.94",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Source Counters","Branch Instructions Ratio","%","0.33",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Source Counters","Branch Instructions","inst","61",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Source Counters","Branch Efficiency","%","94.87",
"58","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::sqrt_kernel_cuda(at::TensorIterator&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:49","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,759,075,907.59",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","987,049,151.34",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,193",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","Memory [%]","%","0.83",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","Duration","nsecond","3,232",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","53.63",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.83",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","16.78",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.85",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Compute Workload Analysis","SM Busy","%","1.85",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","198,019,801.98",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","Mem Busy","%","0.83",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.50",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Scheduler Statistics","One or More Eligible","%","3.74",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Scheduler Statistics","No Eligible","%","96.26",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 26.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","26.52",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","30.06",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.80",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.31",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 13.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 50.9% of the total average of 26.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.27",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Instruction Statistics","Executed Instructions","inst","90",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.31",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Instruction Statistics","Issued Instructions","inst","102",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","NVLink","Logical Links","","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","NVLink","Physical Links","","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Block Size","","64",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Grid Size","","1",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Threads","thread","64",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Launch Statistics","Waves Per SM","","0.00",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Block Limit SM","block","16",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Block Limit Registers","block","64",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Block Limit Warps","block","24",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Achieved Occupancy","%","4.12",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Source Counters","Branch Instructions Ratio","%","0.22",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Source Counters","Branch Instructions","inst","20",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Source Counters","Branch Efficiency","%","90.91",
"59","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2> >(int, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:53","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,968,992,248.06",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,166,735,880.40",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,213",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","Memory [%]","%","0.82",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","Duration","nsecond","2,752",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","55.12",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.82",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","16.33",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.94",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Compute Workload Analysis","SM Busy","%","1.94",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","232,558,139.53",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","Mem Busy","%","0.82",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","50",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.57",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Scheduler Statistics","One or More Eligible","%","3.95",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Scheduler Statistics","No Eligible","%","96.05",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 25.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.44",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","28.76",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.87",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.43",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 12.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 50.9% of the total average of 25.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.28",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Instruction Statistics","Executed Instructions","inst","92",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.32",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Instruction Statistics","Issued Instructions","inst","104",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","NVLink","Logical Links","","0",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","NVLink","Physical Links","","0",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Block Size","","64",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Grid Size","","1",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Threads","thread","64",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Launch Statistics","Waves Per SM","","0.00",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Block Limit SM","block","16",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Block Limit Registers","block","64",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Block Limit Shared Mem","block","100",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Block Limit Warps","block","24",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Achieved Occupancy","%","4.16",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Source Counters","Branch Instructions Ratio","%","0.22",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Source Counters","Branch Instructions","inst","20",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Source Counters","Branch Efficiency","%","90.91",
"60","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<at::native::AddFunctor<float> >, at::detail::Array<char*, 2>)","2021-Feb-26 11:36:57","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,950,131,233.60",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,025,098,425.20",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,168",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","Memory [%]","%","0.65",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","SOL DRAM","%","0.06",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","Duration","nsecond","4,064",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","31.21",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.65",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","SM Active Cycles","cycle","28.84",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.14",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Compute Workload Analysis","Issue Slots Busy","%","3.70",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.15",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Compute Workload Analysis","SM Busy","%","3.70",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","Memory Throughput","byte/second","377,952,755.91",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","Mem Busy","%","0.65",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","Max Bandwidth","%","0.22",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","25",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","L2 Hit Rate","%","96.53",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Scheduler Statistics","One or More Eligible","%","7.49",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Scheduler Statistics","No Eligible","%","92.51",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.07",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 13.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.07 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","13.37",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","14.18",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.41",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","24.68",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 4.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 31.4% of the total average of 13.4 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.01",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Instruction Statistics","Executed Instructions","inst","330",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.07",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Instruction Statistics","Issued Instructions","inst","350",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","NVLink","Logical Links","","0",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","NVLink","Physical Links","","0",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Block Size","","64",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Grid Size","","1",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Registers Per Thread","register/thread","28",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Threads","thread","64",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Launch Statistics","Waves Per SM","","0.00",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Block Limit SM","block","16",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Block Limit Registers","block","32",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Block Limit Shared Mem","block","100",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Block Limit Warps","block","24",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Achieved Occupancy","%","4.07",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.95",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Source Counters","Branch Instructions Ratio","%","0.31",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Source Counters","Branch Instructions","inst","101",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Source Counters","Branch Efficiency","%","96.61",
"61","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4> >(int, at::native::addcdiv_cuda_kernel(at::TensorIterator&, c10::Scalar)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(float, float, float)#1}, at::detail::Array<char*, 4>)","2021-Feb-26 11:37:01","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,330,251,669.23",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,369,811,729.58",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","113,836",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","Memory [%]","%","21.45",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","SOL DRAM","%","21.45",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","Duration","nsecond","83,072",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","30.11",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","SOL L2 Cache","%","12.03",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","SM Active Cycles","cycle","58,197.20",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","GPU Speed Of Light","SM [%]","%","10.62",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details."
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.44",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.22",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Compute Workload Analysis","Issue Slots Busy","%","10.95",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.44",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Compute Workload Analysis","SM Busy","%","20.76",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","Memory Throughput","byte/second","192,089,368,258.86",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","Mem Busy","%","15.40",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","Max Bandwidth","%","21.45",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","77.55",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","L2 Hit Rate","%","56.45",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Memory Workload Analysis","Mem Pipes Busy","%","10.06",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Scheduler Statistics","One or More Eligible","%","21.84",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Scheduler Statistics","No Eligible","%","78.16",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.22",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.58",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.58",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.72",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","WarpStateStats","","","","CPIStallMathPipeThrottle","WRN","On average each warp of this kernel spends 2.6 cycles being stalled waiting for a math execution pipeline to be available. This represents about 57.9% of the total average of 4.6 cycles between issuing two instructions. This stall occurs when all active warps execute their next instruction on a specific, oversubscribed math pipeline. Try to increase the number of active warps to hide the existent latency or try changing the instruction mix to utilize all available pipelines in a more balanced way."
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6,369.51",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Instruction Statistics","Executed Instructions","inst","2,089,198",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6,374.75",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Instruction Statistics","Issued Instructions","inst","2,090,918",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","NVLink","Logical Links","","0",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","NVLink","Physical Links","","0",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Block Size","","64",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Grid Size","","43",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Threads","thread","2,752",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Launch Statistics","Waves Per SM","","0.52",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 43 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Block Limit SM","block","16",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Block Limit Registers","block","6",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Block Limit Shared Mem","block","1",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Block Limit Warps","block","24",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Achieved Occupancy","%","4.18",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Source Counters","Branch Instructions","inst","4,644",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Source Counters","Branch Efficiency","%","100",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","Source Counters","Avg. Divergent Branches","","0",
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 71400 (5.00x) at PC 0x7f595eb11e70"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 71400 (5.00x) at PC 0x7f595eb126d0"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 71400 (5.00x) at PC 0x7f595eb12f20"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14112 sectors, got 70560 (5.00x) at PC 0x7f595eb13190"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb11e30"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb11ff0"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb126b0"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb12790"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14280 sectors, got 67830 (4.75x) at PC 0x7f595eb12f10"
"62","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:05","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 14112 sectors, got 67032 (4.75x) at PC 0x7f595eb12fc0"
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,703,703,703.70",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,133,487,654.32",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,940",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","Memory [%]","%","5.00",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","Duration","nsecond","2,592",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.54",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","SOL L2 Cache","%","5.00",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","SM Active Cycles","cycle","928.41",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","GPU Speed Of Light","SM [%]","%","0.78",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.45",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Compute Workload Analysis","SM Busy","%","2.45",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","Memory Throughput","byte/second","345,679,012.35",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","Mem Busy","%","5.00",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","Max Bandwidth","%","4.34",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.93",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.56",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Scheduler Statistics","One or More Eligible","%","3.04",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Scheduler Statistics","No Eligible","%","96.96",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 32.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.27",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","42.98",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.12",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 25.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 76.5% of the total average of 33.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","17.63",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Instruction Statistics","Executed Instructions","inst","5,782",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","22.77",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Instruction Statistics","Issued Instructions","inst","7,470",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","NVLink","Logical Links","","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","NVLink","Physical Links","","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Block Size","","64",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Grid Size","","170",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Threads","thread","10,880",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Launch Statistics","Waves Per SM","","0.13",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Block Limit SM","block","16",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Block Limit Registers","block","64",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Block Limit Shared Mem","block","100",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Block Limit Warps","block","24",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Achieved Occupancy","%","6.95",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.34",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Source Counters","Branch Instructions","inst","682",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Source Counters","Branch Efficiency","%","100",
"63","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:08","1","7","Source Counters","Avg. Divergent Branches","","0",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,324,163,227.88",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,369,470,303.60",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","63,737",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","Memory [%]","%","2.65",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","SOL DRAM","%","0.75",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","Duration","nsecond","46,528",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.53",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.07",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","SM Active Cycles","cycle","22,413.01",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","GPU Speed Of Light","SM [%]","%","3.44",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.39",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.14",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.78",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.39",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Compute Workload Analysis","SM Busy","%","9.78",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,671,251,719.39",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","Mem Busy","%","2.19",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","Max Bandwidth","%","2.65",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","64.62",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","L2 Hit Rate","%","70.72",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Memory Workload Analysis","Mem Pipes Busy","%","2.65",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Scheduler Statistics","One or More Eligible","%","9.90",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.10",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Scheduler Statistics","No Eligible","%","90.10",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.59",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.12",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.59 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.24",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","36.79",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.22",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.18",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 24.6 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 67.8% of the total average of 36.2 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2,158.08",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Instruction Statistics","Executed Instructions","inst","707,850",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2,191.12",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Instruction Statistics","Issued Instructions","inst","718,687",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","NVLink","Logical Links","","0",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","NVLink","Physical Links","","0",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Block Size","","256",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Grid Size","","170",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Threads","thread","43,520",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Launch Statistics","Waves Per SM","","0.35",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Block Limit SM","block","16",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Block Limit Registers","block","6",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Block Limit Shared Mem","block","47",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Block Limit Warps","block","6",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Theoretical Occupancy","%","100",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Achieved Occupancy","%","29.48",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.15",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Source Counters","Branch Instructions Ratio","%","0.23",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Source Counters","Branch Instructions","inst","166,189",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Source Counters","Branch Efficiency","%","94.80",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","Source Counters","Avg. Divergent Branches","","14.16",
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 19328 sectors, got 38332 (1.98x) at PC 0x7f59ce8cd300"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0c0"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0f0"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 11725 sectors, got 13982 (1.19x) at PC 0x7f59ce8cd020"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd440"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd460"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd480"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd4a0"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd4e0"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd500"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd520"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 1268 sectors, got 2536 (2.00x) at PC 0x7f59ce8cd530"
"64","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:12","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 2821 sectors, got 2875 (1.02x) at PC 0x7f59ce8cd190"
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,396,039,603.96",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,226,573,550.21",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,969",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","Memory [%]","%","6.88",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","SOL DRAM","%","6.88",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","Duration","nsecond","3,232",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.42",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","SOL L2 Cache","%","6.82",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,772.28",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","GPU Speed Of Light","SM [%]","%","0.89",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.99",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Compute Workload Analysis","SM Busy","%","1.99",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","Memory Throughput","byte/second","55,445,544,554.46",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","Mem Busy","%","6.82",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","Max Bandwidth","%","6.88",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.33",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","L2 Hit Rate","%","58.27",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.84",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Scheduler Statistics","One or More Eligible","%","2.39",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.02",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Scheduler Statistics","No Eligible","%","97.61",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.02",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 41.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.02 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.01",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","54.07",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.81",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 18.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 42.5% of the total average of 43.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 18.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 42.4% of the total average of 43.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","28.09",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Instruction Statistics","Executed Instructions","inst","9,212",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","35.31",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Instruction Statistics","Issued Instructions","inst","11,582",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","NVLink","Logical Links","","0",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","NVLink","Physical Links","","0",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Block Size","","64",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Grid Size","","170",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Registers Per Thread","register/thread","19",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Threads","thread","10,880",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Launch Statistics","Waves Per SM","","0.13",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Block Limit SM","block","16",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Block Limit Registers","block","42",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Block Limit Shared Mem","block","100",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Block Limit Warps","block","24",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Achieved Occupancy","%","7.29",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.50",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Source Counters","Branch Instructions","inst","690",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Source Counters","Branch Efficiency","%","100",
"65","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3> >(int, at::native::threshold_kernel_impl<float>(at::TensorIterator&, float, float)::{lambda(float, float)#1}, at::detail::Array<char*, 3>)","2021-Feb-26 11:37:16","1","7","Source Counters","Avg. Divergent Branches","","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,133,333,333.33",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,193,514,384.92",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","13,766",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","Memory [%]","%","22.01",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","SOL DRAM","%","1.99",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","Duration","nsecond","11,520",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.77",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.93",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","SM Active Cycles","cycle","6,079.80",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","GPU Speed Of Light","SM [%]","%","7.44",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details."
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.40",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.17",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.97",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.40",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Compute Workload Analysis","SM Busy","%","9.97",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","Memory Throughput","byte/second","15,544,444,444.44",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","Mem Busy","%","22.01",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","Max Bandwidth","%","7.44",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","72.24",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","L2 Hit Rate","%","57.48",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Memory Workload Analysis","Mem Pipes Busy","%","7.44",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Scheduler Statistics","One or More Eligible","%","20.14",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.20",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Scheduler Statistics","No Eligible","%","79.86",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.20",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.20 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.98",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","5.03",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.83",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","601.21",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Instruction Statistics","Executed Instructions","inst","197,198",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","606.46",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Instruction Statistics","Issued Instructions","inst","198,918",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","NVLink","Logical Links","","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","NVLink","Physical Links","","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Block Size","","64",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Grid Size","","43",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Registers Per Thread","register/thread","168",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Shared Memory Configuration Size","byte","102,400",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","65,536",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Threads","thread","2,752",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Launch Statistics","Waves Per SM","","0.52",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 43 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Block Limit SM","block","16",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Block Limit Registers","block","6",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Block Limit Shared Mem","block","1",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Block Limit Warps","block","24",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Theoretical Active Warps per SM","warp","2",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Theoretical Occupancy","%","4.17",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Achieved Occupancy","%","4.15",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Source Counters","Branch Instructions Ratio","%","0.00",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Source Counters","Branch Instructions","inst","860",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Source Counters","Branch Efficiency","%","100",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","Source Counters","Avg. Divergent Branches","","0",
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ec60"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0eca0"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ecd0"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ed00"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0edd0"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ee00"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ee30"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ee60"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0eee0"
"66","11868","python3.7","127.0.0.1","void cutlass::Kernel<cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1>(cutlass_80_tensorop_s1688gemm_64x64_32x4_nn_align1::Params)","2021-Feb-26 11:37:20","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 170 sectors, got 680 (4.00x) at PC 0x7f595eb0ef00"
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,561,181,434.60",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,111,607,142.86",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,811",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","Memory [%]","%","2.95",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","SOL DRAM","%","0.06",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","Duration","nsecond","2,528",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.31",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","SOL L2 Cache","%","2.95",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","SM Active Cycles","cycle","763.56",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","GPU Speed Of Light","SM [%]","%","0.36",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.04",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.31",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.05",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Compute Workload Analysis","SM Busy","%","1.31",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","Memory Throughput","byte/second","405,063,291.14",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","Mem Busy","%","2.95",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","Max Bandwidth","%","2.26",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.87",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Scheduler Statistics","One or More Eligible","%","2.71",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Scheduler Statistics","No Eligible","%","97.29",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 36.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","36.29",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","46.97",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.93",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.06",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 78.8% of the total average of 36.3 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7.75",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Instruction Statistics","Executed Instructions","inst","2,543",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10.03",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Instruction Statistics","Issued Instructions","inst","3,291",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","NVLink","Logical Links","","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","NVLink","Physical Links","","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Block Size","","64",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Grid Size","","75",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Threads","thread","4,800",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Launch Statistics","Waves Per SM","","0.06",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 75 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Block Limit SM","block","16",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Block Limit Registers","block","64",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Block Limit Shared Mem","block","100",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Block Limit Warps","block","24",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Achieved Occupancy","%","4.14",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Source Counters","Branch Instructions","inst","301",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Source Counters","Branch Efficiency","%","100",
"67","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>)","2021-Feb-26 11:37:24","1","7","Source Counters","Avg. Divergent Branches","","0",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","9,118,796,992.48",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,338,319,011.82",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","28,497",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","Memory [%]","%","3.14",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","SOL DRAM","%","0.76",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","Duration","nsecond","21,280",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","9.92",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.53",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","SM Active Cycles","cycle","9,018.30",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","GPU Speed Of Light","SM [%]","%","5.68",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.3 full waves across all SMs. Look at Launch Statistics for more details."
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.70",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.22",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Compute Workload Analysis","Issue Slots Busy","%","17.94",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.72",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Compute Workload Analysis","SM Busy","%","17.94",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,610,526,315.79",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","Mem Busy","%","2.08",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","Max Bandwidth","%","3.14",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","84.68",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","L2 Hit Rate","%","42.96",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.14",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Scheduler Statistics","One or More Eligible","%","17.95",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.18",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Scheduler Statistics","No Eligible","%","82.05",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.58",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.23",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.58 active warps per scheduler, but only an average of 0.23 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.93",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.34",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.30",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.33",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 8.3 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 41.7% of the total average of 19.9 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1,584.85",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Instruction Statistics","Executed Instructions","inst","519,832",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1,617.85",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Instruction Statistics","Issued Instructions","inst","530,654",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","NVLink","Logical Links","","0",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","NVLink","Physical Links","","0",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Block Size","","256",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Grid Size","","170",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","512",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","544",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Threads","thread","43,520",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Launch Statistics","Waves Per SM","","0.35",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Block Limit SM","block","16",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Block Limit Registers","block","6",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Block Limit Shared Mem","block","47",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Block Limit Warps","block","6",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Theoretical Occupancy","%","100",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Achieved Occupancy","%","29.71",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Occupancy","Achieved Active Warps Per SM","warp","14.26",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Source Counters","Branch Instructions Ratio","%","0.28",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Source Counters","Branch Instructions","inst","143,705",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Source Counters","Branch Efficiency","%","94.52",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","Source Counters","Avg. Divergent Branches","","12.37",
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0c0"
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 2892 sectors, got 6399 (2.21x) at PC 0x7f59ce8cd0f0"
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 11725 sectors, got 13982 (1.19x) at PC 0x7f59ce8cd020"
"68","11868","python3.7","127.0.0.1","spmm_forward_cuda_kernel(int const*, int const*, int const*, int const*, int const*, int, int, int, float const*, float*)","2021-Feb-26 11:37:27","1","7","SourceCounters","","","","UncoalescedSharedAccess","WRN","Uncoalesced shared access, expected 2821 sectors, got 2875 (1.02x) at PC 0x7f59ce8cd190"
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,025,396,825.40",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,174,489,795.92",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,951",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","Memory [%]","%","4.67",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","SOL DRAM","%","2.98",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","Duration","nsecond","3,360",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","8.14",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","SOL L2 Cache","%","4.67",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,834.21",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","GPU Speed Of Light","SM [%]","%","3.96",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 1% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.33",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.15",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.51",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.34",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Compute Workload Analysis","SM Busy","%","8.51",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","Memory Throughput","byte/second","22,933,333,333.33",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","Mem Busy","%","4.67",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","Max Bandwidth","%","3.91",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","53.27",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","L2 Hit Rate","%","73.28",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Memory Workload Analysis","Mem Pipes Busy","%","3.78",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Scheduler Statistics","One or More Eligible","%","8.69",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Scheduler Statistics","No Eligible","%","91.31",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.03",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.03 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","11.80",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","12.28",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.70",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.47",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 4.3 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 36.8% of the total average of 11.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","150.09",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Instruction Statistics","Executed Instructions","inst","49,231",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","156.17",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Instruction Statistics","Issued Instructions","inst","51,223",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","NVLink","Logical Links","","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","NVLink","Physical Links","","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Block Size","","128",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Grid Size","","85",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Registers Per Thread","register/thread","17",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Threads","thread","10,880",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Launch Statistics","Waves Per SM","","0.09",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","LaunchStats","","","","LaunchConfiguration","WRN","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy."
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Block Limit SM","block","16",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Block Limit Registers","block","21",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Block Limit Shared Mem","block","100",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Block Limit Warps","block","12",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Theoretical Occupancy","%","100",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Achieved Occupancy","%","8.46",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.06",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Source Counters","Branch Instructions Ratio","%","0.03",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Source Counters","Branch Instructions","inst","1,357",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Source Counters","Branch Efficiency","%","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","Source Counters","Avg. Divergent Branches","","0",
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af68f180"
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2370 (1.75x) at PC 0x7f59af68f8f0"
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2369 (1.75x) at PC 0x7f59af68f130"
"69","11868","python3.7","127.0.0.1","void (anonymous namespace)::softmax_warp_forward<float, float, float, 3, true>(float*, float const*, int, int, int)","2021-Feb-26 11:37:30","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 1354 sectors, got 2369 (1.75x) at PC 0x7f59af68f6e0"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,131,455,399.06",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,199,289,486.92",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,451",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","Memory [%]","%","0.51",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","SOL DRAM","%","0.08",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.47",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.51",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","SM Active Cycles","cycle","44.02",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.35",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Compute Workload Analysis","Issue Slots Busy","%","9.07",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Compute Workload Analysis","SM Busy","%","9.07",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","Memory Throughput","byte/second","647,887,323.94",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","Mem Busy","%","0.51",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.30",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.08",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Scheduler Statistics","One or More Eligible","%","9.44",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Scheduler Statistics","No Eligible","%","90.56",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.09",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.09 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.12",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.89",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 44.5% of the total average of 22.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","NVLink","Logical Links","","0",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","NVLink","Physical Links","","0",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Block Size","","256",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Grid Size","","1",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Threads","thread","256",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Launch Statistics","Waves Per SM","","0.00",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Block Limit SM","block","16",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Block Limit Registers","block","6",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Block Limit Shared Mem","block","88",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Block Limit Warps","block","6",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Theoretical Occupancy","%","100",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Achieved Occupancy","%","16.07",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.72",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Source Counters","Branch Instructions","inst","154",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Source Counters","Branch Efficiency","%","98.86",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"70","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:34","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,684,210,526.32",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,124,236,372.18",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,738",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","85.22",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.56",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.80",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Compute Workload Analysis","SM Busy","%","2.80",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","Memory Throughput","byte/second","52,631,578.95",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.68",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Scheduler Statistics","One or More Eligible","%","2.94",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Scheduler Statistics","No Eligible","%","97.06",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.48",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.30",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 27.6 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 80.1% of the total average of 34.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Instruction Statistics","Executed Instructions","inst","81",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Instruction Statistics","Issued Instructions","inst","97",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","NVLink","Logical Links","","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","NVLink","Physical Links","","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Block Size","","128",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Grid Size","","1",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Threads","thread","128",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Launch Statistics","Waves Per SM","","0.00",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Block Limit SM","block","16",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Block Limit Registers","block","32",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Block Limit Shared Mem","block","100",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Block Limit Warps","block","12",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Theoretical Occupancy","%","100",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Achieved Occupancy","%","8.20",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.93",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Source Counters","Branch Instructions","inst","5",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Source Counters","Branch Efficiency","%","0",
"71","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:37","1","7","Source Counters","Avg. Divergent Branches","","0",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,678,486,997.64",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,123,353,596.76",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,070",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","Duration","nsecond","4,512",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.23",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","SM Active Cycles","cycle","168.28",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","GPU Speed Of Light","SM [%]","%","0.24",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.21",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.53",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.22",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Compute Workload Analysis","SM Busy","%","5.53",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","Memory Throughput","byte/second","680,851,063.83",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","68.61",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.96",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.24",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Scheduler Statistics","One or More Eligible","%","5.63",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Scheduler Statistics","No Eligible","%","94.37",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 17.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","17.86",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","18.87",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.71",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.56",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8.80",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Instruction Statistics","Executed Instructions","inst","2,888",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.30",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Instruction Statistics","Issued Instructions","inst","3,051",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","NVLink","Logical Links","","0",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","NVLink","Physical Links","","0",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Block Size","","128",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Grid Size","","5",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Threads","thread","640",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Launch Statistics","Waves Per SM","","0.01",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Block Limit SM","block","16",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Block Limit Registers","block","10",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Block Limit Shared Mem","block","16",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Block Limit Warps","block","12",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Achieved Occupancy","%","8.24",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.95",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Source Counters","Branch Instructions","inst","189",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Source Counters","Branch Efficiency","%","99.06",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"72","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:37:41","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,299,065,420.56",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,223,464,619.49",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,380",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","Memory [%]","%","0.41",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","Duration","nsecond","6,848",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.80",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.41",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","SM Active Cycles","cycle","155.29",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","GPU Speed Of Light","SM [%]","%","0.09",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.19",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.94",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Compute Workload Analysis","SM Busy","%","4.94",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","Memory Throughput","byte/second","785,046,728.97",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","Mem Busy","%","0.41",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","Max Bandwidth","%","0.19",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","8.14",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","L2 Hit Rate","%","90.04",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Scheduler Statistics","One or More Eligible","%","5.11",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Scheduler Statistics","No Eligible","%","94.89",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.59",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.17",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.62",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.41",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 50.0% of the total average of 19.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","7.45",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Instruction Statistics","Executed Instructions","inst","2,442",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","7.66",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Instruction Statistics","Issued Instructions","inst","2,514",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","NVLink","Logical Links","","0",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","NVLink","Physical Links","","0",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Block Size","","128",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Grid Size","","2",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Threads","thread","256",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Launch Statistics","Waves Per SM","","0.00",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Block Limit SM","block","16",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Block Limit Registers","block","16",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Block Limit Shared Mem","block","100",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Block Limit Warps","block","12",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Theoretical Occupancy","%","100",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Achieved Occupancy","%","8.15",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.91",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Source Counters","Branch Instructions","inst","280",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Source Counters","Branch Efficiency","%","100",
"73","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:37:44","1","7","Source Counters","Avg. Divergent Branches","","0",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,672,645,739.91",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","981,642,376.68",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","7,007",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","SOL DRAM","%","0.19",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","Duration","nsecond","7,136",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","11.07",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","114.52",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","GPU Speed Of Light","SM [%]","%","0.35",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.77",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","21.53",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.86",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Compute Workload Analysis","SM Busy","%","21.53",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,219,730,941.70",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","42.93",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","87.49",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.18",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Scheduler Statistics","One or More Eligible","%","21.87",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Scheduler Statistics","No Eligible","%","78.13",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.96",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.35",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.96 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.12",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.13",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","29.44",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.08",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.9 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 32.8% of the total average of 18.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","22.19",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Instruction Statistics","Executed Instructions","inst","7,277",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","24.65",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Instruction Statistics","Issued Instructions","inst","8,086",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","NVLink","Logical Links","","0",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","NVLink","Physical Links","","0",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Block Size","","512",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Grid Size","","2",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Threads","thread","1,024",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Launch Statistics","Waves Per SM","","0.01",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Block Limit SM","block","16",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Block Limit Registers","block","4",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Block Limit Shared Mem","block","88",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Block Limit Warps","block","3",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Theoretical Occupancy","%","100",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Achieved Occupancy","%","32.58",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.64",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Source Counters","Branch Instructions","inst","1,267",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Source Counters","Branch Efficiency","%","95.91",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","Source Counters","Avg. Divergent Branches","","0.11",
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 53 sectors, got 123 (2.32x) at PC 0x7f59935f73f0"
"74","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:37:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 70 sectors, got 123 (1.76x) at PC 0x7f59935f7390"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,807,511,737.09",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,295,334,507.04",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,888",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","Memory [%]","%","0.47",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","SOL DRAM","%","0.07",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.14",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.47",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","44.76",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","GPU Speed Of Light","SM [%]","%","0.07",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.34",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.92",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Compute Workload Analysis","SM Busy","%","8.92",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","619,718,309.86",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","Mem Busy","%","0.47",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.15",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.07",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Scheduler Statistics","One or More Eligible","%","9.38",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Scheduler Statistics","No Eligible","%","90.62",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.01",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.01 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.43",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.17",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.90",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 45.7% of the total average of 21.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","NVLink","Logical Links","","0",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","NVLink","Physical Links","","0",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Block Size","","256",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Grid Size","","1",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Threads","thread","256",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Launch Statistics","Waves Per SM","","0.00",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Block Limit SM","block","16",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Block Limit Registers","block","6",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Block Limit Shared Mem","block","88",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Block Limit Warps","block","6",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Theoretical Occupancy","%","100",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Achieved Occupancy","%","15.87",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.62",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Source Counters","Branch Instructions","inst","154",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Source Counters","Branch Efficiency","%","98.86",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"75","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:37:53","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,786,666,666.67",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,141,071,428.57",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,744",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","85.02",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.59",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.79",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Compute Workload Analysis","SM Busy","%","2.79",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Scheduler Statistics","One or More Eligible","%","2.89",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Scheduler Statistics","No Eligible","%","97.11",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.06",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.79",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.1 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 82.6% of the total average of 34.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Instruction Statistics","Executed Instructions","inst","81",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Instruction Statistics","Issued Instructions","inst","97",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","NVLink","Logical Links","","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","NVLink","Physical Links","","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Block Size","","128",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Grid Size","","1",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Threads","thread","128",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Launch Statistics","Waves Per SM","","0.00",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Block Limit SM","block","16",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Block Limit Registers","block","32",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Block Limit Shared Mem","block","100",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Block Limit Warps","block","12",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Theoretical Occupancy","%","100",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Achieved Occupancy","%","8.11",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.89",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Source Counters","Branch Instructions","inst","5",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Source Counters","Branch Efficiency","%","0",
"76","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:37:56","1","7","Source Counters","Avg. Divergent Branches","","0",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,680,751,173.71",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,129,338,531.19",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,135",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.16",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","SM Active Cycles","cycle","169.93",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","GPU Speed Of Light","SM [%]","%","0.24",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.21",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.50",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.22",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Compute Workload Analysis","SM Busy","%","5.50",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","Memory Throughput","byte/second","704,225,352.11",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","Max Bandwidth","%","0.24",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","67.52",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.45",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.24",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Scheduler Statistics","One or More Eligible","%","5.61",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Scheduler Statistics","No Eligible","%","94.39",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 17.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","17.83",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","18.94",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.85",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.71",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 6.0 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 33.8% of the total average of 17.8 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8.80",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Instruction Statistics","Executed Instructions","inst","2,888",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.35",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Instruction Statistics","Issued Instructions","inst","3,067",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","NVLink","Logical Links","","0",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","NVLink","Physical Links","","0",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Block Size","","128",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Grid Size","","5",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Threads","thread","640",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Launch Statistics","Waves Per SM","","0.01",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Block Limit SM","block","16",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Block Limit Registers","block","10",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Block Limit Shared Mem","block","16",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Block Limit Warps","block","12",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Achieved Occupancy","%","8.29",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.98",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Source Counters","Branch Instructions","inst","189",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Source Counters","Branch Efficiency","%","99.06",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"77","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:00","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,965,811,965.81",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,164,863,782.05",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,817",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","Memory [%]","%","0.50",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","SOL DRAM","%","0.05",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","Duration","nsecond","4,992",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","18.91",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.50",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","SM Active Cycles","cycle","47.59",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.10",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.73",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Compute Workload Analysis","SM Busy","%","2.73",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","Memory Throughput","byte/second","410,256,410.26",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","Mem Busy","%","0.50",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","L2 Hit Rate","%","94.93",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Scheduler Statistics","One or More Eligible","%","3.57",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Scheduler Statistics","No Eligible","%","96.43",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 28.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","28.00",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","30.27",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.82",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.34",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 32.0% of the total average of 28.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.20",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Instruction Statistics","Executed Instructions","inst","394",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.30",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Instruction Statistics","Issued Instructions","inst","426",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","NVLink","Logical Links","","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","NVLink","Physical Links","","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Block Size","","128",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Grid Size","","1",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Threads","thread","128",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Launch Statistics","Waves Per SM","","0.00",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Block Limit SM","block","16",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Block Limit Registers","block","16",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Block Limit Shared Mem","block","100",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Block Limit Warps","block","12",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Theoretical Occupancy","%","100",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Achieved Occupancy","%","6.43",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.08",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Source Counters","Branch Instructions Ratio","%","0.15",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Source Counters","Branch Instructions","inst","59",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Source Counters","Branch Efficiency","%","97.30",
"78","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:03","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,882,783,882.78",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,154,729,199.37",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,365",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","Memory [%]","%","0.82",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","Duration","nsecond","2,912",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","49.13",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.82",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","SM Active Cycles","cycle","18.32",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.31",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Compute Workload Analysis","SM Busy","%","2.31",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","Memory Throughput","byte/second","791,208,791.21",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","Mem Busy","%","0.82",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","2.67",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","L2 Hit Rate","%","94.38",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Scheduler Statistics","One or More Eligible","%","4.70",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Scheduler Statistics","No Eligible","%","95.30",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.06",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.05",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.54",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.14",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 9.7 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.9% of the total average of 21.1 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 6.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 30.8% of the total average of 21.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.39",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Instruction Statistics","Executed Instructions","inst","127",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.42",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Instruction Statistics","Issued Instructions","inst","139",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","NVLink","Logical Links","","0",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","NVLink","Physical Links","","0",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Block Size","","64",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Grid Size","","1",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Registers Per Thread","register/thread","26",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Threads","thread","64",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Launch Statistics","Waves Per SM","","0.00",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Block Limit SM","block","16",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Block Limit Registers","block","32",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Block Limit Shared Mem","block","100",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Block Limit Warps","block","24",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Achieved Occupancy","%","4.08",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.96",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Source Counters","Branch Instructions Ratio","%","0.17",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Source Counters","Branch Instructions","inst","21",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Source Counters","Branch Efficiency","%","90.91",
"79","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:07","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,832,167,832.17",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,150,505,744.26",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,266",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","Memory [%]","%","0.51",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","Duration","nsecond","4,576",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","21.75",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.51",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","SM Active Cycles","cycle","41.38",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","GPU Speed Of Light","SM [%]","%","0.02",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.08",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.97",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Compute Workload Analysis","SM Busy","%","2.36",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","Memory Throughput","byte/second","167,832,167.83",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","Mem Busy","%","0.51",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.22",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Scheduler Statistics","One or More Eligible","%","4.50",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Scheduler Statistics","No Eligible","%","95.50",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.25",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.29",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.33",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.58",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 35.6% of the total average of 22.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.78",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Instruction Statistics","Executed Instructions","inst","255",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.81",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Instruction Statistics","Issued Instructions","inst","267",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","NVLink","Logical Links","","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","NVLink","Physical Links","","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Block Size","","64",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Grid Size","","1",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Threads","thread","64",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Launch Statistics","Waves Per SM","","0.00",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Block Limit SM","block","16",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Block Limit Registers","block","32",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Block Limit Shared Mem","block","100",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Block Limit Warps","block","24",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Achieved Occupancy","%","3.65",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.75",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Source Counters","Branch Instructions","inst","69",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Source Counters","Branch Efficiency","%","98.31",
"80","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:11","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,830,107,526.88",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,004,579,493.09",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,985",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","Memory [%]","%","0.54",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","SOL DRAM","%","0.04",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","Duration","nsecond","4,960",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","23.49",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.54",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","SM Active Cycles","cycle","38.48",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.25",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Compute Workload Analysis","Issue Slots Busy","%","6.86",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.27",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Compute Workload Analysis","SM Busy","%","6.86",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","Memory Throughput","byte/second","283,870,967.74",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","Mem Busy","%","0.54",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.30",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","L2 Hit Rate","%","96.98",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Scheduler Statistics","One or More Eligible","%","7.01",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.07",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Scheduler Statistics","No Eligible","%","92.99",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.07",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 14.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","14.18",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","15.66",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.93",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.66",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.5 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 39.0% of the total average of 14.2 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2.39",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Instruction Statistics","Executed Instructions","inst","784",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2.64",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Instruction Statistics","Issued Instructions","inst","866",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","NVLink","Logical Links","","0",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","NVLink","Physical Links","","0",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Block Size","","128",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Grid Size","","1",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","1,024",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Threads","thread","128",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Launch Statistics","Waves Per SM","","0.00",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Block Limit SM","block","16",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Block Limit Registers","block","16",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Block Limit Shared Mem","block","47",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Block Limit Warps","block","12",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Theoretical Occupancy","%","100",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Achieved Occupancy","%","8.17",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.92",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Source Counters","Branch Instructions Ratio","%","0.15",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Source Counters","Branch Instructions","inst","115",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Source Counters","Branch Efficiency","%","98.91",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 18 sectors, got 35 (1.94x) at PC 0x7f597f31ef40"
"81","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:14","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 18 sectors, got 35 (1.94x) at PC 0x7f597f31ef50"
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,682,539,682.54",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,128,002,763.61",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,065",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","Memory [%]","%","0.74",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","Duration","nsecond","5,376",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.41",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.74",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","SM Active Cycles","cycle","530.84",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","GPU Speed Of Light","SM [%]","%","0.27",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.32",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Compute Workload Analysis","SM Busy","%","3.12",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","Memory Throughput","byte/second","666,666,666.67",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","Mem Busy","%","0.74",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","Max Bandwidth","%","0.40",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","L2 Hit Rate","%","95.85",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.11",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Scheduler Statistics","One or More Eligible","%","4.72",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Scheduler Statistics","No Eligible","%","95.28",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.24",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.95",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.85",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.39",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 43.4% of the total average of 21.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.94",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Instruction Statistics","Executed Instructions","inst","3,915",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.34",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Instruction Statistics","Issued Instructions","inst","4,047",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","NVLink","Logical Links","","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","NVLink","Physical Links","","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Block Size","","64",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Grid Size","","11",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Threads","thread","704",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Launch Statistics","Waves Per SM","","0.01",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 11 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Block Limit SM","block","16",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Block Limit Registers","block","32",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Block Limit Shared Mem","block","100",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Block Limit Warps","block","24",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Achieved Occupancy","%","4.14",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Source Counters","Branch Instructions","inst","1,049",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Source Counters","Branch Efficiency","%","99.89",
"82","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:38:18","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,416,666,666.67",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,092,285,156.25",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,714",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","SOL DRAM","%","0.50",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","Duration","nsecond","6,144",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","22.21",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","SM Active Cycles","cycle","59.40",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","GPU Speed Of Light","SM [%]","%","0.20",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.77",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Compute Workload Analysis","Issue Slots Busy","%","20.80",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.83",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Compute Workload Analysis","SM Busy","%","20.80",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,541,666,666.67",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","Max Bandwidth","%","0.50",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.96",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","L2 Hit Rate","%","65.38",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.20",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Scheduler Statistics","One or More Eligible","%","21.28",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Scheduler Statistics","No Eligible","%","78.72",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.95",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.31",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.95 active warps per scheduler, but only an average of 0.31 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.58",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.95",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.79",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.39",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.50",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Instruction Statistics","Executed Instructions","inst","3,772",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.35",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Instruction Statistics","Issued Instructions","inst","4,052",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","NVLink","Logical Links","","0",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","NVLink","Physical Links","","0",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Block Size","","512",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Grid Size","","1",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Threads","thread","512",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Launch Statistics","Waves Per SM","","0.00",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Block Limit SM","block","16",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Block Limit Registers","block","4",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Block Limit Shared Mem","block","19",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Block Limit Warps","block","3",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Theoretical Occupancy","%","100",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Achieved Occupancy","%","32.14",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.43",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Source Counters","Branch Instructions Ratio","%","0.13",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Source Counters","Branch Instructions","inst","491",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Source Counters","Branch Efficiency","%","99.75",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 339 sectors, got 677 (2.00x) at PC 0x7f597f31ef40"
"83","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:38:22","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 339 sectors, got 677 (2.00x) at PC 0x7f597f31ef50"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,244,131,455.40",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,212,588,028.17",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,511",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","Memory [%]","%","0.50",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","SOL DRAM","%","0.12",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","Duration","nsecond","4,544",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.94",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.50",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","SM Active Cycles","cycle","45.21",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.34",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.83",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.35",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Compute Workload Analysis","SM Busy","%","8.83",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","Memory Throughput","byte/second","957,746,478.87",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","Mem Busy","%","0.50",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.30",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.08",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Scheduler Statistics","One or More Eligible","%","9.19",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Scheduler Statistics","No Eligible","%","90.81",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.99",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 10.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.99 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.66",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.41",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.60",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 48.2% of the total average of 21.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","NVLink","Logical Links","","0",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","NVLink","Physical Links","","0",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Block Size","","256",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Grid Size","","1",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Threads","thread","256",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Launch Statistics","Waves Per SM","","0.00",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Block Limit SM","block","16",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Block Limit Registers","block","6",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Block Limit Shared Mem","block","88",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Block Limit Warps","block","6",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Theoretical Occupancy","%","100",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Achieved Occupancy","%","16.02",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.69",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Source Counters","Branch Instructions","inst","154",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Source Counters","Branch Efficiency","%","98.86",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"84","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:26","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,822,222,222.22",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,143,690,476.19",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,749",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","85.02",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.59",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.79",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Compute Workload Analysis","SM Busy","%","2.79",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.68",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Scheduler Statistics","One or More Eligible","%","2.93",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Scheduler Statistics","No Eligible","%","97.07",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.57",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.40",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.6% of the total average of 34.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Instruction Statistics","Executed Instructions","inst","81",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Instruction Statistics","Issued Instructions","inst","97",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","NVLink","Logical Links","","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","NVLink","Physical Links","","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Block Size","","128",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Grid Size","","1",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Threads","thread","128",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Launch Statistics","Waves Per SM","","0.00",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Block Limit SM","block","16",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Block Limit Registers","block","32",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Block Limit Shared Mem","block","100",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Block Limit Warps","block","12",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Theoretical Occupancy","%","100",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Achieved Occupancy","%","8.21",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.94",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Source Counters","Branch Instructions","inst","5",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Source Counters","Branch Efficiency","%","0",
"85","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:29","1","7","Source Counters","Avg. Divergent Branches","","0",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,160,839,160.84",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,051,479,770.23",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,813",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","Memory [%]","%","0.69",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","Duration","nsecond","4,576",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.57",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.69",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","SM Active Cycles","cycle","169.16",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","GPU Speed Of Light","SM [%]","%","0.27",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.21",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.66",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.23",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Compute Workload Analysis","SM Busy","%","5.66",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","Memory Throughput","byte/second","699,300,699.30",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","Mem Busy","%","0.69",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","Max Bandwidth","%","0.27",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","57.45",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.87",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.27",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Scheduler Statistics","One or More Eligible","%","5.78",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Scheduler Statistics","No Eligible","%","94.22",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.05",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 17.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.05 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.20",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.38",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.68",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.72",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 5.6 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 30.5% of the total average of 18.2 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","8.99",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Instruction Statistics","Executed Instructions","inst","2,949",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.58",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Instruction Statistics","Issued Instructions","inst","3,141",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","NVLink","Logical Links","","0",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","NVLink","Physical Links","","0",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Block Size","","128",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Grid Size","","5",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Threads","thread","640",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Launch Statistics","Waves Per SM","","0.01",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Block Limit SM","block","16",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Block Limit Registers","block","10",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Block Limit Shared Mem","block","16",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Block Limit Warps","block","12",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Achieved Occupancy","%","8.62",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.14",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Source Counters","Branch Instructions","inst","207",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Source Counters","Branch Efficiency","%","99.12",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"86","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:32","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,316,590,563.17",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,222,969,667.32",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,574",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","Memory [%]","%","0.65",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","SOL DRAM","%","0.33",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","Duration","nsecond","7,008",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.06",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.65",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","SM Active Cycles","cycle","550.61",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","GPU Speed Of Light","SM [%]","%","0.32",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.19",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.94",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Compute Workload Analysis","SM Busy","%","4.94",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,648,401,826.48",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","Mem Busy","%","0.65",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","16.90",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","L2 Hit Rate","%","78.91",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.16",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Scheduler Statistics","One or More Eligible","%","5.02",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Scheduler Statistics","No Eligible","%","94.98",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.97",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.55",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.82",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.59",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 50.5% of the total average of 20.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","26.41",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Instruction Statistics","Executed Instructions","inst","8,664",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","27.18",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Instruction Statistics","Issued Instructions","inst","8,916",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","NVLink","Logical Links","","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","NVLink","Physical Links","","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Block Size","","128",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Grid Size","","7",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Threads","thread","896",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Launch Statistics","Waves Per SM","","0.01",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 7 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Block Limit SM","block","16",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Block Limit Registers","block","16",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Block Limit Shared Mem","block","100",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Block Limit Warps","block","12",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Theoretical Occupancy","%","100",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Achieved Occupancy","%","8.18",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.93",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Source Counters","Branch Instructions","inst","992",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Source Counters","Branch Efficiency","%","100",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","Source Counters","Avg. Divergent Branches","","0",
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 112 sectors, got 140 (1.25x) at PC 0x7f5979a86d20"
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 112 sectors, got 140 (1.25x) at PC 0x7f5979a88150"
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 112 sectors, got 140 (1.25x) at PC 0x7f5979a89570"
"87","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:36","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 102 sectors, got 127 (1.25x) at PC 0x7f5979a8aa00"
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,443,438,914.03",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","SM Frequency","cycle/second","946,307,369.10",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,696",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","Memory [%]","%","0.98",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","SOL DRAM","%","0.52",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","Duration","nsecond","7,072",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","11.89",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.98",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","SM Active Cycles","cycle","235.45",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","GPU Speed Of Light","SM [%]","%","0.95",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.99",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.03",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Compute Workload Analysis","Issue Slots Busy","%","26.90",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.08",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Compute Workload Analysis","SM Busy","%","26.90",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,221,719,457.01",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","Mem Busy","%","0.98",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","Max Bandwidth","%","0.52",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","44.59",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","L2 Hit Rate","%","77.84",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.42",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Scheduler Statistics","One or More Eligible","%","27.21",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Scheduler Statistics","No Eligible","%","72.79",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.99",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.45",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.45 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","14.65",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","15.99",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.87",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.21",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 34.0% of the total average of 14.6 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","58.03",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Instruction Statistics","Executed Instructions","inst","19,035",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","63.34",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Instruction Statistics","Issued Instructions","inst","20,777",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","NVLink","Logical Links","","0",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","NVLink","Physical Links","","0",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Block Size","","512",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Grid Size","","4",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Threads","thread","2,048",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Launch Statistics","Waves Per SM","","0.02",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Block Limit SM","block","16",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Block Limit Registers","block","4",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Block Limit Shared Mem","block","88",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Block Limit Warps","block","3",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Theoretical Occupancy","%","100",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Achieved Occupancy","%","33.42",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Occupancy","Achieved Active Warps Per SM","warp","16.04",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Source Counters","Branch Instructions","inst","3,424",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Source Counters","Branch Efficiency","%","94.59",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","Source Counters","Avg. Divergent Branches","","0.39",
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 188 sectors, got 438 (2.33x) at PC 0x7f59935f73f0"
"88","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:38:40","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 250 sectors, got 438 (1.75x) at PC 0x7f59935f7390"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,589,743,589.74",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,115,499,084.25",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,571",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","Memory [%]","%","0.50",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","Duration","nsecond","4,992",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","20.15",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.50",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","SM Active Cycles","cycle","44.73",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.34",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.92",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.36",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Compute Workload Analysis","SM Busy","%","8.92",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","Memory Throughput","byte/second","717,948,717.95",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","Mem Busy","%","0.50",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.15",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.08",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Scheduler Statistics","One or More Eligible","%","9.11",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Scheduler Statistics","No Eligible","%","90.89",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.99",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.99 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.90",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.66",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.60",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 45.8% of the total average of 21.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","NVLink","Logical Links","","0",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","NVLink","Physical Links","","0",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Block Size","","256",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Grid Size","","1",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Threads","thread","256",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Launch Statistics","Waves Per SM","","0.00",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Block Limit SM","block","16",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Block Limit Registers","block","6",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Block Limit Shared Mem","block","88",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Block Limit Warps","block","6",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Theoretical Occupancy","%","100",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Achieved Occupancy","%","16.53",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.94",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Source Counters","Branch Instructions","inst","154",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Source Counters","Branch Efficiency","%","98.86",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"89","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:38:44","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,649,122,807.02",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,121,945,488.72",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,732",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","Duration","nsecond","2,432",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","83.96",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.72",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.76",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Compute Workload Analysis","SM Busy","%","2.76",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","Memory Throughput","byte/second","105,263,157.89",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Scheduler Statistics","One or More Eligible","%","2.93",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Scheduler Statistics","No Eligible","%","97.07",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.03",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.75",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 82.9% of the total average of 34.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Instruction Statistics","Executed Instructions","inst","81",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Instruction Statistics","Issued Instructions","inst","97",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","NVLink","Logical Links","","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","NVLink","Physical Links","","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Block Size","","128",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Grid Size","","1",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Threads","thread","128",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Launch Statistics","Waves Per SM","","0.00",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Block Limit SM","block","16",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Block Limit Registers","block","32",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Block Limit Shared Mem","block","100",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Block Limit Warps","block","12",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Theoretical Occupancy","%","100",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Achieved Occupancy","%","8.09",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.89",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Source Counters","Branch Instructions","inst","5",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Source Counters","Branch Efficiency","%","0",
"90","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:38:48","1","7","Source Counters","Avg. Divergent Branches","","0",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,310,023,310.02",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,071,210,039.96",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","4,904",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","Memory [%]","%","0.67",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","SOL DRAM","%","0.10",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","Duration","nsecond","4,576",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.48",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.67",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","SM Active Cycles","cycle","170.63",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","GPU Speed Of Light","SM [%]","%","0.26",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.21",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.61",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.22",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Compute Workload Analysis","SM Busy","%","5.61",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","Memory Throughput","byte/second","727,272,727.27",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","Mem Busy","%","0.67",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","Max Bandwidth","%","0.26",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","57.65",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.87",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Scheduler Statistics","One or More Eligible","%","5.12",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Scheduler Statistics","No Eligible","%","94.88",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.94",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.94 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.43",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.54",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.54",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","26.60",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 6.1 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 33.2% of the total average of 18.4 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9.03",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Instruction Statistics","Executed Instructions","inst","2,962",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","9.58",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Instruction Statistics","Issued Instructions","inst","3,141",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","NVLink","Logical Links","","0",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","NVLink","Physical Links","","0",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Block Size","","128",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Grid Size","","5",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Threads","thread","640",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Launch Statistics","Waves Per SM","","0.01",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Block Limit SM","block","16",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Block Limit Registers","block","10",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Block Limit Shared Mem","block","16",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Block Limit Warps","block","12",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Achieved Occupancy","%","8.33",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Source Counters","Branch Instructions Ratio","%","0.07",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Source Counters","Branch Instructions","inst","204",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Source Counters","Branch Efficiency","%","99.10",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"91","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:38:51","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,163,522,012.58",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,202,240,566.04",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,158",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","Memory [%]","%","0.43",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","SOL DRAM","%","0.17",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","Duration","nsecond","6,784",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","11.74",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.43",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","SM Active Cycles","cycle","76.67",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","GPU Speed Of Light","SM [%]","%","0.04",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.17",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.45",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.18",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Compute Workload Analysis","SM Busy","%","4.52",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,339,622,641.51",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","Mem Busy","%","0.43",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","L2 Hit Rate","%","84.78",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Scheduler Statistics","One or More Eligible","%","4.49",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Scheduler Statistics","No Eligible","%","95.51",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.26",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.91",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.26",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.04",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 11.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 52.4% of the total average of 22.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.32",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Instruction Statistics","Executed Instructions","inst","1,088",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.41",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Instruction Statistics","Issued Instructions","inst","1,120",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","NVLink","Logical Links","","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","NVLink","Physical Links","","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Block Size","","128",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Grid Size","","1",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Threads","thread","128",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Launch Statistics","Waves Per SM","","0.00",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Block Limit SM","block","16",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Block Limit Registers","block","16",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Block Limit Shared Mem","block","100",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Block Limit Warps","block","12",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Theoretical Occupancy","%","100",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Achieved Occupancy","%","8.27",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.97",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Source Counters","Branch Instructions","inst","128",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Source Counters","Branch Efficiency","%","100",
"92","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:38:54","1","7","Source Counters","Avg. Divergent Branches","","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,971,014,492.75",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,169,012,034.16",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,444",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","Memory [%]","%","0.93",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","SOL DRAM","%","0.36",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","Duration","nsecond","2,944",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","24.26",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.93",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","SM Active Cycles","cycle","37.10",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","GPU Speed Of Light","SM [%]","%","0.03",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.07",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.97",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.08",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Compute Workload Analysis","SM Busy","%","1.97",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,782,608,695.65",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","Mem Busy","%","0.93",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","Max Bandwidth","%","0.40",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","33.50",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","L2 Hit Rate","%","83.45",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.03",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Scheduler Statistics","One or More Eligible","%","3.96",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Scheduler Statistics","No Eligible","%","96.04",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 25.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","25.00",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","27.53",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.78",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.14",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 11.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 44.8% of the total average of 25.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 36.3% of the total average of 25.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.66",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Instruction Statistics","Executed Instructions","inst","218",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.73",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Instruction Statistics","Issued Instructions","inst","240",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","NVLink","Logical Links","","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","NVLink","Physical Links","","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Block Size","","64",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Grid Size","","2",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Registers Per Thread","register/thread","26",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Threads","thread","128",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Launch Statistics","Waves Per SM","","0.00",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Block Limit SM","block","16",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Block Limit Registers","block","32",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Block Limit Shared Mem","block","100",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Block Limit Warps","block","24",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Achieved Occupancy","%","4.15",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.99",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Source Counters","Branch Instructions","inst","26",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Source Counters","Branch Efficiency","%","100",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","Source Counters","Avg. Divergent Branches","","0",
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f597bf2a1d0"
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f597bf2a1e0"
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f597bf2a1f0"
"93","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:38:58","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 32 sectors, got 64 (2.00x) at PC 0x7f597bf2a200"
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,802,469,135.80",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,150,049,603.17",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,964",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","Memory [%]","%","0.49",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","Duration","nsecond","5,184",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","9.29",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.49",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","SM Active Cycles","cycle","96.93",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","GPU Speed Of Light","SM [%]","%","0.05",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.38",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Compute Workload Analysis","SM Busy","%","3.22",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","Memory Throughput","byte/second","123,456,790.12",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","Mem Busy","%","0.49",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","Max Bandwidth","%","0.18",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","L2 Hit Rate","%","98.58",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.02",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Scheduler Statistics","One or More Eligible","%","4.80",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Scheduler Statistics","No Eligible","%","95.20",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 20.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.89",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.57",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.37",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.96",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 8.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 41.6% of the total average of 20.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2.23",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Instruction Statistics","Executed Instructions","inst","733",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2.31",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Instruction Statistics","Issued Instructions","inst","757",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","NVLink","Logical Links","","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","NVLink","Physical Links","","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Block Size","","64",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Grid Size","","2",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Threads","thread","128",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Launch Statistics","Waves Per SM","","0.00",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Block Limit SM","block","16",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Block Limit Registers","block","32",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Block Limit Shared Mem","block","100",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Block Limit Warps","block","24",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Achieved Occupancy","%","4.16",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Source Counters","Branch Instructions","inst","197",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Source Counters","Branch Efficiency","%","99.42",
"94","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:02","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,956,521,739.13",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,022,737,355.81",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,271",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","Memory [%]","%","0.55",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","SOL DRAM","%","0.12",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","Duration","nsecond","5,152",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","21.33",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.55",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","SM Active Cycles","cycle","42.43",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","GPU Speed Of Light","SM [%]","%","0.11",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.48",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Compute Workload Analysis","Issue Slots Busy","%","13.05",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.52",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Compute Workload Analysis","SM Busy","%","13.05",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","Memory Throughput","byte/second","819,875,776.40",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","Mem Busy","%","0.55",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","Max Bandwidth","%","0.21",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.80",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","L2 Hit Rate","%","90.70",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.11",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Scheduler Statistics","One or More Eligible","%","13.59",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.14",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Scheduler Statistics","No Eligible","%","86.41",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","2.04",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.16",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 2.04 active warps per scheduler, but only an average of 0.16 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","15.00",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.33",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.75",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.80",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 34.8% of the total average of 15.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","5.09",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Instruction Statistics","Executed Instructions","inst","1,668",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","5.54",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Instruction Statistics","Issued Instructions","inst","1,816",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","NVLink","Logical Links","","0",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","NVLink","Physical Links","","0",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Block Size","","256",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Grid Size","","1",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Shared Memory Configuration Size","byte","32,768",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","2,048",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Threads","thread","256",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Launch Statistics","Waves Per SM","","0.00",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Block Limit SM","block","16",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Block Limit Registers","block","8",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Block Limit Shared Mem","block","32",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Block Limit Warps","block","6",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Theoretical Occupancy","%","100",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Achieved Occupancy","%","16.07",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.72",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Source Counters","Branch Instructions Ratio","%","0.14",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Source Counters","Branch Instructions","inst","233",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Source Counters","Branch Efficiency","%","99.47",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 63 sectors, got 125 (1.98x) at PC 0x7f597f31ef40"
"95","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:06","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 63 sectors, got 125 (1.98x) at PC 0x7f597f31ef50"
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,046,783,625.73",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,183,348,997.49",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,477",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","Memory [%]","%","0.69",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","SOL DRAM","%","0.08",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","Duration","nsecond","5,472",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.33",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.69",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","SM Active Cycles","cycle","548.33",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","GPU Speed Of Light","SM [%]","%","0.26",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.25",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Compute Workload Analysis","SM Busy","%","3.02",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","Memory Throughput","byte/second","654,970,760.23",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","Mem Busy","%","0.69",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","Max Bandwidth","%","0.38",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","L2 Hit Rate","%","95.81",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.11",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Scheduler Statistics","One or More Eligible","%","4.57",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Scheduler Statistics","No Eligible","%","95.43",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.80",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.53",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.85",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.39",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 44.2% of the total average of 21.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.94",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Instruction Statistics","Executed Instructions","inst","3,915",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.34",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Instruction Statistics","Issued Instructions","inst","4,047",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","NVLink","Logical Links","","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","NVLink","Physical Links","","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Block Size","","64",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Grid Size","","11",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Threads","thread","704",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Launch Statistics","Waves Per SM","","0.01",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 11 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Block Limit SM","block","16",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Block Limit Registers","block","32",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Block Limit Shared Mem","block","100",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Block Limit Warps","block","24",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Achieved Occupancy","%","4.11",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.97",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Source Counters","Branch Instructions","inst","1,049",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Source Counters","Branch Efficiency","%","99.89",
"96","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:09","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,958,115,183.25",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,169,479,244.58",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","7,149",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","Memory [%]","%","0.57",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","SOL DRAM","%","0.47",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","Duration","nsecond","6,112",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","22.50",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.57",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","SM Active Cycles","cycle","58.63",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","GPU Speed Of Light","SM [%]","%","0.18",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.78",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Compute Workload Analysis","Issue Slots Busy","%","21.07",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.84",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Compute Workload Analysis","SM Busy","%","21.07",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,560,209,424.08",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","Mem Busy","%","0.57",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","Max Bandwidth","%","0.47",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.96",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","L2 Hit Rate","%","65.29",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.18",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Scheduler Statistics","One or More Eligible","%","21.53",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.22",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Scheduler Statistics","No Eligible","%","78.47",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.02",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.31",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.02 active warps per scheduler, but only an average of 0.31 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.69",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.08",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.79",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.39",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.50",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Instruction Statistics","Executed Instructions","inst","3,772",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.35",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Instruction Statistics","Issued Instructions","inst","4,052",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","NVLink","Logical Links","","0",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","NVLink","Physical Links","","0",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Block Size","","512",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Grid Size","","1",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Threads","thread","512",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Launch Statistics","Waves Per SM","","0.00",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Block Limit SM","block","16",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Block Limit Registers","block","4",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Block Limit Shared Mem","block","19",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Block Limit Warps","block","3",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Theoretical Occupancy","%","100",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Achieved Occupancy","%","32.42",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.56",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Source Counters","Branch Instructions Ratio","%","0.13",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Source Counters","Branch Instructions","inst","491",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Source Counters","Branch Efficiency","%","99.75",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 339 sectors, got 677 (2.00x) at PC 0x7f597f31ef40"
"97","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:13","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 339 sectors, got 677 (2.00x) at PC 0x7f597f31ef50"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,833,333,333.33",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,304,501,488.10",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,015",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","Memory [%]","%","0.46",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","SOL DRAM","%","0.07",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","Duration","nsecond","4,608",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.91",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.46",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","SM Active Cycles","cycle","45.26",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","GPU Speed Of Light","SM [%]","%","0.07",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.34",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.82",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.35",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Compute Workload Analysis","SM Busy","%","8.82",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","Memory Throughput","byte/second","611,111,111.11",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","Mem Busy","%","0.46",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","Max Bandwidth","%","0.16",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.23",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.07",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Scheduler Statistics","One or More Eligible","%","9.06",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.09",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Scheduler Statistics","No Eligible","%","90.94",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.99",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.10",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.99 active warps per scheduler, but only an average of 0.10 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.99",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.75",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.11",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 47.2% of the total average of 22.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","NVLink","Logical Links","","0",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","NVLink","Physical Links","","0",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Block Size","","256",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Grid Size","","1",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Threads","thread","256",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Launch Statistics","Waves Per SM","","0.00",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Block Limit SM","block","16",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Block Limit Registers","block","6",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Block Limit Shared Mem","block","88",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Block Limit Warps","block","6",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Theoretical Occupancy","%","100",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Achieved Occupancy","%","16.18",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Occupancy","Achieved Active Warps Per SM","warp","7.77",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Source Counters","Branch Instructions","inst","154",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Source Counters","Branch Efficiency","%","98.86",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"98","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:17","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,715,555,555.56",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,145,654,761.90",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,753",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","Memory [%]","%","0.94",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","SOL DRAM","%","0.01",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","85.02",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.94",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.59",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.79",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Compute Workload Analysis","SM Busy","%","2.79",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","Memory Throughput","byte/second","53,333,333.33",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","Mem Busy","%","0.94",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.52",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Scheduler Statistics","One or More Eligible","%","2.89",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Scheduler Statistics","No Eligible","%","97.11",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.98",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 34.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","33.98",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.69",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 83.1% of the total average of 34.0 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Instruction Statistics","Executed Instructions","inst","81",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Instruction Statistics","Issued Instructions","inst","97",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","NVLink","Logical Links","","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","NVLink","Physical Links","","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Block Size","","128",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Grid Size","","1",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Threads","thread","128",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Launch Statistics","Waves Per SM","","0.00",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Block Limit SM","block","16",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Block Limit Registers","block","32",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Block Limit Shared Mem","block","100",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Block Limit Warps","block","12",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Theoretical Occupancy","%","100",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Achieved Occupancy","%","8.08",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.88",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Source Counters","Branch Instructions","inst","5",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Source Counters","Branch Efficiency","%","0",
"99","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:20","1","7","Source Counters","Avg. Divergent Branches","","0",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,578,947,368.42",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,113,574,953.01",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,419",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","Memory [%]","%","0.66",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","Duration","nsecond","4,864",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.79",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.66",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","SM Active Cycles","cycle","178.83",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","GPU Speed Of Light","SM [%]","%","0.26",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.22",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.82",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.23",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Compute Workload Analysis","SM Busy","%","5.82",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","Memory Throughput","byte/second","657,894,736.84",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","Mem Busy","%","0.66",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","55.14",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","L2 Hit Rate","%","94.24",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Scheduler Statistics","One or More Eligible","%","5.92",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Scheduler Statistics","No Eligible","%","94.08",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 16.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.83",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.76",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.82",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.66",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 5.4 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 32.4% of the total average of 16.8 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9.86",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Instruction Statistics","Executed Instructions","inst","3,233",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10.40",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Instruction Statistics","Issued Instructions","inst","3,412",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","NVLink","Logical Links","","0",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","NVLink","Physical Links","","0",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Block Size","","128",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Grid Size","","5",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Threads","thread","640",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Launch Statistics","Waves Per SM","","0.01",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Block Limit SM","block","16",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Block Limit Registers","block","10",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Block Limit Shared Mem","block","16",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Block Limit Warps","block","12",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Achieved Occupancy","%","8.20",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.94",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Source Counters","Branch Instructions Ratio","%","0.08",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Source Counters","Branch Instructions","inst","254",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Source Counters","Branch Efficiency","%","98.53",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"100","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:23","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,430,107,526.88",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,240,618,828.18",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,616",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","Memory [%]","%","0.91",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","SOL DRAM","%","0.65",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","Duration","nsecond","6,944",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","1.72",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.91",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","SM Active Cycles","cycle","1,071.57",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","GPU Speed Of Light","SM [%]","%","0.63",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.20",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.02",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.05",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.20",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Compute Workload Analysis","SM Busy","%","5.05",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","Memory Throughput","byte/second","5,253,456,221.20",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","Mem Busy","%","0.91",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","Max Bandwidth","%","0.65",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","16.83",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","L2 Hit Rate","%","69.93",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.31",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Scheduler Statistics","One or More Eligible","%","5.13",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Scheduler Statistics","No Eligible","%","94.87",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 19.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.47",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.04",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.96",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.72",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 50.4% of the total average of 19.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","52.61",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Instruction Statistics","Executed Instructions","inst","17,257",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","54.15",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Instruction Statistics","Issued Instructions","inst","17,761",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","NVLink","Logical Links","","0",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","NVLink","Physical Links","","0",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Block Size","","128",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Grid Size","","14",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Threads","thread","1,792",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Launch Statistics","Waves Per SM","","0.01",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 14 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Block Limit SM","block","16",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Block Limit Registers","block","16",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Block Limit Shared Mem","block","100",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Block Limit Warps","block","12",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Theoretical Occupancy","%","100",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Achieved Occupancy","%","8.22",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.94",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Source Counters","Branch Instructions Ratio","%","0.11",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Source Counters","Branch Instructions","inst","1,978",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Source Counters","Branch Efficiency","%","99.93",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 224 sectors, got 280 (1.25x) at PC 0x7f5979a86d20"
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 224 sectors, got 280 (1.25x) at PC 0x7f5979a88150"
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 219 sectors, got 274 (1.25x) at PC 0x7f5979a89570"
"101","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<4> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:27","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 208 sectors, got 260 (1.25x) at PC 0x7f5979a8aa00"
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","6,467,571,644.04",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","SM Frequency","cycle/second","952,448,287.01",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,742",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","Memory [%]","%","1.56",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","SOL DRAM","%","1.02",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","Duration","nsecond","7,072",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","11.79",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.56",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","SM Active Cycles","cycle","474.07",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","GPU Speed Of Light","SM [%]","%","1.87",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.97",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.07",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Compute Workload Analysis","Issue Slots Busy","%","26.61",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.06",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Compute Workload Analysis","SM Busy","%","26.61",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","Memory Throughput","byte/second","6,334,841,628.96",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","Mem Busy","%","1.56",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","Max Bandwidth","%","1.02",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","44.48",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","L2 Hit Rate","%","72.45",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.83",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Scheduler Statistics","One or More Eligible","%","26.97",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.27",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Scheduler Statistics","No Eligible","%","73.03",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.00",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.44",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 3.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","14.83",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","16.19",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","28.99",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","25.32",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 5.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 33.7% of the total average of 14.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","115.55",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Instruction Statistics","Executed Instructions","inst","37,902",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","126.16",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Instruction Statistics","Issued Instructions","inst","41,381",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","NVLink","Logical Links","","0",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","NVLink","Physical Links","","0",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Block Size","","512",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Grid Size","","8",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Shared Memory Configuration Size","byte","8,192",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Threads","thread","4,096",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Launch Statistics","Waves Per SM","","0.03",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Block Limit SM","block","16",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Block Limit Registers","block","4",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Block Limit Shared Mem","block","88",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Block Limit Warps","block","3",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Theoretical Occupancy","%","100",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Achieved Occupancy","%","32.66",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.68",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Source Counters","Branch Instructions Ratio","%","0.18",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Source Counters","Branch Instructions","inst","6,813",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Source Counters","Branch Efficiency","%","94.65",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","Source Counters","Avg. Divergent Branches","","0.76",
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 375 sectors, got 875 (2.33x) at PC 0x7f59935f73f0"
"102","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::MaxOps<float>, unsigned int, float, 4>)","2021-Feb-26 11:39:31","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 500 sectors, got 875 (1.75x) at PC 0x7f59935f7390"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,220,689,655.17",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,204,556,650.25",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,590",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","Memory [%]","%","0.49",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","SOL DRAM","%","0.08",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","Duration","nsecond","4,640",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","19.75",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.49",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","SM Active Cycles","cycle","45.62",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.34",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Compute Workload Analysis","Issue Slots Busy","%","8.75",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.35",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Compute Workload Analysis","SM Busy","%","8.75",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","Memory Throughput","byte/second","606,896,551.72",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","Mem Busy","%","0.49",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","Max Bandwidth","%","0.17",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","9.47",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","L2 Hit Rate","%","93.22",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.08",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Scheduler Statistics","One or More Eligible","%","8.43",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.08",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Scheduler Statistics","No Eligible","%","91.57",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.92",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.09",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 11.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.92 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.75",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","23.54",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.66",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.11",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 47.2% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3.86",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Instruction Statistics","Executed Instructions","inst","1,265",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3.99",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Instruction Statistics","Issued Instructions","inst","1,309",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","NVLink","Logical Links","","0",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","NVLink","Physical Links","","0",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Block Size","","256",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Grid Size","","1",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Registers Per Thread","register/thread","40",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","48",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Threads","thread","256",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Launch Statistics","Waves Per SM","","0.00",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Block Limit SM","block","16",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Block Limit Registers","block","6",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Block Limit Shared Mem","block","88",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Block Limit Warps","block","6",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Theoretical Occupancy","%","100",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Achieved Occupancy","%","16.71",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Occupancy","Achieved Active Warps Per SM","warp","8.02",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Source Counters","Branch Instructions","inst","154",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Source Counters","Branch Efficiency","%","98.86",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961060"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961650"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961660"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961670"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961680"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f5977961690"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616a0"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616b0"
"103","11868","python3.7","127.0.0.1","void cub::DeviceReduceSingleTileKernel<cub::DeviceReducePolicy<bool, int, int, cub::Sum>::Policy600, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int>(cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, int*, int, cub::Sum, int)","2021-Feb-26 11:39:35","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 8 sectors, got 9 (1.12x) at PC 0x7f59779616c0"
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,786,666,666.67",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,140,297,619.05",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","2,739",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","Memory [%]","%","0.95",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","SOL DRAM","%","0.02",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","Duration","nsecond","2,400",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","84.05",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.95",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","SM Active Cycles","cycle","10.71",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","GPU Speed Of Light","SM [%]","%","0.01",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.76",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.11",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Compute Workload Analysis","SM Busy","%","2.76",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","Memory Throughput","byte/second","160,000,000",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","Mem Busy","%","0.95",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","Max Bandwidth","%","0.33",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","L2 Hit Rate","%","99.60",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.01",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Scheduler Statistics","One or More Eligible","%","2.95",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.03",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Scheduler Statistics","No Eligible","%","97.05",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.02",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.03",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 33.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.02 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","34.53",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","41.35",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","30.09",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","18.93",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 28.2 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 81.7% of the total average of 34.5 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","WarpStateStats","","","","ThreadDivergence","WRN","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 30.1 threads being active per cycle. This is further reduced to 18.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible. In addition, ensure your kernel makes use of Independent Thread Scheduling, which allows a warp to reconverge after a data-dependent conditional block by explicitly calling __syncwarp()."
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","0.25",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Instruction Statistics","Executed Instructions","inst","81",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","0.30",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Instruction Statistics","Issued Instructions","inst","97",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","NVLink","Logical Links","","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","NVLink","Physical Links","","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Block Size","","128",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Grid Size","","1",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Registers Per Thread","register/thread","16",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Threads","thread","128",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Launch Statistics","Waves Per SM","","0.00",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Block Limit SM","block","16",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Block Limit Registers","block","32",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Block Limit Shared Mem","block","100",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Block Limit Warps","block","12",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Theoretical Occupancy","%","100",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Achieved Occupancy","%","8.11",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.89",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Source Counters","Branch Instructions Ratio","%","0.06",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Source Counters","Branch Instructions","inst","5",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Source Counters","Branch Efficiency","%","0",
"104","11868","python3.7","127.0.0.1","void cub::DeviceCompactInitKernel<cub::ScanTileState<int, true>, int*>(cub::ScanTileState<int, true>, int, int*)","2021-Feb-26 11:39:39","1","7","Source Counters","Avg. Divergent Branches","","0",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,526,315,789.47",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,105,850,563.91",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,381",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","Memory [%]","%","0.68",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","SOL DRAM","%","0.09",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","Duration","nsecond","4,864",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","7.79",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.68",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","SM Active Cycles","cycle","178.68",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","GPU Speed Of Light","SM [%]","%","0.26",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.22",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Compute Workload Analysis","Issue Slots Busy","%","5.82",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.23",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Compute Workload Analysis","SM Busy","%","5.82",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","Memory Throughput","byte/second","631,578,947.37",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","Mem Busy","%","0.68",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","Max Bandwidth","%","0.28",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","54.99",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","L2 Hit Rate","%","92.87",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.26",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Scheduler Statistics","One or More Eligible","%","5.87",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.06",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Scheduler Statistics","No Eligible","%","94.13",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.06",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 17.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.83",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.76",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.82",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.66",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","WarpStateStats","","","","CPIStallBarrier","WRN","On average each warp of this kernel spends 5.5 cycles being stalled waiting for sibling warps at a CTA barrier. This represents about 32.5% of the total average of 16.8 cycles between issuing two instructions. A high number of warps waiting at a barrier is commonly caused by diverging code paths before a barrier that causes some warps to wait a long time until other warps reach the synchronization point. Whenever possible try to divide up the work into blocks of uniform workloads. Use the Source View's sampling columns to identify which barrier instruction causes the most stalls and optimize the code executed before that synchronization point first."
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","9.86",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Instruction Statistics","Executed Instructions","inst","3,233",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","10.40",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Instruction Statistics","Issued Instructions","inst","3,412",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","NVLink","Logical Links","","0",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","NVLink","Physical Links","","0",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Block Size","","128",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Grid Size","","5",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Registers Per Thread","register/thread","44",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Shared Memory Configuration Size","byte","65,536",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","5,120",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Threads","thread","640",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Launch Statistics","Waves Per SM","","0.01",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Block Limit SM","block","16",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Block Limit Registers","block","10",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Block Limit Shared Mem","block","16",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Block Limit Warps","block","12",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Theoretical Active Warps per SM","warp","40",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Theoretical Occupancy","%","83.33",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Achieved Occupancy","%","8.31",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.99",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Source Counters","Branch Instructions Ratio","%","0.08",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Source Counters","Branch Instructions","inst","254",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Source Counters","Branch Efficiency","%","98.51",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","Source Counters","Avg. Divergent Branches","","0.01",
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e10"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e20"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e30"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e40"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 12 sectors, got 60 (5.00x) at PC 0x7f5977956e50"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584c0"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584d0"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584e0"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584f0"
"105","11868","python3.7","127.0.0.1","void cub::DeviceSelectSweepKernel<cub::DispatchSelectIf<cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::NullType, cub::NullType, int, false>::PtxSelectIfPolicyT, cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, false>(cub::CountingInputIterator<long, long>, cub::TransformInputIterator<bool, at::native::NonZeroOp<bool>, bool*, long>, long*, int*, cub::ScanTileState<int, true>, cub::NullType, cub::NullType, int, int)","2021-Feb-26 11:39:42","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 4 sectors, got 20 (5.00x) at PC 0x7f59779584b0"
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,198,449,612.40",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,205,606,312.29",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","8,297",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","Memory [%]","%","0.54",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","SOL DRAM","%","0.30",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","Duration","nsecond","6,880",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","5.85",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.54",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","SM Active Cycles","cycle","153.93",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","GPU Speed Of Light","SM [%]","%","0.08",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.17",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Compute Workload Analysis","Issue Slots Busy","%","4.44",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.18",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Compute Workload Analysis","SM Busy","%","4.50",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","Memory Throughput","byte/second","2,325,581,395.35",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","Mem Busy","%","0.54",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","Max Bandwidth","%","0.30",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","L2 Hit Rate","%","76.32",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.05",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Scheduler Statistics","One or More Eligible","%","4.47",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Scheduler Statistics","No Eligible","%","95.53",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","0.99",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 22.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction. Try to increase the number of active warps by increasing occupancy and/or avoid possible load imbalances due to highly different execution durations per warp."
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","22.22",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.87",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.26",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.04",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 11.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 52.4% of the total average of 22.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","6.63",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Instruction Statistics","Executed Instructions","inst","2,176",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","6.83",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Instruction Statistics","Issued Instructions","inst","2,240",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","NVLink","Logical Links","","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","NVLink","Physical Links","","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Block Size","","128",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Grid Size","","2",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Threads","thread","256",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Launch Statistics","Waves Per SM","","0.00",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Block Limit SM","block","16",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Block Limit Registers","block","16",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Block Limit Shared Mem","block","100",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Block Limit Warps","block","12",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Theoretical Occupancy","%","100",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Achieved Occupancy","%","8.21",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Occupancy","Achieved Active Warps Per SM","warp","3.94",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Source Counters","Branch Instructions Ratio","%","0.12",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Source Counters","Branch Instructions","inst","256",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Source Counters","Branch Efficiency","%","100",
"106","11868","python3.7","127.0.0.1","void at::native::index_elementwise_kernel<128, 4, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_index_kernel<at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1}>(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>, at::native::index_kernel_impl<at::native::OpaqueType<8> >(at::TensorIterator&, c10::ArrayRef<long>, c10::ArrayRef<long>)::{lambda(char*, char*, long)#1} const&)::{lambda(int)#1})","2021-Feb-26 11:39:46","1","7","Source Counters","Avg. Divergent Branches","","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,716,312,056.74",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,127,992,021.28",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","3,397",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","Memory [%]","%","1.14",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","SOL DRAM","%","0.72",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","Duration","nsecond","3,008",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","12.24",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","SOL L2 Cache","%","1.14",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","SM Active Cycles","cycle","73.54",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","GPU Speed Of Light","SM [%]","%","0.04",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.06",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Compute Workload Analysis","Issue Slots Busy","%","1.69",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.07",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Compute Workload Analysis","SM Busy","%","1.69",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","Memory Throughput","byte/second","5,361,702,127.66",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","Mem Busy","%","1.14",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","Max Bandwidth","%","0.72",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","42.58",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","L2 Hit Rate","%","73.03",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.04",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Scheduler Statistics","One or More Eligible","%","3.50",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.04",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Scheduler Statistics","No Eligible","%","96.50",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.04",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 28.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.04 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","28.82",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","32.12",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.74",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.81",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","WarpStateStats","","","","CPIStallImcMiss","WRN","On average each warp of this kernel spends 13.0 cycles being stalled waiting for an immediate constant cache (IMC) miss. This represents about 45.0% of the total average of 28.8 cycles between issuing two instructions. A read from constant memory costs one memory read from global memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. All threads access the same value."
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 10.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 37.5% of the total average of 28.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1.12",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Instruction Statistics","Executed Instructions","inst","366",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1.24",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Instruction Statistics","Issued Instructions","inst","408",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","NVLink","Logical Links","","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","NVLink","Physical Links","","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Block Size","","64",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Grid Size","","4",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Registers Per Thread","register/thread","26",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Threads","thread","256",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Launch Statistics","Waves Per SM","","0.00",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Block Limit SM","block","16",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Block Limit Registers","block","32",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Block Limit Shared Mem","block","100",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Block Limit Warps","block","24",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Achieved Occupancy","%","4.09",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Occupancy","Achieved Active Warps Per SM","warp","1.97",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Source Counters","Branch Instructions Ratio","%","0.09",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Source Counters","Branch Instructions","inst","34",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Source Counters","Branch Efficiency","%","100",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","Source Counters","Avg. Divergent Branches","","0",
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f597bf2a1d0"
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f597bf2a1e0"
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f597bf2a1f0"
"107","11868","python3.7","127.0.0.1","void at::native::vectorized_elementwise_kernel<4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3> >(int, at::native::CompareEqFunctor<long>, at::detail::Array<char*, 3>)","2021-Feb-26 11:39:49","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 96 sectors, got 192 (2.00x) at PC 0x7f597bf2a200"
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","8,245,398,773.01",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,210,752,629.27",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,317",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","Memory [%]","%","0.52",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","SOL DRAM","%","0.03",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","Duration","nsecond","5,216",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","4.67",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.52",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","SM Active Cycles","cycle","192.85",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","GPU Speed Of Light","SM [%]","%","0.10",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.00",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.39",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.10",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Compute Workload Analysis","SM Busy","%","3.24",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","Memory Throughput","byte/second","220,858,895.71",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","Mem Busy","%","0.52",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","Max Bandwidth","%","0.22",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","L2 Hit Rate","%","97.67",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.04",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Scheduler Statistics","One or More Eligible","%","4.79",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Scheduler Statistics","No Eligible","%","95.21",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.00",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 20.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.00 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.90",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.58",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.39",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.98",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 8.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 41.3% of the total average of 20.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","4.47",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Instruction Statistics","Executed Instructions","inst","1,465",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","4.61",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Instruction Statistics","Issued Instructions","inst","1,513",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","NVLink","Logical Links","","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","NVLink","Physical Links","","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Block Size","","64",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Grid Size","","4",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Threads","thread","256",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Launch Statistics","Waves Per SM","","0.00",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 4 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Block Limit SM","block","16",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Block Limit Registers","block","32",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Block Limit Shared Mem","block","100",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Block Limit Warps","block","24",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Achieved Occupancy","%","4.18",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Source Counters","Branch Instructions","inst","393",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Source Counters","Branch Efficiency","%","99.71",
"108","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:39:53","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,136,801,541.43",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,051,223,162.68",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","5,822",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","Memory [%]","%","0.54",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","SOL DRAM","%","0.22",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","Duration","nsecond","5,536",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","25.64",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.54",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","SM Active Cycles","cycle","48.79",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","GPU Speed Of Light","SM [%]","%","0.22",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.89",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Compute Workload Analysis","Issue Slots Busy","%","23.87",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.95",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Compute Workload Analysis","SM Busy","%","23.87",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","Memory Throughput","byte/second","1,479,768,786.13",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","Mem Busy","%","0.54",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","Max Bandwidth","%","0.23",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.90",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","L2 Hit Rate","%","83.28",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.22",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Scheduler Statistics","One or More Eligible","%","24.37",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.24",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Scheduler Statistics","No Eligible","%","75.63",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","3.93",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.36",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.93 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.12",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","17.36",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.87",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.34",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","10.82",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Instruction Statistics","Executed Instructions","inst","3,548",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","11.65",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Instruction Statistics","Issued Instructions","inst","3,820",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","NVLink","Logical Links","","0",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","NVLink","Physical Links","","0",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Block Size","","512",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Grid Size","","1",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Threads","thread","512",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Launch Statistics","Waves Per SM","","0.00",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Block Limit SM","block","16",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Block Limit Registers","block","4",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Block Limit Shared Mem","block","19",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Block Limit Warps","block","3",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Theoretical Occupancy","%","100",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Achieved Occupancy","%","32.40",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.55",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Source Counters","Branch Instructions Ratio","%","0.13",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Source Counters","Branch Instructions","inst","477",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Source Counters","Branch Efficiency","%","99.75",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 125 sectors, got 250 (2.00x) at PC 0x7f597f31ef40"
"109","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:39:57","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 125 sectors, got 250 (2.00x) at PC 0x7f597f31ef50"
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,953,216,374.27",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,169,355,680.87",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,400",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","Memory [%]","%","0.70",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","SOL DRAM","%","0.07",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","Duration","nsecond","5,472",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","2.35",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.70",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","SM Active Cycles","cycle","544.70",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","GPU Speed Of Light","SM [%]","%","0.26",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.09",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Compute Workload Analysis","Issue Slots Busy","%","2.27",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.09",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Compute Workload Analysis","SM Busy","%","3.04",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","Memory Throughput","byte/second","561,403,508.77",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","Mem Busy","%","0.70",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","Max Bandwidth","%","0.38",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","L2 Hit Rate","%","96.00",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.11",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Scheduler Statistics","One or More Eligible","%","4.60",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.05",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Scheduler Statistics","No Eligible","%","95.40",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","1.01",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.05",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 21.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 1.01 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.89",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","22.63",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.85",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","28.39",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","WarpStateStats","","","","CPIStallLongScoreboard","WRN","On average each warp of this kernel spends 9.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. This represents about 43.9% of the total average of 21.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to shared memory."
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.94",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Instruction Statistics","Executed Instructions","inst","3,915",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.34",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Instruction Statistics","Issued Instructions","inst","4,047",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","NVLink","Logical Links","","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","NVLink","Physical Links","","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Block Size","","64",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Grid Size","","11",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Threads","thread","704",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Launch Statistics","Waves Per SM","","0.01",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 11 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Block Limit SM","block","16",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Block Limit Registers","block","32",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Block Limit Shared Mem","block","100",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Block Limit Warps","block","24",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Theoretical Active Warps per SM","warp","32",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Theoretical Occupancy","%","66.67",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Achieved Occupancy","%","4.86",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Occupancy","Achieved Active Warps Per SM","warp","2.33",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Source Counters","Branch Instructions Ratio","%","0.27",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Source Counters","Branch Instructions","inst","1,049",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Source Counters","Branch Efficiency","%","99.89",
"110","11868","python3.7","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast>(int, at::native::copy_device_to_device(at::TensorIterator&, bool)::{lambda()#2}::operator()() const::{lambda()#6}::operator()() const::{lambda(long)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, char*, at::native::memory::LoadWithCast<1>, at::detail::Array<char*, 2>::StoreWithCast)","2021-Feb-26 11:40:00","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","DRAM Frequency","cycle/second","7,353,951,890.03",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","SM Frequency","cycle/second","1,081,415,684.83",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","Elapsed Cycles","cycle","6,717",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","Memory [%]","%","0.61",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","SOL DRAM","%","0.50",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","Duration","nsecond","6,208",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","SOL L1/TEX Cache","%","22.35",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","SOL L2 Cache","%","0.61",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","SM Active Cycles","cycle","59.05",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","GPU Speed Of Light","SM [%]","%","0.20",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","OK","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance."
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.78",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.01",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Compute Workload Analysis","Issue Slots Busy","%","20.92",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.84",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Compute Workload Analysis","SM Busy","%","20.92",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","Memory Throughput","byte/second","3,505,154,639.18",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","Mem Busy","%","0.61",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","Max Bandwidth","%","0.50",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","L1/TEX Hit Rate","%","49.96",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","L2 Compression Ratio","","0",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","L2 Hit Rate","%","65.40",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Memory Workload Analysis","Mem Pipes Busy","%","0.20",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Scheduler Statistics","One or More Eligible","%","21.44",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Scheduler Statistics","Issued Warp Per Scheduler","","0.21",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Scheduler Statistics","No Eligible","%","78.56",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Scheduler Statistics","Active Warps Per Scheduler","warp","4.00",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.31",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","SchedulerStats","","","","IssueSlotUtilization","WRN","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.31 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled."
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.65",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.03",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Warp State Statistics","Avg. Active Threads Per Warp","","31.79",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","27.39",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","11.50",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Instruction Statistics","Executed Instructions","inst","3,772",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","12.35",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Instruction Statistics","Issued Instructions","inst","4,052",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","NVLink","Logical Links","","0",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","NVLink","Physical Links","","0",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Block Size","","512",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Function Cache Configuration","","cudaFuncCachePreferNone",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Grid Size","","1",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Registers Per Thread","register/thread","32",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Shared Memory Configuration Size","byte","16,384",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Driver Shared Memory Per Block","byte/block","1,024",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","4,096",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Static Shared Memory Per Block","byte/block","16",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Threads","thread","512",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Launch Statistics","Waves Per SM","","0.00",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 82 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources."
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Block Limit SM","block","16",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Block Limit Registers","block","4",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Block Limit Shared Mem","block","19",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Block Limit Warps","block","3",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Theoretical Active Warps per SM","warp","48",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Theoretical Occupancy","%","100",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Achieved Occupancy","%","32.55",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Occupancy","Achieved Active Warps Per SM","warp","15.62",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Source Counters","Branch Instructions Ratio","%","0.13",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Source Counters","Branch Instructions","inst","491",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Source Counters","Branch Efficiency","%","99.75",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","Source Counters","Avg. Divergent Branches","","0.00",
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 339 sectors, got 677 (2.00x) at PC 0x7f597f31ef40"
"111","11868","python3.7","127.0.0.1","void at::native::reduce_kernel<512, 1, at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4> >(at::native::ReduceOp<long, at::native::func_wrapper_t<long, at::native::sum_functor<long, long, long>::operator()(at::TensorIterator&)::{lambda(long, long)#1}>, unsigned int, long, 4>)","2021-Feb-26 11:40:04","1","7","SourceCounters","","","","UncoalescedGlobalAccess","WRN","Uncoalesced global access, expected 339 sectors, got 677 (2.00x) at PC 0x7f597f31ef50"
